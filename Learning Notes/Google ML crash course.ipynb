{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "https://www.kaggle.com/thumbsnail/coming-from-google-s-machine-learning-crash-course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5241d4c-f7cd-4cc7-8d8d-4b0921319d9b",
    "_uuid": "a9e40d392209ab85e8a5bbed1891cc6a08812f2b"
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "**I.  [Getting Started](#gettingstarted)**\n",
    "1. [Introduction](#intro)\n",
    "2. [Kaggle vs Colab (Google's Colabratory)](#kaggle)\n",
    "3. [Setup - Importing Packages and Data Files](#setup)\n",
    " - [Importing Data from Other Data Sources](#otherdata)\n",
    "4. [Some Quirks in the Data:  Training Data Split over Multiple Files, Multiple Rows for One Entry, Missing Data](#quirks)\n",
    "5. [Pandas Pain](#pandaspain)\n",
    "  - [NaNs in the Data](#nans)\n",
    "  - [Condensing Multiple Rows into a Single Row](#multiplerows)\n",
    "  - [Combining Two DataSets](#combining)\n",
    "  - [Changing Only Certain \"Cells\" (that is, Update Column Values for Only Certain Rows)](#changecells)\n",
    "  - [Editing Lots of String Data (Removing Punctuation, Setting to Lowercase)](#stringdata)\n",
    "      - **[NEW:  Looking for Misspelled Words](#misspelledwords)**\n",
    "  - [Handling Date Information](#dateinfo)\n",
    "\n",
    "**II.  [Data Visualization](#datavisual)**\n",
    "1.  [Bar Chart](#barchart)\n",
    "2. [Scatter Plot](#scatterplot)\n",
    "3. [Line Plot](#lineplot)\n",
    "4. [Stacked-Bars Barchart](#stackedbars)\n",
    "5. [Pie Chart](#piechart)\n",
    "6. [More Line Plots to Look at Dates (Month, Week, Weekday)](#datedata)\n",
    "7.  [Violin Plots (Using Spelling/Typing Error Data)](#violin)\n",
    "\n",
    "**III.  [Applying the Crash Course to this Project to Create a Linear Classifier (Does NOT Use Text-Heavy Features)](#linearclassifier)**\n",
    "1. [Randomizing the Data](#random)\n",
    "2. [Creating Training and Validation Sets](#sets)\n",
    "3. [Choosing Good Features (Location of DESIRED_FEATURES const)](#desiredfeatures)\n",
    "4. [Setting Up for Training](#trainingsetup)\n",
    "5. [Regularization](#regular)\n",
    "6. [Training](#lineartraining)\n",
    "7. [ROC and AUC](#rocandauc)\n",
    "8. [Experiment Results](#linearpretextexps)\n",
    "\n",
    "**IV. [Dealing with Text-Heavy Features:  Using the Crash Course to Bypass TensorFlow Pain](#bruteforcebaby)**\n",
    "1. [Converting from Pandas to TFRecord](#tfrecordrescue)\n",
    " - [Handle the Test Dataset](#testrecord)\n",
    "2. [Loading TFRecord into TensorFlow](#recordtotf)\n",
    "3. [Setting up for Training (with Text-Heavy Features)](#traintextheavy)\n",
    "4. [Training](#lineartexttraining)\n",
    "5. [AUC](#lineartextauc)\n",
    "6. [Experiment Results](#lineartextexps)\n",
    "\n",
    "**V. [Applying the Crash Course to Create a DNNClassifier](#dnnclassifier)**\n",
    "1. [Setting Up for Training](#dnntrainingsetup)\n",
    "2. [Training](#dnntraining)\n",
    "3. [AUC](#dnnauc)\n",
    "4. [Experiment Results](#dnnpretextexps)\n",
    "\n",
    "**VI. [Submitting Predictions](#submit)**\n",
    "1. [Making Predictions on the Test Dataset](#predictions)\n",
    "2. [Convert to .csv](#csv)\n",
    "3. [Submit to Kaggle](#submitfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "771886da-193d-403e-b73c-e5375c0ef53b",
    "_uuid": "b5e3b9a735693c2d82bc265e9afc836871e52791"
   },
   "source": [
    "**I. Getting Started**\n",
    "<a id=\"gettingstarted\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cdea0fe9-88b6-43fa-8c8c-7cff57bd6c6a",
    "_uuid": "56faf290965fc711bb954234d464262ee5ff63c4"
   },
   "source": [
    "**1. Introduction**\n",
    "<a id=\"intro\"></a>\n",
    "\n",
    "If you're like me, perhaps your only machine learning experience is from Google's [ Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/next-steps).  This will be a brief guide to help you transition from the nice and tidy Programming Exercises of that course to this Kaggle competition that has some issues not encountered in those exercises.  In short, hopefully this will help save you some time and spare you from some hassle.  I'll also be documenting my own attempt to tackle this machine learning problem.\n",
    "\n",
    "I originally built a Linear Classifier and DNN Classifier using a lot of the code from the Programming Exercises.  For the features, I only incorporated a few features (basically, any whose data did NOT consist of a bunch of sentences like the essays and the descriptions).  But these models didn't do much better than a random guesser.  So the next step was to incorporate those text-heavy features into the model.\n",
    "\n",
    "However, when I attempted to do so, I couldn't get the code to work in the way that I wanted.  I ended up having to completely change how I load in the data (because I couldn't find another solution) so that TensorFlow would look at those strings as individual words to then compare to a vocabulary_list as opposed to it looking at those strings as full sentences (making the comparison to a vocabulary_list made up of individual words useless).  I've kept the old code for the few-feature models if you want to see how to get those models up and running.  But mixed within some of that code may be code that only really applies to the newer model that looks at text data as individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4de273a4-fd43-497f-83b7-b6adda45116b",
    "_uuid": "132724635d112f07e09135849812112600b85409"
   },
   "source": [
    "**2.  Kaggle vs Colab (Google's Colabratory)**\n",
    "<a id=\"kaggle\"></a>\n",
    "\n",
    "The Programming Exercises in the Machine Learning Crash Course were done in Google's Colabratory environment.  So my first question was \"How do I get the data from this Kaggle competition into Colab?\"  While you can likely do that using...\n",
    "\n",
    "* [External data: Drive, Sheets, and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=c2W5A2px3doP)\n",
    "* and/or [Kaggle API](https://github.com/Kaggle/kaggle-api)\n",
    "\n",
    "...it's unnecessary.  The Kaggle environment is effectively the same, making it easier just to get started right here.\n",
    "\n",
    "So, to use Kaggle and get your programming environment set up, you'll need:\n",
    "\n",
    "i. A [Kaggle account](https://www.kaggle.com/account/login) (which requires an email address and a phone number for verification)\n",
    "\n",
    "ii. To start [a new kernel](https://www.kaggle.com/kernels)\n",
    "\n",
    "iii. To then choose Notebook (if you want your environment to pretty much have the same feel as Colab)\n",
    "\n",
    "iv. To click on the Data tab at the top of your Notebook and then click \"Add Data Source\" (searching for the DonorsChoose competition and then agreeing to its rules)\n",
    "\n",
    "That's pretty much it!  You should now have access to the DonorsChoose files in your Notebook and can begin working with them in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "763e11e0-25d6-4377-97bc-23373bdd7aea",
    "_uuid": "58191d2e983396bd839f66774e06d4638daa761f"
   },
   "source": [
    "**3. Setup - Importing Packages and Data Files**\n",
    "<a id=\"setup\"></a>\n",
    "\n",
    "This should be similar to what you saw in the Programming Exercises.  Your Notebook should load up with a handy note that the data files have this file path:  '../input/filename.csv', which we'll use to load the csv files into a Pandas DataFrame in the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "import sklearn.metrics as metrics\n",
    "import os # to access data files (found in the \"../input/\" directory)\n",
    "import re  #regular expressions\n",
    "\n",
    "# More Packages from https://colab.research.google.com/notebooks/mlcc/sparsity_and_l1_regularization.ipynb?hl=en#scrollTo=pb7rSrLKIjnS\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Data Files  NOTE:  My file paths here may look different since I'm using files from multiple data sources \n",
    "training_dataset = pd.read_csv('../input/donorschoose-application-screening/train.csv', sep=',')\n",
    "resources_dataset = pd.read_csv('../input/donorschoose-application-screening/resources.csv', sep=',')\n",
    "test_dataset = pd.read_csv('../input/donorschoose-application-screening/test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b47606e1-ee17-4ece-ad51-eccf82a997c1",
    "_uuid": "e9d71297b8998a5e4aed06b1c69d048c61a6555f"
   },
   "source": [
    "*i.  Importing Data from Other Data Sources*\n",
    "<a id=\"otherdata\"></a>\n",
    "\n",
    "As I continued to work on this project, I discovered that I wanted to check for misspelled words in the data as an additional feature.  A very long story short, I came to the conclusion that it would be great to have an actual dictionary of English words to compare to.  The following additional data sources are an attempt to construct such a dictionary (though they aren't sufficiently complete):\n",
    "\n",
    " - [English Word Frequency](https://www.kaggle.com/rtatman/english-word-frequency)\n",
    " - [Unix words](https://www.kaggle.com/nltkdata/unix-words) (not many unique words compared to above)\n",
    " - [Brown Corpus](https://www.kaggle.com/nltkdata/brown-corpus) (also didn't add much, so I opted not to actually use this one)\n",
    "\n",
    "If you do want to add additional data sources to your Notebook/Kernel, it's the exact same process as described earlier via the \"Data\" tab at the top of the Notebook.  The only difference is that adding a 2nd data source *alters the file paths of where those data files are stored in your Notebook*, meaning there are now subfolders under the 'input' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "406a5773-c21f-49c1-9a2a-d64e1288b922",
    "_uuid": "1d53b93d8fb4b62ad359443738d31e24fd845b22",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#English Word Frequency\n",
    "dictionary_dataset = pd.read_csv('../input/english-word-frequency/unigram_freq.csv', sep=',')\n",
    "\n",
    "#Unix Words\n",
    "#https://stackoverflow.com/questions/3277503/in-python-how-do-i-read-a-file-line-by-line-into-a-list\n",
    "with open('../input/unix-words/words/en') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] \n",
    "\n",
    "unix_set = set(content)\n",
    "\n",
    "#Brown Corpus\n",
    "USING_BROWN = False  #Though Brown has \"1 million words,\" most of them are NOT unique.  So this wasn't that helpful\n",
    "if USING_BROWN:\n",
    "    #https://www.kaggle.com/alvations/testing-1000-files-datasets-from-nltk\n",
    "    #^Code to get data into pandas dataframe:\n",
    "    from nltk.corpus import (LazyCorpusLoader, CategorizedTaggedCorpusReader)\n",
    "\n",
    "    import nltk\n",
    "    # Removing the original path\n",
    "    if '/usr/share/nltk_data' in nltk.data.path:\n",
    "        nltk.data.path.remove('/usr/share/nltk_data')\n",
    "    nltk.data.path.append('../input/brown-corpus/brown')\n",
    "    nltk.data.path\n",
    "\n",
    "    brown = LazyCorpusLoader('brown', CategorizedTaggedCorpusReader, r'c[a-z]\\d\\d',\n",
    "                             cat_file='cats.txt', tagset='brown', encoding=\"ascii\",\n",
    "                            nltk_data_subdir='brown-corpus/brown')\n",
    "\n",
    "    list_of_lists_of_tuples = brown.tagged_sents()\n",
    "\n",
    "    list_sents = []\n",
    "    for list_of_tuples in list_of_lists_of_tuples:\n",
    "        sentence = \" \".join([tupl[0] for tupl in list_of_tuples])\n",
    "        list_sents.append(sentence)\n",
    "\n",
    "    list_sents\n",
    "\n",
    "    pd_brown = pd.DataFrame({'sentence': list_sents})\n",
    "    \n",
    "    #some text formatting:\n",
    "    pd_brown['sentence'] = pd_brown['sentence'].replace('(\\-)|(/)|(\\.\\.\\.\\.)|(\\.\\.\\.)', ' ', regex=True)\n",
    "    pd_brown['sentence'] = pd_brown['sentence'].replace('[\"#$%&\\'()*+,.:!;<>?@\\\\^_`{|}~]', '', regex=True)\n",
    "    pd_brown['sentence'] = pd_brown['sentence'].replace('(   )|(  )', ' ', regex=True)\n",
    "    pd_brown['sentence'] = pd_brown['sentence'].str.lower()\n",
    "    pd_brown[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a86c664-024f-4653-ac11-3a037aaa24a2",
    "_uuid": "7d1089ab4a68a72a13cdb7a07be6dff9dd1eacad"
   },
   "source": [
    "**4. Some Quirks in the Data**\n",
    "<a id=\"quirks\"></a>\n",
    "\n",
    "i.  In the Programming Exercises, all your feature training data would come from a single csv file and get loaded into a single Pandas DataFrame.  However, in this project, part of the training data is in train.csv and part is in resources.csv, the common link being the same 'id' for a given entry in both files.  Thus, we're going to need some way to combine the two data sets.\n",
    "\n",
    "ii.  Additionally, resources.csv has the added twist that *not all of an id's information is in the same row *.  Instead, some project ids are requesting multiple resources, which thus span multiple lines.  For example, submission p069063:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "d15e7082-1a44-4e30-bc01-fc7cff4aef90",
    "_kg_hide-input": false,
    "_uuid": "a713291c0b549cdd75567abec13f25cc7a177b51",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resources_dataset[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b4e23c07-f91f-4000-a43f-7b591789f060",
    "_uuid": "4abfa0e20d0f6093170ff8fb3e99b6d493ee7e08"
   },
   "source": [
    "  So, you might be thinking about how to group some (or all) of that information together as a single-line entry.\n",
    "  \n",
    "  iii.  If you play around with resources.csv, you may eventually discover that some of the description data is actually missing.  Some of the cells are filled with NaN, which you may want to handle.  For example, p194324:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "719fe24f-ec4c-4262-8bd7-fe2159c3363c",
    "_kg_hide-input": false,
    "_uuid": "ec0fd4f929e974b6545aeec70e5073c116cd5108",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resources_dataset[37602:37612]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f5a30a8-7dc5-40a5-b61b-dececd801f3e",
    "_uuid": "1171b1537584a06f0e1098d8d780c54964e083a3"
   },
   "source": [
    "**5. Pandas Pain**\n",
    "<a id=\"pandaspain\"></a>\n",
    "\n",
    "In the Programming Exercises from the course, there really weren't that many different functions used from Pandas.  Pandas is cool because it can do a million different complex things.  However, Pandas is also frustrating for the same reason, especially when you just want to do something that seems trivial and yet have to comb through documentation or StackOverflow to figure out how to do that.  To spare you from (some of) that searching, let's work through the quirks listed above in reverse order.\n",
    "\n",
    "*iii.  NaNs in the data*\n",
    "<a id=\"nans\"></a>\n",
    "\n",
    "A handy Pandas function to know for a DataFrame is:  **.isnull().any()**, which alerts you to missing values in any of your columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "c5eb9497-b965-4b43-80a5-343accb0e07b",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "02884f4b8955b39be119649abcb179f4d4d6546d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resources_dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "22dd889f-91d5-4aad-8982-17637f44de61",
    "_uuid": "fd1a01dc8870fb0c645a8c6b2334ec50865a69e7"
   },
   "source": [
    "(Note:  If you wanted to locate one of the rows and see the missing data for yourself, you could use the **.loc()** function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "bee34a5d-e94e-4a85-8d0f-b3eee41c4d04",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "6cd5f1ffdf2e1cf88c1e1ca259f4e4e0cbfe2f30",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resources_dataset.loc[resources_dataset[\"description\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "3c0e7420-4ab9-4a97-814d-9f6804469d71",
    "_uuid": "f63962735b778004ca296d5831e1ba970593f765"
   },
   "source": [
    "Now, let's go ahead and replace those NaNs so that they don't cause any trouble if we were to, say, try to join together all the description strings for a project id into a single string.  To do so, we'll use the function **.fillna({column_name : replacement_value})**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "eb708f72-803e-46c2-aa02-bf621859295f",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "c4c3e4d41234f46ca0e8930137581db955badf93"
   },
   "outputs": [],
   "source": [
    "resources_dataset = resources_dataset.fillna({'description' : 'no_detail'})\n",
    "resources_dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "66f8b92d-f50b-446f-b678-2e3dbc859ff6",
    "_uuid": "a3716ca903e882f2d1159825083663cc091f4159"
   },
   "source": [
    "*ii. Condensing multiple rows into one single row*\n",
    "<a id=\"multiplerows\"></a>\n",
    "\n",
    "In order to do this, we'll need the **.groupby()** function for grouping and the **.agg()** function in order to perform an operation on the cells that are being condensed.  But first, let's create a new feature that we'll likely want to consider:  the cost of a given request.  The resources.csv file contains a request's quantity and its price.  If we multiply those two values together, we'll end up with the cost for that request.  This is something done in the Programming Exercises, so it should likely look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6b7b721f-8527-4a2c-9887-3cf6cfbd87a7",
    "_kg_hide-input": false,
    "_uuid": "b72247019982d2b173178d53f4dee50365c1b1c9"
   },
   "outputs": [],
   "source": [
    "# Make a cost column (quantity * price)\n",
    "resources_dataset['cost'] = resources_dataset['quantity'] * resources_dataset['price']\n",
    "resources_dataset[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f403e6fc-4f4d-4e08-bd4f-c0fc328de845",
    "_uuid": "93eb60d7e0a9434d1a09e2c3780b4a63167ee395"
   },
   "source": [
    "Now, let's use the **.groupby()** function and the **.agg()** function to condense, for example, all those p069063 rows into a single line such that we just have one p069063 entry with a total cost that has summed up all the costs of the individual requests.  In this example, I'm only interested in the 'id' and the 'cost' columns, so the final DataFrame will only have those two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "d876ec7b-993b-4d5f-be03-47083a61c17d",
    "_kg_hide-input": false,
    "_uuid": "61bfe496067ff78a2589025ce927a8769c19c490"
   },
   "outputs": [],
   "source": [
    "#create a total_cost column\n",
    "\n",
    "grouped_ids = resources_dataset.groupby(['id'], as_index=False)\n",
    "resources_condensed = grouped_ids.agg({'cost' : 'sum'}).rename(columns={'cost' : 'total_cost'})\n",
    "#resources_condensed.loc[resources_condensed['id'] == 'p069063']\n",
    "resources_condensed[69060:69065]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "327d3ce8-f4b4-4530-affc-45a02f528fc9",
    "_uuid": "0a24f1f38fc6416663d1e6896871bb2c8c1f29b6"
   },
   "source": [
    "Cool!  Let's step through the code.  We pass to **.groupby()** the column that contains the values that repeat in multiple rows.  We also pass in as_index as False so that 'id' (such as p069061, p069062, etc.) doesn't get used as the index.  Instead, each row will have a number index like normal (such as 69060, 69061, etc.)\n",
    "\n",
    "The **.agg()** function is then called so that we can perform an operation on the cost values that all belong to the same id.  (In this case, we want to add them all up together.)  We pass in a dictionary of the form *{ column_name : operation_we_want_to_perform }*.  Lastly, I just renamed the column to show that it's the final, total cost for all of the requests made by a given id.\n",
    "\n",
    "Now, if you wanted to, you could combine the other columns as well by passing multiple dictionaries into **.agg()**.  I'm going to go ahead and do that just in case I decide to do anything with the description data later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "429add84-eb85-487e-bfd3-45dfe624d7c3",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "035f2fd8e6eb9b8faaaf62b6ae4c758c04616a4a"
   },
   "outputs": [],
   "source": [
    "#Join together all of the columns\n",
    "group_the_ids = resources_dataset.groupby(['id'], as_index=False)\n",
    "all_condensed = grouped_ids.agg({'description' : lambda x: ' '.join(x), #there's no simple single keyword option like 'sum'\n",
    "                                 'quantity' : 'sum',\n",
    "                                 'cost' : 'sum'}).rename(columns={'description' : 'full_description',\n",
    "                                                                  'quantity' : 'total_quantity',\n",
    "                                                                  'cost' : 'total_cost'})\n",
    "all_condensed[69060:69065]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "5def2388-9640-4487-8801-72a931058c61",
    "_uuid": "38882cf65b498644ed5487835b933bf3178856af"
   },
   "source": [
    "Trying to look at the full string of a single cell (to check that this actually worked) is a bit of a pain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "49805dc5-e57e-4a75-8797-ea561cd87e9a",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "1e5c2e88b62462ff01f93c4d34c9c4d428962785",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Show that p069063's full_description actually contains the joined text:\n",
    "entry = all_condensed.loc[all_condensed['id'] == 'p069063'].reset_index()\n",
    "entry.loc[0, 'full_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c80d08c-1443-4a53-8efd-5cbc2a9e7d39",
    "_uuid": "99df476c49b333716c91abdf007ce51c136cd5d8"
   },
   "source": [
    "*i. Merging together two DataFrames*\n",
    "<a id=\"combining\"></a>\n",
    "\n",
    "Okay, we finally have the desired data from resources.csv organized!  We now want to combine this DataFrame with our other training DataFrame.  To do so, we'll use the **.merge()** function.  This function takes as arguments the DataFrame we want as our \"left columns\" followed by the DataFrame we want to tack on as the \"right columns\".  We also specify the common link between the two DataFrames (in this case, 'id') via *on='id'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f7c374b6-0915-4b25-9fd0-748855ada948",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "73ef6326e44472af37cc0f4c2c5f108e0e6e3aee"
   },
   "outputs": [],
   "source": [
    "combined_training_dataset = pd.merge(training_dataset, all_condensed, on='id')\n",
    "combined_training_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "64df3d0f-ba09-4442-a1de-3fa1c6789f96",
    "_uuid": "be09014be7efa6614be23d5b961f99769f15ca95"
   },
   "source": [
    "Woo hoo!  All that's left to do is the same kind of merging but with resources.csv and test.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "96e8c302-fda9-43ca-a767-a6ca00317f91",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "b556b6b1afb54221152ec465c1113464433eea78"
   },
   "outputs": [],
   "source": [
    "combined_test_dataset = pd.merge(test_dataset, all_condensed, on='id')\n",
    "combined_test_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "8dcaea1b-d535-4049-b34f-9e7d3e71df40",
    "_uuid": "f0f9f055805ffcea1ab38d4e6cec698ded721415"
   },
   "source": [
    "I believe the data is now organized in a manner familiar to what we encountered in the Programming Exercises.  Don't forget to also check for and handle other NaNs in the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "799f9bd9-9984-4efc-a387-8ddd4aa9d5b3",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "df242a9f9eaf4b3e26e1dea2ced302de4d4288de",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['teacher_prefix'] = combined_training_dataset['teacher_prefix'].fillna('none')\n",
    "combined_test_dataset['teacher_prefix'] = combined_test_dataset['teacher_prefix'].fillna('none')\n",
    "\n",
    "combined_training_dataset = combined_training_dataset.fillna('')\n",
    "combined_test_dataset = combined_test_dataset.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "479f8e13-19b2-40c5-b8bd-70dd55a63328",
    "_uuid": "3693df4a18c08442a09d656beebc6dfd7eb0abbd"
   },
   "source": [
    "*iv. Changing Only Certain \"Cells\" (that is, Update Column Values for Only Certain Rows)*\n",
    "<a id=\"changecells\"></a>\n",
    "\n",
    "I'm using \"cells\" because I think it's easier to think in terms of spreadsheets.  In the data for this competition is the note that applications before a certain date required 4 essays.  It turns out that old essays 1 and 2 cover the same topics as current essay 1, and old essays 3 and 4 cover the same topics as current essay 2.  So what I'd like to do is combine essays 1 and 2 together into the project_essay_1 column and combine essays 3 and 4 together into the project_essay_2 column **ONLY for those \"old application\" rows**.  The \"new application\" rows should be left alone.\n",
    "\n",
    "*(Note:  I don't entirely know if it matters that the two essays are treated as two separate features.  However, in experiments, my models had a slightly better AUC when they were left as separate features.  I also tried smashing ALL of the text features together out of curiosity, but that did result in a worse AUC.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "d3c48451-8336-4d88-a2b4-69cc268577d1",
    "_uuid": "a62a54606424c880e8344efba92ebdff7d347364"
   },
   "source": [
    "The \"cell selection\" method in Pandas that I'll be using is .loc[row_indexer, col_indexer].  To select the \"old application\" rows, I'm going to ask Pandas to find all the rows that currently DO have text in project_essay_4.  That will be the row_indexer.  The col_indexer will be the column where I want to leave the two combined essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "84c82cee-584f-4c7e-b8b4-7b14426e0754",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d7b622565aa687137c22912963d5ec0208a043fd"
   },
   "outputs": [],
   "source": [
    "combined_training_dataset.loc[combined_training_dataset['project_essay_4'] != '', 'project_essay_1'] = combined_training_dataset['project_essay_1'] + ' ' + combined_training_dataset['project_essay_2']\n",
    "combined_training_dataset.loc[combined_training_dataset['project_essay_4'] != '', 'project_essay_2'] = combined_training_dataset['project_essay_3'] + ' ' + combined_training_dataset['project_essay_4']\n",
    "\n",
    "#and do the same for the test data:\n",
    "combined_test_dataset.loc[combined_test_dataset['project_essay_4'] != '', 'project_essay_1'] = combined_test_dataset['project_essay_1'] + ' ' + combined_test_dataset['project_essay_2']\n",
    "combined_test_dataset.loc[combined_test_dataset['project_essay_4'] != '', 'project_essay_2'] = combined_test_dataset['project_essay_3'] + ' ' + combined_test_dataset['project_essay_4']\n",
    "\n",
    "combined_training_dataset[16:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "4b67b3ae-10f9-4385-ac4a-85df73adbd27",
    "_uuid": "7b440aa9e8ad31a5ec98b41a3e7143a6082b05e2"
   },
   "source": [
    "To see if the above worked, look at the whole string in an \"old application\"'s cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b7bbeed4-e9d3-45f2-8ae0-7c9bda833be5",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d560ffee344e42483e68dbcd42702390bdb7f8a7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset.loc[18, 'project_essay_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc914dc8-c2a7-4f3a-907b-cccb430210e3",
    "_uuid": "6390c42e492030996a87ffe4bd819cd35057757a"
   },
   "source": [
    "*v. Editing Lots of String Data (Removing Punctuation, Setting to Lowercase)*\n",
    "<a id=\"stringdata\"></a>\n",
    "\n",
    "This pertains only to the newer models where I'm incorporating the text-heavy features.  To make it so that the vocabulary_lists can match the words in the text sentences, it's best to strip out punctuation marks and lowercase all the letters.\n",
    "\n",
    "Additionally, this now attempts to better format the text such that it's easier to identify misspelled words and typos.  For comparing words to an English dictionary, I now strip out URLs and Twitter hashtags that obviously aren't in the dictionary.  For typos (like not spacing after a comma or quotation mark), I replace those with an equal sign ( = ) and then tally them up later on.\n",
    "\n",
    "The 'check the dictionary' code takes a while to run, so I've left the old code here if you're not interested in looking for misspellings.  (From my experiments, incorporating the misspellings data hasn't yield any better AUC.)  And if you want to skip ahead, here's [the next section](#dateinfo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f5152947-1427-46aa-b56f-9eaa8368aa46",
    "_uuid": "7ed9d681517f28a42ba389b8710cf1d90ed2dec6"
   },
   "source": [
    "*Old, adequate code to format text* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "24a5e7f8-3d7d-45df-a22a-47c015a31d4a",
    "_uuid": "278e6ba03d2b26192133cf27aa3519fcdd518be7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "USING_DICTIONARY_CODE = True\n",
    "if USING_DICTIONARY_CODE == False:\n",
    "    #OLD Text preprocessing\n",
    "    the_essays = [\n",
    "        'project_essay_1',\n",
    "        'project_essay_2',\n",
    "        'full_description',  #not an essay... but has the same problems\n",
    "        'project_resource_summary'  # probably has the same issues\n",
    "    ]\n",
    "\n",
    "    def tidy_essays(dataset):\n",
    "        for col_name in the_essays:\n",
    "            #strip out the irritating new line stuff:\n",
    "            dataset[col_name] = dataset[col_name].replace(r'(\\\\r)|(\\\\n)', ' ', regex=True)\n",
    "            #get everything down to just one space (hopefully):\n",
    "            dataset[col_name] = dataset[col_name].replace(r'  ', ' ', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace(r'  ', ' ', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace(r'  ', ' ', regex=True)\n",
    "\n",
    "    #do both training and test datasets:\n",
    "    tidy_essays(combined_training_dataset)\n",
    "    tidy_essays(combined_test_dataset)\n",
    "    \n",
    "    #OLD finish cleaning up all the text\n",
    "    text_columns = [\n",
    "        'project_title',\n",
    "        'project_essay_1',\n",
    "        'project_essay_2',\n",
    "        'project_resource_summary',\n",
    "        'project_subject_categories',\n",
    "        'project_subject_subcategories',\n",
    "        'full_description'\n",
    "    ]\n",
    "\n",
    "    punc_pattern = '[\"#$%&\\'()*+,.:;<=>?@\\\\\\\\^_`{|}~]'\n",
    "\n",
    "    def text_edits(dataset):\n",
    "        for col_name in text_columns:\n",
    "            #lowercase all\n",
    "            dataset[col_name] = dataset[col_name].str.lower()\n",
    "            #treat !'s as separate words just in case the model picks up on something:\n",
    "            dataset[col_name] = dataset[col_name].replace(r'!', ' !', regex=True)\n",
    "            #don't remove hyphens and smash words together; keep the words separate\n",
    "            #same (hopefully) for /\n",
    "            dataset[col_name] = dataset[col_name].replace(r'(\\-)|(/)', ' ', regex=True)  #ADDED so hopefully works\n",
    "            #replace the rest of the punctuation:\n",
    "            dataset[col_name] = dataset[col_name].replace(punc_pattern, '', regex=True)\n",
    "\n",
    "    #do both training and test datasets:\n",
    "    text_edits(combined_training_dataset)\n",
    "    text_edits(combined_test_dataset)\n",
    "\n",
    "    combined_training_dataset[16:22]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "521083de-038d-4571-86e5-e1e92bda450c",
    "_uuid": "9b4d9836240549ab4fd256b44cec972118455edf"
   },
   "source": [
    "*New code to find misspellings and typos.*\n",
    "<a id=\"misspelledwords\"></a>\n",
    "\n",
    "Note:  This first cell takes a long time to run (probably 3 minutes on just the training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "8d29a9e0-c81f-4b7b-b75c-42e3199e0446",
    "_uuid": "724c70cbb88dc0476166d53840b174329327bebb"
   },
   "outputs": [],
   "source": [
    "#USING_DICTIONARY_CODE located in cell above\n",
    "if USING_DICTIONARY_CODE:\n",
    "    text_columns = [\n",
    "        'project_title',\n",
    "        'project_essay_1',\n",
    "        'project_essay_2',\n",
    "        'project_resource_summary',\n",
    "        'project_subject_categories',\n",
    "        'project_subject_subcategories',\n",
    "        'full_description'\n",
    "    ]\n",
    "\n",
    "    def tidy_text(dataset):\n",
    "        for col_name in text_columns:\n",
    "            #new idea:  handle weird utf8 symbol characters like smiley faces\n",
    "            #  replace with ?, then strip out that question mark and replace it with a space:\n",
    "            #first, remove actual ?'s and replace with no space '' (this is because some URLs contain ?'s):\n",
    "            dataset[col_name] = dataset[col_name].replace('\\?', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].str.encode('ascii', 'replace')  # replaces with a ? mark\n",
    "            dataset[col_name] = dataset[col_name].str.decode('utf8') #turn back to normal string\n",
    "                #new ?'s will be removed in a later regex\n",
    "            #new idea:  handle acronyms since they're not going to show up in dictionaries:\n",
    "                #is the actual acronym important?  I'm assuming not.  But perhaps you should work on a copy of the data\n",
    "            dataset[col_name] = dataset[col_name].replace('([A-Z][.])+', '', regex=True)\n",
    "            #0th - lowercase everything:\n",
    "            dataset[col_name] = dataset[col_name].str.lower()\n",
    "            #1st - strip out the irritating new lines that literally became \\\\r\\\\n in the text:\n",
    "            #replace with a space so that words aren't unfairly smashed together and count as an error\n",
    "            dataset[col_name] = dataset[col_name].replace('(\\\\\\\\r)|(\\\\\\\\n)', ' ', regex=True)\n",
    "            #2nd - remove all the website links\n",
    "            # .com AND .org AND .gov AND .edu AND .net\n",
    "            dataset[col_name] = dataset[col_name].replace('[^ ]*\\.com[^ ]*', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace('[^ ]*\\.org[^ ]*', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace('[^ ]*\\.gov[^ ]*', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace('[^ ]*\\.edu[^ ]*', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace('[^ ]*\\.net[^ ]*', '', regex=True)\n",
    "            #2ndb - also remove tweet hashtags:\n",
    "            dataset[col_name] = dataset[col_name].replace('#[a-z][^ ]*', '', regex=True)\n",
    "            #3rd - remove other \\\\ affecting things like \" marks\n",
    "            dataset[col_name] = dataset[col_name].replace('\\\\\\\\', '', regex=True)\n",
    "            #4th - don't remove hyphens/slashes and smash words together; instead, keep the words separate\n",
    "            #same with ellipsis\n",
    "            dataset[col_name] = dataset[col_name].replace('(\\-)|(/)|(\\.\\.\\.\\.)|(\\.\\.\\.)', ' ', regex=True)\n",
    "            #5th - some people didn't space correctly.  Replace those specific instances involving punctuation marks\n",
    "            #with a = sign to better handle that particular typo later on when looking for misspelled words\n",
    "            #(first, remove any equal signs already in the text just in case)\n",
    "            dataset[col_name] = dataset[col_name].replace('=', '', regex=True)\n",
    "            dataset[col_name] = dataset[col_name].replace('(?<=\\w)[,.;!\"](?=\\w)', '=', regex=True)\n",
    "            #6th - treat !'s as separate words in case the model picks up on something related to too many exclamatory remarks:\n",
    "            dataset[col_name] = dataset[col_name].replace('!', ' ! ', regex=True)\n",
    "            #7th remove the remaining punctuation:\n",
    "            dataset[col_name] = dataset[col_name].replace('[\"#$%&\\'()*+,.:;[\\]<>?@\\\\^_`{|}~]', '', regex=True)\n",
    "            #8th - make everything have a separation of just one space:\n",
    "            dataset[col_name] = dataset[col_name].replace(' *   *', ' ', regex=True)\n",
    "            #and no space at the end:\n",
    "            dataset[col_name] = dataset[col_name].str.strip()\n",
    "\n",
    "        return\n",
    "\n",
    "    tidy_text(combined_training_dataset)\n",
    "    tidy_text(combined_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b5062455-465e-43dc-b112-863e98e96705",
    "_uuid": "8239e945000432a7c14f814c6911661970f1df72"
   },
   "source": [
    "Here's an example of an essay with two cases of an '=' sign inserted where someone forgot to space after punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f6d6b174-e157-4ded-96f4-c4eb1d099a3d",
    "_uuid": "9769f452d9dc91894367a6366d08ffdd9681c4a2",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['project_essay_2'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "8b6d8b0d-4157-4799-a59b-b227024590bc",
    "_uuid": "d803a0d772456af7269de86319bb4930d607d9be"
   },
   "source": [
    "Now, count up these \"typing errors\" in each text column.  I assume a reader would be more forgiving of these, but track them anyways.  Then, fix them (replace the = sign with a space) so that they don't show up in the vocabulary list for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "75405da8-5d46-4122-99ff-c4f4511b182b",
    "_uuid": "09fdf7c0ac9e84399f01e1ac56fef2355f0c19ea",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    COLUMNS_TO_CHECK = [\n",
    "        'project_title',\n",
    "        'project_essay_1',\n",
    "        'project_essay_2',\n",
    "        'project_resource_summary'\n",
    "    ]\n",
    "\n",
    "    def total_typing_errors(df):\n",
    "        new_col_names = []\n",
    "        for col in COLUMNS_TO_CHECK:\n",
    "            new_col_name = col + '_typing_errors'\n",
    "            new_col_name = new_col_name[8:]  #drop 'project_' from the name to make it shorter\n",
    "            df[new_col_name] = df[col].str.count('=')\n",
    "            new_col_names.append(new_col_name)\n",
    "\n",
    "        df['total_typing_errors'] = df[new_col_names].sum(axis=1)\n",
    "\n",
    "        return\n",
    "\n",
    "    total_typing_errors(combined_training_dataset)\n",
    "    total_typing_errors(combined_test_dataset)\n",
    "    \n",
    "    #remove those = signs now:\n",
    "    for col_name in text_columns:\n",
    "        combined_training_dataset[col_name] = combined_training_dataset[col_name].replace('=', ' ', regex=True)\n",
    "        combined_test_dataset[col_name] = combined_test_dataset[col_name].replace('=', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "59629594-59bc-491f-b390-2280e24da578",
    "_uuid": "09cb916e4e6e152d5e2f655d4513d72321f24e66"
   },
   "source": [
    "Here's the full row for the example from earlier to check that the columns and values are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1ed7b5c0-0f63-4dec-9a14-b738e93adb21",
    "_uuid": "0a19f81932d94051db1d5df3296738234c9f71e4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset[3:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "eaae28f0-42c9-4730-a101-6a69c7c3af29",
    "_uuid": "4ab2717989e4bd7d369d0e18dd787e00ff9ef77e"
   },
   "source": [
    "And here's to check that the '=' errors have been removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "3f590c22-3e26-4b2a-9c13-4fe13fd6716d",
    "_uuid": "e36c9e8a362e264308492f92b603e55eccdb1965",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['project_essay_2'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "3704be68-6a4c-4c27-8eae-386b7f7cc7a8",
    "_uuid": "32cc68c52d3f9940bb19edd13d2e6cd3226df7ef"
   },
   "source": [
    "Now that the text is fully formatted, grab the unique words from each column to later compare to a dictionary, looking for potential misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "e435f38f-2aa5-4559-8006-8cdb7eeabac8",
    "_uuid": "543cb3334c3f8b4a42422acd29f76bbb0d849e2a"
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    join_cols = [\n",
    "        'project_title',\n",
    "        'project_essay_1',\n",
    "        'project_essay_2',\n",
    "        'project_resource_summary',\n",
    "        'full_description'\n",
    "    ]\n",
    "    #first, join the rows of the test dataset with the training dataset to get a full look at all the words in the data\n",
    "    all_dataset = pd.concat([combined_training_dataset[join_cols], combined_test_dataset[join_cols]])\n",
    "    \n",
    "    def get_unique_word_sets(df):\n",
    "        new_set = set()\n",
    "        df.str.split().apply(new_set.update)\n",
    "\n",
    "        return new_set\n",
    "\n",
    "    title_set = get_unique_word_sets(all_dataset['project_title'])  \n",
    "    essay_1_set = get_unique_word_sets(all_dataset['project_essay_1'])  \n",
    "    essay_2_set = get_unique_word_sets(all_dataset['project_essay_2']) \n",
    "    summary_set = get_unique_word_sets(all_dataset['project_resource_summary'])\n",
    "    description_set = get_unique_word_sets(all_dataset['full_description'])\n",
    "    print(len(title_set))\n",
    "    print(len(essay_1_set))\n",
    "    print(len(essay_2_set))\n",
    "    print(len(summary_set))\n",
    "    print(len(description_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "168ed8fa-d548-47bc-bf01-0374844d47f0",
    "_uuid": "e97a5f5bb01e94b6d28c6557deb59a5429818097"
   },
   "source": [
    "Turn the English Word Frequency data into a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f8078896-5b22-4345-80b1-5f886fa12143",
    "_uuid": "b076de75fed369075d3f432669533c5e81b76d5d"
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    #Contains 333,332 words\n",
    "    #And '!' as acceptable word\n",
    "    dictionary_set = set(dictionary_dataset['word'])\n",
    "    dictionary_set.add('!')\n",
    "    print(len(dictionary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "8c85151d-d91b-4a04-876b-7a8a6b068a3a",
    "_uuid": "d1db5ee836a1f36c790b5ec1dcfd3780d9e326eb"
   },
   "source": [
    "From each column's unique words, subract out the ones that do exist in the dictionary (using both the English Word Frequency data and the Unix data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "151c16b9-4740-40d4-b2ec-ee4358c48205",
    "_uuid": "4c15e3adee847332a6485f88c9cc71c4cab1805c"
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    title_set = title_set - dictionary_set - unix_set  \n",
    "    essay_1_set = essay_1_set - dictionary_set - unix_set\n",
    "    essay_2_set = essay_2_set - dictionary_set - unix_set\n",
    "    summary_set = summary_set - dictionary_set - unix_set\n",
    "    description_set = description_set - dictionary_set - unix_set\n",
    "    print(len(title_set))\n",
    "    print(len(essay_1_set))\n",
    "    print(len(essay_2_set))\n",
    "    print(len(summary_set))\n",
    "    print(len(description_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f35c6b29-3a99-4243-8f92-358aad1784f8",
    "_uuid": "070f6a4bee3466a34cf08ff664ca9310ff266a43"
   },
   "source": [
    "Most of the columns reference product names that aren't going to be in a dictionary.  Thus, work under the assumption that full_description is mostly product descriptions (and hopefully copied-and-pasted from a product page that has been proofread and doesn't have many misspellings itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "3819a2bb-01d0-4a67-8fdd-c9e376697a04",
    "_uuid": "51a1a80befd87f14d50233e3f81040bab9b26135"
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    title_set = title_set - description_set\n",
    "    essay_1_set = essay_1_set - description_set\n",
    "    essay_2_set = essay_2_set - description_set\n",
    "    summary_set = summary_set - description_set\n",
    "    print(len(title_set))\n",
    "    print(len(essay_1_set))\n",
    "    print(len(essay_2_set))\n",
    "    print(len(summary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "18503f5b-ed89-4aea-a27c-56a96c6380af",
    "_uuid": "4ca21891f5740a5e315b845a576703cead93e8c6"
   },
   "source": [
    "Lastly, I'm going to remove from the sets any word that contains a number in it.  More than likely, that's some sort of product name or acceptable abbreviation (like 3rd):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "a08e7611-45c3-4868-a809-920f25b5460e",
    "_uuid": "a8df02d0466031af33c083703419c9273ff8a67f"
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    def make_set_alpha(word_set):\n",
    "        words_to_drop = set()\n",
    "        for word in word_set:\n",
    "            if word.isalpha() == False:\n",
    "                words_to_drop.add(word)\n",
    "\n",
    "        return word_set - words_to_drop\n",
    "\n",
    "    title_set = make_set_alpha(title_set)\n",
    "    essay_1_set = make_set_alpha(essay_1_set)\n",
    "    essay_2_set = make_set_alpha(essay_2_set)\n",
    "    summary_set = make_set_alpha(summary_set)\n",
    "    print(len(title_set))\n",
    "    print(len(essay_1_set))\n",
    "    print(len(essay_2_set))\n",
    "    print(len(summary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "7b8e83bb-e272-4f57-98e6-b01e4782833f",
    "_uuid": "74a9492446b68c9d55313817ded0e98f17d9fd6c"
   },
   "source": [
    "Now, unfortunately, the dictionaries used in making these sets are not complete.  Thus, the above sets are still a mix of real words, product names, and misspelled words, so this won't be very accurate.  However, I'll still use them to count up the possibly misspelled words in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "3e6611d4-85d5-4339-8d31-8f40084cca13",
    "_uuid": "38687a3d3c2c1627d3feedefa1e5c7b9bbfd4556",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    #for each column is a matching set of 'misspelled' words:\n",
    "    COLS_AND_SETS = {\n",
    "        'project_title' : title_set,\n",
    "        'project_essay_1' : essay_1_set,\n",
    "        'project_essay_2' : essay_2_set,\n",
    "        'project_resource_summary' : summary_set\n",
    "    }\n",
    "    \n",
    "    def count_spelling_errors(string, word_set):\n",
    "        list_words = string.split()\n",
    "        error_count = 0\n",
    "        for word in list_words:\n",
    "            if word in word_set:\n",
    "                error_count += 1\n",
    "\n",
    "        return error_count\n",
    "\n",
    "    def total_spelling_errors(df):\n",
    "        new_col_names = []\n",
    "        for col, word_set in COLS_AND_SETS.items():\n",
    "            new_col_name = col + '_spelling_errors'\n",
    "            new_col_name = new_col_name[8:]  #drop 'project_' from the name to make it shorter\n",
    "            df[new_col_name] = df[col].apply(count_spelling_errors, args=(word_set,))\n",
    "            new_col_names.append(new_col_name)\n",
    "\n",
    "        df['total_spelling_errors'] = df[new_col_names].sum(axis=1)\n",
    "\n",
    "        return\n",
    "        \n",
    "    total_spelling_errors(combined_training_dataset)\n",
    "    total_spelling_errors(combined_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "e1b63b96-b516-45b6-b4d5-dadc4d902037",
    "_uuid": "e71369be6df6cb90bce81b39af126a94602515e3"
   },
   "source": [
    "Let's make sure all the columns got added correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "2f9e5ec5-f9b4-4cb8-8558-51bf5c580603",
    "_uuid": "908a695221e4091f6a1a8ffbac88641d38197ce7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    combined_training_dataset.loc[combined_training_dataset['total_spelling_errors'] > 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "85e63652-8dc3-42f6-b038-b93fae99059b",
    "_uuid": "bd518f488babcb1c4a267b2c405a92d7ab38fbb5"
   },
   "source": [
    "And let's look at my favorite essay in that bunch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "564f4668-4e09-466e-b4d4-438c0a66c5e2",
    "_uuid": "ae72b7d76bf3f1539c86575dc77b1eba12ec7715"
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['project_essay_2'].loc[110575]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "04ab2c15-f133-46e7-9ba6-376dd2a7fe31",
    "_uuid": "7ef8886bd03b0965fd6142f3dcf49566e15e4b0d"
   },
   "source": [
    "Nice.  And... just to make sure that randomly smashing the keyboard is *bad* for an application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f9a378bd-31a5-470f-be6f-0cf792e24657",
    "_uuid": "de0d54c5ca6c728582cfcc06d55751881b06a340",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['project_is_approved'].loc[110575]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1486347b-a39c-4be0-a63e-103a9f534cfa",
    "_uuid": "e3142880da15e1979ac09ffffcdee8afdabb3405"
   },
   "source": [
    "Okay!  But, sadly, since my dictionaries aren't complete, here's an example of one with a bunch of  \"errors\" that aren't really:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "ff107fc2-a753-4f6c-9a29-9c6662696296",
    "_uuid": "4a3586b7885f3fe862cc965a4b0b77fc34223617",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_training_dataset['project_essay_2'].loc[172786]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "56a70d7f-8822-49b3-be36-0a916998468d",
    "_uuid": "69eb1095b70cd096a9cab09e529918e2b5f508eb"
   },
   "source": [
    "That's full of a bunch of websites names (that weren't written as URLs) as well as product names that didn't show up in full_description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7dd372fe-caa1-4dc0-a2ed-2eab09cdca76",
    "_uuid": "165c523c55259c546c1ea528061ce03fe6aad8d1"
   },
   "source": [
    "*vi. Handling Date Information*\n",
    "<a id=\"dateinfo\"></a>\n",
    "\n",
    "After working more on this project, I've decided to incorporate the project_submitted_datetime as a feature.  Based on some data visualization later on, I've decided to split this information by week of the years this dataset spans.  I don't know if there's some preferred method for doing this, but I like the idea of converting these values to an integer like 'last two digits of year' * 1000 + week (for example, 16 * 1000 + 25 = 1625).  Since they're numbers, I originally thought about bucketizing them.  However, I wonder if treating them as strings and thus as a vocabulary_list would be a better representation.  I'm going with the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "80196164-fe5d-4446-a892-1d4b3ef62be1",
    "_uuid": "08668898fc7b0388b8538ff28528554d8a9e871d"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-from-pandas-datetime-column-python\n",
    "#https://stackoverflow.com/questions/17950374/converting-a-column-within-pandas-dataframe-from-int-to-string\n",
    "#First, get the project_submitted_datetime column into a format that Pandas can work with\n",
    "combined_training_dataset['project_submitted_datetime'] = pd.to_datetime(combined_training_dataset['project_submitted_datetime'])\n",
    "combined_training_dataset['year_week'] = combined_training_dataset['project_submitted_datetime'].map(lambda x: 100*(x.year - 2000) + x.week).apply(str)\n",
    "\n",
    "combined_test_dataset['project_submitted_datetime'] = pd.to_datetime(combined_test_dataset['project_submitted_datetime'])\n",
    "combined_test_dataset['year_week'] = combined_test_dataset['project_submitted_datetime'].map(lambda x: 100*(x.year - 2000) + x.week).apply(str)\n",
    "combined_training_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "558c7ff2-a001-4ad7-83cc-579d3d2fe6d1",
    "_uuid": "0450e2e2ecf3d3a6ddc3e50eba0549db2c155db0"
   },
   "source": [
    "**II.   Data Visualization**\n",
    "<a id=\"datavisual\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1ebfe513-fb80-4f3b-9b4e-6a0e6c0ca267",
    "_uuid": "1876143479747d2c5aaeb869775a8d389ab32185"
   },
   "source": [
    "Colaboratory has a [Welcome notebook](https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=yv2XIwi5hQ_g) with a small section on Visualization, but a more detailed [Charts notebook](https://colab.research.google.com/notebooks/charts.ipynb) with several Matplotlib examples can be found in the \"For more information\" section of that Welcome site.\n",
    "\n",
    "Much of the visualization in the Programming Exercises was already coded in, so.... I want to try making some of these charts for myself!\n",
    "\n",
    "You can find much nicer and more interesting charts in other kernels, so definitely check those out.  This is now just me messing around with code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ededde39-ff84-483b-9785-da27fb238d24",
    "_uuid": "730fbb39464a0c93a739ce7fd890c725bd249fff"
   },
   "source": [
    "**1.  Bar Chart** - *Number of Approved/Not-Approved Applications Based on teacher_prefix*\n",
    "<a id=\"barchart\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b4de09d9-754f-4512-ac0f-8653c70276fe",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1ce3484cd8f40f7926cf59b537774b18cefb8485",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "95a5026d-f506-41c8-8878-7d51f645ef00",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "c8c5150030f43b7dd94d73bded1beffa93d981e9",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Prefixes and Approved vs Unapproved Application\n",
    "grouped_prefixes = combined_training_dataset.groupby([\"teacher_prefix\", \"project_is_approved\"])\n",
    "grouped_prefixes = grouped_prefixes.agg({'teacher_prefix' : 'count'}).rename(columns={'teacher_prefix' : 'count'})\n",
    "grouped_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6ef6d724-94f2-4262-9031-778bdf56002a",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "93e2af742d2ceb2b73bcbe9796cf2ab15f872289",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr_counts = grouped_prefixes[\"count\"].tolist()\n",
    "#split arr_counts into two separate lists:\n",
    "y_no = []\n",
    "y_yes = []\n",
    "for i in range(0, len(arr_counts) - 1):\n",
    "    if i % 2 == 0:\n",
    "        y_no.append(arr_counts[i])\n",
    "    else:\n",
    "        y_yes.append(arr_counts[i])\n",
    "y_yes.append(arr_counts[i+1])\n",
    "y_no.append(0)\n",
    "x_labels = ['Dr', 'Mr.', 'Mrs', 'Ms', 'Teacher', 'None']\n",
    "\n",
    "#Make a multiple bar graph\n",
    "#via https://stackoverflow.com/questions/14270391/python-matplotlib-multiple-bars\n",
    "N = len(x_labels)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.4      # the width of the bars\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "rects1 = ax.bar(ind, y_yes, width, color='#7CFC00')\n",
    "rects2 = ax.bar(ind+width, y_no, width, color='#DC143C')\n",
    "\n",
    "ax.set_ylabel('Submissions Count')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.legend( (rects1[0], rects2[0]), ('Approved', 'NOT Approved') )\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2., 1.05*h, '%d'%int(h),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.title('Submissions Approved or NOT Approved Based on Name Prefix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6faf9a4b-4e85-40be-b14a-4732c6bfd808",
    "_uuid": "bdf6effa4bb71759669301cdb1607dbaec52c46e"
   },
   "source": [
    "And what are those percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "38a67378-b2ca-4e62-a4ae-d2aee46a268e",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "aa5f21558fd1a4c1298366d64ba0188a7fb347d9",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Chance to get approved by prefix\n",
    "print('Chance to get approved by prefix')\n",
    "arr_total_applications = []\n",
    "for i in range(0, len(y_yes)):\n",
    "    arr_total_applications.append(y_yes[i] + y_no[i])\n",
    "    print(x_labels[i] + ': ' + \"%f\" % (y_yes[i] / arr_total_applications[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "40afafc0-2745-4624-964e-c105fa0580b3",
    "_uuid": "7850996ce4c9eeafbb22e8b291689c2b80ea1099"
   },
   "source": [
    "So, for immediate acceptance, just don't submit a prefix on your application, obviously!  #percentagesneverlie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d190a1a7-dc64-40d9-9457-35915bd891b3",
    "_uuid": "6f0190eb9bb9b1612eee61191e3c4931a4d73cd1"
   },
   "source": [
    "**2. Scatter Plot** - *Acceptance Rate vs. Request Cost within a Binned Range*\n",
    "<a id=\"scatterplot\"></a>\n",
    "\n",
    "Curious to see if the acceptance rate differs as the cost of the request gets higher.  There are too many different values for the cost (100, 100.01, 100.02, etc.), so I'm also going to try binning with Pandas to group those costs into ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1064ed6c-5715-4b18-9c0c-168ed892a6ff",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "78637774c0a008645fc16892532a78e66355d098",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = [0, 150, 200, 250, 300, 350, 400, 500, 600, 750, 1000, 1000000]\n",
    "combined_training_dataset['binned_costs'] = pd.cut(combined_training_dataset['total_cost'], bins)\n",
    "\n",
    "grouped_costs = combined_training_dataset.groupby([\"binned_costs\", \"project_is_approved\"])\n",
    "grouped_costs = grouped_costs.agg({'binned_costs' : 'count'}).rename(columns={'binned_costs' : 'count'})\n",
    "\n",
    "arr_percent_yes = []\n",
    "for i in range(0, (len(bins) - 1) * 2):\n",
    "    if i % 2 == 0:\n",
    "        k = i // 2\n",
    "        label = 'no'\n",
    "        no_count = grouped_costs[\"count\"].values[i]\n",
    "        print(\"%d+ - %s: %d\" % (bins[k], label, no_count))\n",
    "    else:\n",
    "        label = 'yes'\n",
    "        yes_count = grouped_costs[\"count\"].values[i]\n",
    "        percent_yes = yes_count / (no_count + yes_count)\n",
    "        arr_percent_yes.append(percent_yes)\n",
    "        print(\"%d+ - %s: %d, percent: %f\" % (bins[k], label, yes_count, percent_yes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1dfc9dcd-ead1-4bc5-a2e1-9a7008504d68",
    "_kg_hide-input": true,
    "_uuid": "eea8f2969429e2dcf956a29a5e62c2b219b4cc76",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr_x_labels = []\n",
    "for i in range(0, len(bins)-2):\n",
    "    arr_x_labels.append(\"%d+\" % (bins[i]))\n",
    "arr_x_labels.append(\"_1000+\") #add the _ so that the plot doesn't alphabetize the numbers\n",
    " \n",
    "plt.scatter(arr_x_labels, arr_percent_yes)\n",
    "plt.xlabel(\"Cost of Request\")\n",
    "plt.ylabel(\"Yes Acceptance Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "2b978834-64d5-44d8-bc4f-2cb778221bcc",
    "_uuid": "7ab679bffb34e9fbad81347f52812945df9bff6e"
   },
   "source": [
    "I just sorta eyeballed the bin ranges.  Still, that downward trend seems sensible as I would expect a higher percentage of cheaper requests to get approved over much pricier ones.\n",
    "\n",
    "But I'm curious to try out a more official way (using quantiles to split the data into equal group sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "d3f4d971-75ed-4554-90fd-c7b4c35c0ef1",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "7f57ec3e6e948180997ca42d9cb203c0d726bc35",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#function code via https://colab.research.google.com/notebooks/mlcc/sparsity_and_l1_regularization.ipynb?hl=en#scrollTo=bLzK72jkNJPf\n",
    "def quantile_based_buckets(feature_values, num_buckets):\n",
    "    quantiles = feature_values.quantile(\n",
    "        [(i+1.)/(num_buckets + 1.) for i in range(num_buckets)])\n",
    "    return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "quantile_bins = quantile_based_buckets(combined_training_dataset[\"total_cost\"], 12)\n",
    "#^But why does the DataFrame later show NaN for the lowest range (from 0 to 133)?\n",
    "#...Looks like Pandas requires you to specify the lower/upper limits if you pass in a list to pd.cut?\n",
    "quantile_bins.insert(0, 0)\n",
    "quantile_bins.append(1000000)\n",
    "\n",
    "combined_training_dataset['quantile_costs'] = pd.cut(combined_training_dataset['total_cost'], quantile_bins)\n",
    "\n",
    "grouped_costs = combined_training_dataset.groupby([\"quantile_costs\", \"project_is_approved\"])\n",
    "grouped_costs = grouped_costs.agg({'quantile_costs' : 'count'}).rename(columns={'quantile_costs' : 'qcount'})\n",
    "\n",
    "arr_percent_yes = []\n",
    "for i in range(0, (len(quantile_bins) - 1) * 2):\n",
    "    if i % 2 == 0:\n",
    "        k = i // 2\n",
    "        label = 'no'\n",
    "        no_count = grouped_costs[\"qcount\"].values[i]\n",
    "        print(\"%d+ - %s: %d\" % (quantile_bins[k], label, no_count))\n",
    "    else:\n",
    "        label = 'yes'\n",
    "        yes_count = grouped_costs[\"qcount\"].values[i]\n",
    "        percent_yes = yes_count / (no_count + yes_count)\n",
    "        arr_percent_yes.append(percent_yes)\n",
    "        print(\"%d+ - %s: %d, percent: %f\" % (quantile_bins[k], label, yes_count, percent_yes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6328ea56-6834-48b4-a9c5-7c198cc4a06d",
    "_kg_hide-input": true,
    "_uuid": "d210c5ea50757f4508844c8ab7b9e5fd7a53e725",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr_x_labels = []\n",
    "for i in range(0, len(quantile_bins)-2):\n",
    "    arr_x_labels.append(\"%d+\" % (quantile_bins[i]))\n",
    "arr_x_labels.append(\"_1000+\") #add the _ so that the plot doesn't alphabetize the numbers\n",
    " \n",
    "plt.scatter(arr_x_labels, arr_percent_yes)\n",
    "plt.xlabel(\"Cost of Request\")\n",
    "plt.ylabel(\"Yes Acceptance Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3201dd3-f8ae-4993-9276-b836f48941d9",
    "_uuid": "f71560d931b287e90102a65e16a573eee8c2c7c9"
   },
   "source": [
    "**3. Line Plot** - *Chance of Acceptance Based on Number of Previous Submissions from 0 to 100*\n",
    "<a id=\"lineplot\"></a>\n",
    "\n",
    "Out of curiosity, I'm also interested in odds based on previous submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "97525814-6841-46cb-9dbe-530e5942206b",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "a9039e044336aeec10f6e605012e8a6000012a61",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_previous_submissions = combined_training_dataset.groupby([\"teacher_number_of_previously_posted_projects\", \"project_is_approved\"])\n",
    "grouped_previous_submissions = grouped_previous_submissions.agg({'teacher_number_of_previously_posted_projects' : 'count'}).rename(columns={'teacher_number_of_previously_posted_projects' : 'count'})\n",
    "grouped_previous_submissions[0:202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "95746a1d-df2e-4cd4-960a-e302d69ad634",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "5372d5b217dd46c2d4eebfe5543bc69ba0f14a81",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr_percents_yes = []\n",
    "for i in range(0,202):\n",
    "    if i % 2 == 0:\n",
    "        k = i / 2\n",
    "        label = 'no'\n",
    "        no_count = grouped_previous_submissions[\"count\"].values[i]\n",
    "        print(\"%d - %s: %d\" % (k, label, no_count))\n",
    "    else:\n",
    "        label = 'yes'\n",
    "        yes_count = grouped_previous_submissions[\"count\"].values[i]\n",
    "        percent_yes = yes_count / (no_count + yes_count)\n",
    "        arr_percents_yes.append(percent_yes)\n",
    "        print(\"%d - %s: %d, percent: %f\" % (k, label, yes_count, percent_yes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "e9c8635a-1a38-45de-bdb9-a1c2f41daaeb",
    "_kg_hide-input": true,
    "_uuid": "cdb560c9bbc3fb20d9f06c84b89b8dd585047cf8",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_num_previous = np.arange(0, 101)\n",
    "#y is arr_percents_yes\n",
    "\n",
    "plt.plot(x_num_previous, arr_percents_yes)\n",
    "\n",
    "plt.xlabel(\"Number of Previous Submissions\")\n",
    "plt.ylabel(\"Chance of Accepted Submission\")\n",
    "plt.title(\"Chance of Accepted Submission Based on Previous Number of Submissions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "a0ea98ff-5ccd-48dc-bdf5-1429517714ef",
    "_uuid": "dd0614fe679579ab0ea96a4de219e461071b53c5"
   },
   "source": [
    "An expected general upward trend that tapers off (Note: higher x values [say, 30+] have far less entries, especially the much higher x values).  Also interesting that the chance of acceptance for a new applicant is quite high (~82%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e1d4d9c-3ecd-4c72-9942-5888b3401155",
    "_uuid": "f8c7f5c3e9572129f2c303f43fffe317b85bd223"
   },
   "source": [
    "**4. Stacked-Bars Bar Chart** - *Comparing the Top 5 vs Bottom 5 States in Terms of Acceptance Rate*\n",
    "<a id=\"stackedbars\"></a>\n",
    "\n",
    "And why not look at the states, too?  (Also going to try getting the *groupby* to act like a normal DataFrame this time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "691a2e88-2026-42cb-992a-487b73de5715",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "ba4ca57c9bd327440a4584211a9906bc3ac9cac4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_states = combined_training_dataset.groupby([\"school_state\", \"project_is_approved\"])\n",
    "#a way to eliminate the multi-index?:\n",
    "#https://stackoverflow.com/questions/39778686/pandas-reset-index-after-groupby-value-counts\n",
    "grouped_states = grouped_states.size().rename('count').reset_index()\n",
    "\n",
    "arr_percents = []\n",
    "for i in range(0,102):\n",
    "    if i % 2 == 0:\n",
    "        no_count = grouped_states[\"count\"].values[i]\n",
    "    else:\n",
    "        yes_count = grouped_states[\"count\"].values[i]\n",
    "        total_count = no_count + yes_count\n",
    "        percent_no = no_count / total_count\n",
    "        percent_yes = yes_count / total_count\n",
    "        arr_percents.append(percent_no)\n",
    "        arr_percents.append(percent_yes)\n",
    "        \n",
    "grouped_states['chances'] = pd.Series(arr_percents, index=grouped_states.index)\n",
    "grouped_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "dce0bf32-e90b-4ceb-90d6-ecf7fdbb3d89",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "e5e1af50900bedf1f62ac2481a9a4c1eb7572d29",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Find the lowest approval rates:\n",
    "grouped_states.loc[(grouped_states['project_is_approved'] == 1) & (grouped_states['chances'] < .83)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6cd895cc-884c-4723-b4a5-c207074e40aa",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "761f4a34429cb5d23c56d028aae0c24b3690039b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#And the highest ones:\n",
    "grouped_states.loc[(grouped_states['project_is_approved'] == 1) & (grouped_states['chances'] > .868)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "9be65fb6-b260-4203-803f-3a667a465074",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "911932a2786eda7509410f6524f8b68a40cfd872",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Make a stacked bar chart of Top 5 vs Bottom 5 acceptance rates because...  I just wanna see what it looks like...\n",
    "\n",
    "idxes = ['1 DE/DC', '2 WY/TX', '3 OH/NM', '4 CT/FL', '5 WA/MT']\n",
    "lowest = [.812639, .815670, .822052, .824500, .828125]\n",
    "highest = [.891341, .875706, .871467, .871294, .868050]\n",
    "\n",
    "#apparently the 2nd bar just paints over the first, so the 2nd must be smaller or it gets hidden\n",
    "plt.bar(idxes, highest, label=\"DE, WY, OH, CT, WA\", color='#87CEFA')\n",
    "plt.bar(idxes, lowest, label=\"DC, TX, NM, FL, MT\", color='#B22222')\n",
    "\n",
    "plt.plot()\n",
    "\n",
    "#make the scale more useful\n",
    "#https://stackoverflow.com/questions/3777861/setting-y-axis-limit-in-matplotlib\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([.75, .92])\n",
    "\n",
    "plt.title('Submission Acceptance Rates for Top 5 States vs Bottom 5 States')\n",
    "plt.legend()\n",
    "plt.xlabel('State')\n",
    "plt.ylabel(\"Chance of Accepted Submission\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6eb8be3-13fa-4b58-b9b8-45a9410143a3",
    "_uuid": "81e7a07aeb49b9a2e483579f69f8051dbbafaf94"
   },
   "source": [
    "**5. Pie Chart** - *Percentage of Applications Accepted and NOT Accepted According to Grade*\n",
    "<a id=\"piechart\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "9d212f15-dc76-4d4d-86fa-5eb3d0cd8c19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1a1d54b70f626f55844f28efd3683518eb571e80",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_grades = combined_training_dataset.groupby([\"project_grade_category\", \"project_is_approved\"])\n",
    "#a way to eliminate the multi-index?:\n",
    "#https://stackoverflow.com/questions/39778686/pandas-reset-index-after-groupby-value-counts\n",
    "grouped_grades = grouped_grades.size().rename('count').reset_index()\n",
    "grouped_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "088e4d62-8f8f-430b-850d-decafdc15392",
    "_kg_hide-input": true,
    "_uuid": "f49cbac9c5689166202b5a9da600da5e863ed684",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arr_no = []\n",
    "arr_yes = []\n",
    "arr_labels = []\n",
    "arr_all = []\n",
    "for i in range(0, len(grouped_grades[\"project_grade_category\"].values)):\n",
    "    if i % 2 == 0:\n",
    "        arr_labels.append(\"%s - No\" % (grouped_grades[\"project_grade_category\"].values[i]))\n",
    "        arr_no.append(grouped_grades[\"count\"][i])\n",
    "    else:\n",
    "        arr_labels.append(\"%s - Yes\" % (grouped_grades[\"project_grade_category\"].values[i]))\n",
    "        arr_yes.append(grouped_grades[\"count\"][i])\n",
    "    arr_all.append(grouped_grades[\"count\"][i])\n",
    "\n",
    "colors = ['#DC143C', '#7CFC00']\n",
    "plt.pie(arr_all, labels=arr_labels, colors=colors,\n",
    "        startangle=50,\n",
    "        explode = (.3, .3, .3, .3, .3, .3, .3, .3),\n",
    "        autopct = '%1.2f%%',\n",
    "        shadow=True)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Pie Chart Example')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "15f0f17e-ee6e-4f71-b130-773f573e78cd",
    "_uuid": "66e096896cc8e702c9089dd3450da5e79b91bcda"
   },
   "source": [
    "Would be cool to split the pie into grouped pieces (for example, Grades 3-5 Yes and No exploding off as a combined piece), but [that looks nontrivial](http://https://stackoverflow.com/questions/20549016/explode-multiple-slices-of-pie-together-in-matplotlib/20556088)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff333a4d-47ad-4968-bebc-a142fcd9b2a1",
    "_uuid": "bf6f8f57771cfd82264b68d4d3faba142140daf8"
   },
   "source": [
    "**6. More Line Plots to Look at Dates (Month, Week, Weekday)**\n",
    "<a id=\"datedata\"></a>\n",
    "\n",
    "I'm curious if it looks like the date the application was submitted may have any impact on its approval.  If so, this could be a good feature to incorporate.\n",
    "\n",
    "This requires a couple new Pandas functions.  The first is pd.to_datetime to get the \"project_submitted-datetime\" column into a format that Pandas can work with.  To group by month, the groupby function is going to take a pd.Grouper() in place of a typical column name's string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "bbe17e00-3fae-4d9d-b726-da345c1d6cd2",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1783c8368dafc9335386fbd3821efc95ccbaae12",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combined_training_dataset['project_submitted_datetime'] = pd.to_datetime(combined_training_dataset['project_submitted_datetime'])\n",
    "  #^done in an earlier step when preprocessing the data\n",
    "#try grouping by month:\n",
    "#https://stackoverflow.com/questions/44908383/how-can-i-group-by-month-from-a-date-field-using-python-pandas\n",
    "\n",
    "grouped_months = combined_training_dataset.groupby([pd.Grouper(key='project_submitted_datetime', freq='1M'), \"project_is_approved\"])\n",
    "grouped_months = grouped_months.size().rename('count').reset_index()\n",
    "\n",
    "arr_percents_month = []\n",
    "for i in range(0,26):\n",
    "    if i % 2 == 0:\n",
    "        no_count = grouped_months[\"count\"].values[i]\n",
    "    else:\n",
    "        yes_count = grouped_months[\"count\"].values[i]\n",
    "        total_count = no_count + yes_count\n",
    "        percent_no = no_count / total_count\n",
    "        percent_yes = yes_count / total_count\n",
    "        arr_percents_month.append(percent_no)\n",
    "        arr_percents_month.append(percent_yes)\n",
    "        \n",
    "grouped_months['chances'] = pd.Series(arr_percents_month, index=grouped_months.index)\n",
    "grouped_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "7f4780a9-c0de-4bbf-962b-ee619b4f1fc7",
    "_uuid": "571e19a219f75deb038aa8d7eb554fca620aacfa"
   },
   "source": [
    "And there's definitely some variation, so let's graph that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "2ac8be06-b725-478d-b7dc-469a6afc1e55",
    "_kg_hide-input": true,
    "_uuid": "2c3313a3808f50ab372101230c8f954b59675fbb",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_num_previous_m = np.arange(0, 13)\n",
    "\n",
    "arr_percents_m_yes = []\n",
    "for i in range(0,26):\n",
    "    if not (i % 2 == 0):\n",
    "        arr_percents_m_yes.append(arr_percents_month[i])\n",
    "\n",
    "\n",
    "plt.plot(x_num_previous_m, arr_percents_m_yes)\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Chance of Accepted Submission\")\n",
    "plt.title(\"Chance of Accepted Submission Based on Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "a9427451-a5a4-445c-b638-8f485b19afc8",
    "_uuid": "70ceb96acaa1b3440e7d131b21bb7280b186ba9b"
   },
   "source": [
    "So it might be useful to bucketize the data by month.  But would weekly look any different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "de91162c-3f49-4fde-94af-fd16f81e0586",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "7b0ab2e512530a6aa6e99c7a4e2afc022600984b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#only difference here is freq='1W' instead of '1M'\n",
    "grouped_weeks = combined_training_dataset.groupby([pd.Grouper(key='project_submitted_datetime', freq='1W'), \"project_is_approved\"])\n",
    "grouped_weeks = grouped_weeks.size().rename('count').reset_index()\n",
    "\n",
    "arr_percents_w = []\n",
    "for i in range(0,106):\n",
    "    if i % 2 == 0:\n",
    "        no_count = grouped_weeks[\"count\"].values[i]\n",
    "    else:\n",
    "        yes_count = grouped_weeks[\"count\"].values[i]\n",
    "        total_count = no_count + yes_count\n",
    "        percent_no = no_count / total_count\n",
    "        percent_yes = yes_count / total_count\n",
    "        arr_percents_w.append(percent_no)\n",
    "        arr_percents_w.append(percent_yes)\n",
    "        \n",
    "grouped_weeks['chances'] = pd.Series(arr_percents_w, index=grouped_weeks.index)\n",
    "grouped_weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "53da8f72-4b80-4ef7-8d08-4c2d25eea803",
    "_uuid": "1e25dbd9be8477a7ec3e22dc7f24c2431e1a000f"
   },
   "source": [
    "And graphing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "443c7ba7-d4bc-4e9d-9788-420e0b2315fb",
    "_kg_hide-input": true,
    "_uuid": "12a7da66682db749a07c45730946bd996e65c989",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_num_previous_w = np.arange(0, 53)\n",
    "\n",
    "arr_percents_yes_w = []\n",
    "for i in range(0,106):\n",
    "    if not (i % 2 == 0):\n",
    "        arr_percents_yes_w.append(arr_percents_w[i])\n",
    "\n",
    "\n",
    "plt.plot(x_num_previous_w, arr_percents_yes_w)\n",
    "\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Chance of Accepted Submission\")\n",
    "plt.title(\"Chance of Accepted Submission Based on Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "236e9360-3bef-41e2-ac77-f32d7567c9c5",
    "_uuid": "1314c898e00978680117ff8b2fdd026fe515f5d9"
   },
   "source": [
    "Perhaps monthly would do, but I'm going to go with separating by week since that one stretch has some pretty wild swings.\n",
    "\n",
    "I also checked to see if the day of the week an application was submitted mattered, but the odds seemed pretty much the same for each weekday.  The code for that is in the hidden snippet if you're curious to see how it's implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b132d767-12eb-4f79-afcc-d568800fa440",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "c3fbf2e872022b9dde0f4bc12f27cbaa889b0afd",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#going to also try by day of the week:\n",
    "#https://stackoverflow.com/questions/13740672/in-pandas-how-can-i-groupby-weekday-for-a-datetime-column\n",
    "combined_training_dataset['weekday'] = combined_training_dataset['project_submitted_datetime'].dt.weekday\n",
    "\n",
    "grouped_weekday = combined_training_dataset.groupby([\"weekday\", \"project_is_approved\"])\n",
    "#grouped_dates = grouped_dates.agg({'project_submitted_datetime' : 'count'})\n",
    "grouped_weekday = grouped_weekday.size().rename('count').reset_index()\n",
    "\n",
    "arr_percents_weekday = []\n",
    "for i in range(0,14):\n",
    "    if i % 2 == 0:\n",
    "        no_count = grouped_weekday[\"count\"].values[i]\n",
    "    else:\n",
    "        yes_count = grouped_weekday[\"count\"].values[i]\n",
    "        total_count = no_count + yes_count\n",
    "        percent_no = no_count / total_count\n",
    "        percent_yes = yes_count / total_count\n",
    "        arr_percents_weekday.append(percent_no)\n",
    "        arr_percents_weekday.append(percent_yes)\n",
    "        \n",
    "grouped_weekday['chances'] = pd.Series(arr_percents_weekday, index=grouped_weekday.index)\n",
    "grouped_weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bb39eae1-6d05-4d93-856d-4aa18493af00",
    "_uuid": "14e8429547e4c4b7fbfee1a85c84ca4477f0ffcb"
   },
   "source": [
    "**7. Violin Plots (Using Spelling/Typing Error Data)**\n",
    "<a id=\"violin\"></a>\n",
    "\n",
    "In light of the new 'spelling' error data from earlier, wanted to see if that had any effect at all on application approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1915e110-656b-4302-b131-ea8476e82e44",
    "_uuid": "01c1dc9f12080134d3c6f0fe9b23e1338dc9cf2d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #convert data back to int:\n",
    "    pd_plot = combined_training_dataset[['total_spelling_errors', 'total_typing_errors', 'project_is_approved']].copy()\n",
    "    pd_plot['total_spelling_errors'] = pd.to_numeric(pd_plot['total_spelling_errors'])\n",
    "    pd_plot['total_typing_errors'] = pd.to_numeric(pd_plot['total_typing_errors'])\n",
    "    \n",
    "    fig, axes = plt.subplots()\n",
    "    # plot violin. 'project_is_approved' is according to x axis, \n",
    "    # 'comma_count' is y axis, data is training_dataset. ax - is axes instance\n",
    "    sns.violinplot('project_is_approved','total_spelling_errors', data=pd_plot, ax = axes)\n",
    "    axes.set_title('Spelling Errors vs Approval')\n",
    "\n",
    "    axes.yaxis.grid(True)\n",
    "    axes.set_xlabel('project_is_approved')\n",
    "    axes.set_ylabel('total-spelling_errors')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6d2a5fd8-0c47-49c1-9e17-bf9f8a6c02a2",
    "_uuid": "93ddcde17eb0e18b8dea9865445715141c897d1a"
   },
   "source": [
    "But, as with everything else, it seems to not make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "92648dd9-c392-40be-8286-001b8dea68ab",
    "_uuid": "9016a9b03042c2d9c614f312bc1d57d287189bfc",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_DICTIONARY_CODE:\n",
    "    fig, axes = plt.subplots()\n",
    "    # plot violin. 'project_is_approved' is according to x axis, \n",
    "    # 'comma_count' is y axis, data is training_dataset. ax - is axes instance\n",
    "    sns.violinplot('project_is_approved','total_typing_errors', data=pd_plot, ax = axes)\n",
    "    axes.set_title('Typing Errors vs Approval')\n",
    "\n",
    "    axes.yaxis.grid(True)\n",
    "    axes.set_xlabel('project_is_approved')\n",
    "    axes.set_ylabel('total-typing_errors')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7edabbb6-5af1-46aa-a29d-456b85e41c69",
    "_uuid": "a91f714080c2de3ddbe5354af16da7f3219a3021"
   },
   "source": [
    "**III.  Applying the Crash Course to this Project to Create a Linear Classifier (Does NOT Use Text-Heavy Features)**\n",
    "<a id=\"linearclassifier\"></a>\n",
    "\n",
    "I was looking over my course notes, and the code for just about everything I want to incorporate into a model that uses a linear classifier can be found in the [Programming Exercises from Chapter 13 - Regularization: Sparsity](https://colab.research.google.com/notebooks/mlcc/sparsity_and_l1_regularization.ipynb?hl=en):\n",
    "\n",
    "- Randomizing the data; stressed in Chapter 6\n",
    "- Splitting the data into training and validation sets; stressed in Chapter 5\n",
    "- Choosing good features (binning/bucketizing, scaling, handling outliers); stressed in Chapter 8\n",
    "- Setting up everything related to training (feature columns, input functions, training, predictions); from throughout the course\n",
    "- Regularization; from Chapters 10, 11, and 13 (L1 Regularization)\n",
    "\n",
    "The only thing missing is the code for determining the ROC and AUC, which can be found in the [Programming Exercises from Chapter 12 - Classification](https://colab.research.google.com/notebooks/mlcc/logistic_regression.ipynb?hl=en).\n",
    "\n",
    "Thus, I'll be working off the code from Chapter 13, and I'll try to keep it organized in a similar flow so that it looks familiar.\n",
    "\n",
    "Now, why the note about not using text-heavy features?  Well, first I wanted to start small and just use features that would be easier to work with (total_cost, school_state, etc.).  Then, once I did want to incorporate features like the project_essays, I couldn't get it to work!  See [Section IV](#bruteforcebaby) for how I had to change how I stored and loaded the text-heavy data differently into TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "12adaf00-814e-42e5-b053-a4d61f334e94",
    "_uuid": "bb748901625aab0644257033d4696596e514cc6a"
   },
   "source": [
    "**1. Randomizing the Data**\n",
    "<a id=\"random\"></a>\n",
    "\n",
    "The data here looks like it's already been randomized, but just to be safe, I'll go ahead and make sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "73510fdd-4d9f-4a41-a3c0-8f253a05fa37",
    "_uuid": "299edb1714f869b69154064c62a6f33c749a2fcb",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data sets are:\n",
    "# combined_training_dataset\n",
    "# combined_test_dataset\n",
    "\n",
    "combined_training_dataset = combined_training_dataset.reindex(\n",
    "    np.random.permutation(combined_training_dataset.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f7162ed-09ae-4a6f-9b28-801605d10509",
    "_uuid": "48e50fa0bfc0e093a73f275edfdbf5312cc6d62c"
   },
   "source": [
    "**2. Creating Training and Validation Sets**\n",
    "<a id=\"sets\"></a>\n",
    "\n",
    "(Note:  I'll be skipping the \"preprocessing features functions\" from those exercises since that stuff was basically handled in the Getting Started section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "168de3bd-43b9-4659-9ed0-d52a47f753ed",
    "_uuid": "d56cad7357be930e030c700ca1cf2e4dab8da389",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "USING_OLD_MODELS = False\n",
    "\n",
    "#combined_training_dataset.shape  -->  182,080 examples\n",
    "#split 80% training, 20% validation --> 145,664 training, 36,416 validation\n",
    "HEAD = 145664\n",
    "TAIL = 36416\n",
    "TARGET_STR = \"project_is_approved\"\n",
    "\n",
    "training_examples = combined_training_dataset.head(HEAD)\n",
    "validation_examples = combined_training_dataset.tail(TAIL)\n",
    "\n",
    "if USING_OLD_MODELS:  \n",
    "    #new models = don't separate examples from targets here\n",
    "    #old models require targets to be separate:\n",
    "    training_targets = combined_training_dataset[[TARGET_STR]].head(HEAD)\n",
    "    validation_targets = combined_training_dataset[[TARGET_STR]].tail(TAIL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2f2a773-dc70-4c4d-b0ba-aded87867182",
    "_uuid": "507a763896b09f3b21ec8f9f2226a36c92a30047"
   },
   "source": [
    "**3. Choosing Good Features - (DESIRED_FEATURES const)**\n",
    "<a id=\"desiredfeatures\"></a>\n",
    "\n",
    "This is where my code will differ some from the Exercises.  I plan to experiment with the different features to see which I really want to include.  However, doing that in the Exercises can involve editing or commenting out a lot of code.  So instead I just want to make a list of \"desired features\"  and then make that the only thing I'll have to update.\n",
    "\n",
    "*Note:  If you want to play around with this Notebook, one option is to include ALL the features on your first run.  That way, all the data is saved (later on) into a TFRecord from the start.  Then, the only thing you'll have to change is this DESIRED_FEATURES list to experiment with different features.)\n",
    "\n",
    "[Take this link to quickly jump back down to training a Linear Classifier with heavy-text Features.](#lineartexttraining)\n",
    "\n",
    "[Or this link to jump back down to training the DNN Classifier.](#dnntraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6ce84288-6de9-4551-bb53-80a272f761d0",
    "_kg_hide-output": false,
    "_uuid": "7ee17a316e451b2b9ffc90d7de69d26cd19ad190",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS\n",
    "#USING_OLD_MODELS = False  #(located in cell above)\n",
    "\n",
    "# ***  DESIRED_FEATURES  ***\n",
    "#To make it easier to swap features in and out (and even create and toss in new ones), many functions throughout\n",
    "#the rest of this Notebook only require you to update this DESIRED_FEATURES list.\n",
    "# DESIRED_FEATURES is a list whose elements are a tuple of the form:\n",
    "#       (feature_name, feature_type, num_bucket), so (str, str, int)\n",
    "# feature_type - types of features include:\n",
    "WORDS_STRING = 'words_string' #for strings that should be looked at as individual words\n",
    "WHOLE_STRING = 'whole_string' # for strings that should NOT be looked at as individual words\n",
    "BUCKET_INT = 'bucket_int' # ints that will be bucketized\n",
    "BUCKET_FLOAT = 'bucket_float' # floats that will be bucketized\n",
    "NUM_INT = 'num_int' #treat the int as a normal, individual number\n",
    "NUM_FLOAT = 'num_float' # treat the float as a normal, individual number\n",
    "\n",
    "# num_bucket is meant only for the bucket_int and bucket_float features; how many buckets to split the data into\n",
    "# alternatively, you can set num_bucket equal to an array of your choosing for the buckets, like:  [1.0, 2.0, 5.0]\n",
    "ERROR_BUCKETS = [0, 1, 2, 4, 7, 10, 20, 50]\n",
    "\n",
    "if USING_OLD_MODELS:\n",
    "    DESIRED_FEATURES = [\n",
    "            #'id', not used, not handled\n",
    "            #'teacher_id', not used, not handled\n",
    "    ('teacher_prefix', WHOLE_STRING, None),\n",
    "    ('school_state', WHOLE_STRING, None),\n",
    "            #'project_submitted_datetime', not used\n",
    "    #('project_grade_category', WHOLE_STRING, None),\n",
    "            #('project_subject_categories', WORDS_STRING, None), not handled in old model\n",
    "            #('project_subject_subcategories', WORDS_STRING), not handled in old model\n",
    "            #('project_title', WORDS_STRING), not handled in old model\n",
    "            #('project_essay_1', WORDS_STRING), not handled in old model\n",
    "            #('project_essay_2', WORDS_STRING, None), not handled in old model\n",
    "            #'project_essay_3', not used, not handled\n",
    "            #'project_essay_4', not used, not handled\n",
    "            #('project_resource_summary', WORDS_STRING), not handled in old model\n",
    "    ('teacher_number_of_previously_posted_projects', BUCKET_INT, 6),\n",
    "            #('full_description', WORDS_STRING), not handled in old model\n",
    "    #('total_quantity', BUCKET_INT, 10),\n",
    "    #('total_cost', BUCKET_FLOAT, 12),\n",
    "    #('year_week', WHOLE_STRING, None),  #effectively takes the place of project_submitted_datetime\n",
    "    ('project_is_approved', NUM_INT, None) #target\n",
    "]\n",
    "else:\n",
    "    DESIRED_FEATURES = [\n",
    "            #'id', not used, not handled\n",
    "            #'teacher_id', not used, not handled\n",
    "    #('teacher_prefix', WHOLE_STRING, None),\n",
    "    #('school_state', WHOLE_STRING, None),\n",
    "            #'project_submitted_datetime', not used, not handled\n",
    "    #('project_grade_category', WHOLE_STRING, None),\n",
    "    #('project_subject_categories', WORDS_STRING, None), #because some belong to multiple categories\n",
    "    #('project_subject_subcategories', WORDS_STRING, None),\n",
    "    #('project_title', WORDS_STRING, None),\n",
    "    ('project_essay_1', WORDS_STRING, None),\n",
    "    ('project_essay_2', WORDS_STRING, None),\n",
    "            #'project_essay_3', not used, not handled\n",
    "            #'project_essay_4', not used, not handled\n",
    "    #('project_resource_summary', WORDS_STRING, None),\n",
    "    ('teacher_number_of_previously_posted_projects', BUCKET_INT, 6),\n",
    "    ('full_description', WORDS_STRING, None),\n",
    "    #('total_quantity', BUCKET_INT, 10),\n",
    "    ('total_cost', BUCKET_FLOAT, 12),\n",
    "    #('year_week', WHOLE_STRING, None),  #effectively takes the place of project_submitted_datetime\n",
    "#POSSIBLE NEW FEATURES:  (ints are now WHOLE_STRING since treated like vocabulary list)\n",
    "    #('comma_count',  WHOLE_STRING, 6),\n",
    "    #('hyphen_count', WHOLE_STRING, 2),\n",
    "    #('sentence_count', WHOLE_STRING, 6),\n",
    "    #('avg_sentence_length', BUCKET_FLOAT, 5),\n",
    "    #('plural_pronouns', WHOLE_STRING, 5),\n",
    "    #('wrong_article', WHOLE_STRING, 1),  #the error ones = don't have enough to make buckets\n",
    "    #('cap_errors', WHOLE_STRING, 1),\n",
    "    #('singular_pronouns', WHOLE_STRING, 5),\n",
    "    #('repeat_words', WHOLE_STRING, 1),\n",
    "    #('avg_word_size', BUCKET_FLOAT, 6),\n",
    "    #('num_charged_words', NUM_INT, 1),\n",
    "#NEW ERROR-RELATED FEATURES:\n",
    "    #('title_typing_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('title_spelling_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    #('essay_1_typing_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('essay_1_spelling_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    #('essay_2_typing_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('essay_2_spelling_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    #('resource_summary_typing_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    #('resource_summary_spelling_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('total_typing_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('total_spelling_errors', BUCKET_INT, ERROR_BUCKETS),\n",
    "    ('project_is_approved', NUM_INT, None) #target\n",
    "]\n",
    "\n",
    "\n",
    "#some functions (like the ones saving to a TFRecord) just want the string column names, so grab those:\n",
    "DESIRED_COLUMNS = []\n",
    "for str_name, str_type, dontcare in DESIRED_FEATURES:\n",
    "    DESIRED_COLUMNS.append(str_name)\n",
    "\n",
    "STR_TARGET = 'project_is_approved' #target, which needs to be singled out and kept out of some functions\n",
    "\n",
    "# construct_feature_columns (shown later) is designed to work with either the Linear Classifer\n",
    "# or the DNN Classifier depending on the following constant\n",
    "# That is, if training the DNN Classifier, turn categorical_column_with_vocabulary_list into embeddings:\n",
    "IS_DNN_CLASSIFIER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "45763a5e-a0c0-451a-ac9c-cf78435838ed",
    "_uuid": "46fda29a6b79e422e904ca8e4a75c7198389d3d0"
   },
   "source": [
    "*Bucketizing*\n",
    "\n",
    "I'm not currently planning on using any continuous numeric data (though, the code does handle and account for that type of data and feature column).  Instead, I'll be bucketizing them, and here's the function to create quantile-based buckets:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "5aeafd25-a788-4756-abf8-aa99fd4a9ace",
    "_uuid": "47cc72e3a8d8a12e034a60355914f3ab6d7e7965",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_quantile_based_buckets(feature_values, num_buckets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature_values:  Pandas DataFrame (one column)\n",
    "        num_buckets:  how many buckets\n",
    "        \n",
    "    Returns:\n",
    "        An array of the quantiles\n",
    "    \"\"\"\n",
    "    quantiles = feature_values.quantile(\n",
    "        [(i+1.)/(num_buckets + 1.) for i in range(num_buckets)])\n",
    "    return [quantiles[q] for q in quantiles.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6c56d8d4-84ee-428d-ae6e-179cedad30b1",
    "_uuid": "612fd270dc368694c6464427af605172e55f060a"
   },
   "source": [
    "I'd recommend looking at the quantile buckets that get created.  If it's not in an ascending order with *unique* numbers at each position, TensorFlow can't handle it and training crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "7c381ce1-6cb0-4d37-874b-8c9953e1f045",
    "_uuid": "c9e8adc52d62197e7bee93c5ecca5c0ada25afa1"
   },
   "outputs": [],
   "source": [
    "for str_name, str_type, num_bucket in DESIRED_FEATURES:\n",
    "    print(str_name)\n",
    "    if (str_type == BUCKET_INT) or (str_type == BUCKET_FLOAT):\n",
    "        if (type(num_bucket) == list) == False:\n",
    "            print(get_quantile_based_buckets(training_examples[str_name], num_bucket))\n",
    "        else:\n",
    "            print(num_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49c2a825-2db7-42e7-9cc8-de2b5a632598",
    "_uuid": "5de294a64b5a00eb430cb4fa7143dc03ff405c87"
   },
   "source": [
    "**4. Setting Up for Training**\n",
    "<a id=\"trainingsetup\"></a>\n",
    "\n",
    "First up is the general input function to create more specific input functions for training vs validation vs prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "6935b639-e2ef-4dad-a348-c3099ecd9441",
    "_uuid": "6ff543edebe76c90a5fb7fa02fe5f21f89ef66b5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        features: pandas DataFrame of features\n",
    "        targets: pandas DataFrame of targets\n",
    "        batch_size: Size of batches to be passed to the model\n",
    "        shuffle: True or False. Whether to shuffle the data.\n",
    "        num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "        Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grab only the features specified in DESIRED_FEATURES:\n",
    "    selected_features_data = features[DESIRED_COLUMNS]\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(selected_features_data).items()}                                            \n",
    "\n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f2f61377-4423-484e-b2a4-1d533e1a9d44",
    "_uuid": "9516583be94c103dea65ca33fcb5ad2040ac5b24"
   },
   "source": [
    "For the new 'spelling'/typing error data, it's not distributed in such a way that lends itself well to bucketizing with the above function.  Thus, I'm going to turn those values all into strings (and treat them as a categorical columns with vocabulary_lists):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b7c08b07-5e6c-4475-806c-05c19376a647",
    "_uuid": "5006e43af8b16f8a3d0724e9f961716b5081bb20"
   },
   "source": [
    "Next up is the function to make Feature Columns.  But before that, vocabulary_lists need to be created for each categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "a92a5aad-29f7-4453-9a1f-ffd1bb741577",
    "_uuid": "f5bf923a8ecf57b691027a586535439276f42985",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def vocab_lists_for_each_feature():\n",
    "    vocab_for_feature = {}\n",
    "    for str_name, str_type, dont_care in DESIRED_FEATURES:\n",
    "        #features to treat as full strings:\n",
    "        if (str_type == WHOLE_STRING):\n",
    "            vocab_for_feature[str_name] = training_examples[str_name].unique().tolist()\n",
    "        #string features that should be split into individual words\n",
    "        elif (str_type == WORDS_STRING):\n",
    "            new_set = set()\n",
    "            training_examples[str_name].str.split().apply(new_set.update)\n",
    "            vocab_for_feature[str_name] = new_set\n",
    "        #everything else is a number that doesn't need a vocabulary list\n",
    "                  \n",
    "    return vocab_for_feature\n",
    "\n",
    "vocab_lists_for_features = vocab_lists_for_each_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "8731bf2c-8ada-41f8-a1ec-d1c45797ebe7",
    "_uuid": "172ffb8ebb6659fd8ef423267134d91dab268533"
   },
   "source": [
    "And now the feature columns can be constructed.  This contains code for making bucketized numeric columns as well as categorical columns with vocabulary list.  If the value of global IS_DNN_CLASSIFIER is true, it then converts the categorical columns into embedding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "965b1409-cd62-462d-9a4f-b3acc8a569ec",
    "_uuid": "2c70ce494cba6c31351fa6cc47134202c43924f4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "    \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "    Returns:\n",
    "        A set of feature columns\n",
    "    \"\"\"\n",
    "    \n",
    "    arr_num_columns = []\n",
    "    arr_vocabulary_columns = []\n",
    "    #for use in determining embedding dimensions for each embedding_column:\n",
    "    arr_dimensions = []\n",
    "    \n",
    "    for str_name, str_type, num_bucket in DESIRED_FEATURES:\n",
    "        # don't create a feature column for the target:\n",
    "        if (str_name == STR_TARGET):\n",
    "            continue\n",
    "        # create normal numeric_columns for normal numbers\n",
    "        elif (str_type == NUM_INT) or (str_type == NUM_FLOAT):\n",
    "            arr_num_columns.append(tf.feature_column.numeric_column(str_name))\n",
    "        # create bucketized columns for bucket numbers:\n",
    "        elif (str_type == BUCKET_INT) or (str_type == BUCKET_FLOAT):\n",
    "            #print(str_name)  for debugging irritating training error when it doesn't like the values comprising the buckets\n",
    "            if (type(num_bucket) == list) == False:  #then get the buckets\n",
    "                bucket_column = tf.feature_column.bucketized_column(\n",
    "                    tf.feature_column.numeric_column(str_name),\n",
    "                    boundaries=get_quantile_based_buckets(training_examples[str_name], num_bucket))\n",
    "            else:\n",
    "                bucket_column = tf.feature_column.bucketized_column(\n",
    "                    tf.feature_column.numeric_column(str_name), boundaries=num_bucket)\n",
    "            arr_num_columns.append(bucket_column)\n",
    "        # create categorical vocab columns for strings:\n",
    "        elif (str_type == WHOLE_STRING) or (str_type == WORDS_STRING):\n",
    "            arr_vocabulary_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(key=str_name, vocabulary_list=vocab_lists_for_features[str_name]))\n",
    "            #for DNN Classifier, also calculate the number of dimensions for embedding:\n",
    "            if IS_DNN_CLASSIFIER:\n",
    "                fourth_root = int(math.sqrt(math.sqrt(len(vocab_lists_for_features[str_name]))))\n",
    "                if (fourth_root == 0):\n",
    "                    fourth_root += 1\n",
    "                arr_dimensions.append(fourth_root)\n",
    "\n",
    "    #turn vocab_columns into embedding_columns\n",
    "    #arr_vocab and arr_dimensions will match by index\n",
    "    end = len(arr_dimensions)\n",
    "    arr_embedding_columns = []\n",
    "    if IS_DNN_CLASSIFIER:\n",
    "        for i in range(0, end):\n",
    "            arr_embedding_columns.append(tf.feature_column.embedding_column(arr_vocabulary_columns[i], dimension=arr_dimensions[i]))\n",
    "        arr_vocabulary_columns = arr_embedding_columns\n",
    "        \n",
    "    feature_columns = set(arr_num_columns + arr_vocabulary_columns)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7c79f15-f2e9-48d4-bf92-7c69b4cc1920",
    "_uuid": "016aab79aa92832d4fcbd718f7cfcb5c05717e35"
   },
   "source": [
    "Then we have the function to train the model.  (Note:  This is where **5. Regularization** gets implemented.)\n",
    "<a id=\"regular\"></a>\n",
    "\n",
    "**An important note about the train_model function:**\n",
    "In the Colab Programming Exercises, they split the training into \"periods\" so that you can observe changes in loss as the training progresses.  However, as far as I can tell, that makes it *really* slow here in Kaggle.  So I've skipped applying periods in this function.  (All the periods do is split up the number of steps.  For example, if you have 10 periods and 1000 steps, then using periods will train the model for 100 steps, stop, start up again, train for 100 more, stop, start up again, etc.  Using no periods means that all 1000 steps happen after a single start, which appears to be much, much faster with the same end result.  But since there are no periods, you won't get a visual graph of the training/validation loss changes over time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "cbe3ec6f-6746-4479-b7e2-45e4002683f4",
    "_uuid": "66315242d4afff80906bb93c96652a20d27a9a85",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier_model(\n",
    "    learning_rate,\n",
    "    regularization_strength,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    feature_columns,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets,\n",
    "    periods=1):\n",
    "    \"\"\"Trains a linear classifier model.\n",
    "\n",
    "    In addition to training, this function also prints training progress information,\n",
    "    as well as a plot of the training and validation loss over time.\n",
    "\n",
    "    Args:\n",
    "        learning_rate: A `float`, the learning rate.\n",
    "        regularization_strength: A `float` that indicates the strength of the L1\n",
    "            regularization. A value of `0.0` means no regularization.\n",
    "        steps: A non-zero `int`, the total number of training steps. A training step\n",
    "            consists of a forward and backward pass using a single batch.\n",
    "        feature_columns: A `set` specifying the input feature columns to use.\n",
    "        training_examples: A `DataFrame` containing one or more columns to use as input features for training.\n",
    "        training_targets: A `DataFrame` containing exactly one column from to use as target for training.\n",
    "        validation_examples: A `DataFrame` containing one or more columns to use as input features for validation.\n",
    "        validation_targets: A `DataFrame` containing exactly one column to use as target for validation.\n",
    "        periods: A integer, the number of times to train the model.  #Programming Exercises had periods = 7\n",
    "\n",
    "    Returns:\n",
    "    A `LinearClassifier` object trained on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    #steps_per_period = steps / periods  SKIPPING PERIODS\n",
    "    \n",
    "    # Create a linear classifier object.\n",
    "    my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate, l1_regularization_strength=regularization_strength)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    linear_classifier = tf.estimator.LinearClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        optimizer=my_optimizer\n",
    "    )\n",
    "    \n",
    "    # Create input function for training (dependent on batch_size passed in)\n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                      training_targets[TARGET_STR], \n",
    "                      batch_size=batch_size)\n",
    "    \n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss (on validation data):\")\n",
    "    #training_log_losses = []  SKIPPING PERIODS\n",
    "    #validation_log_losses = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        linear_classifier.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps\n",
    "        )\n",
    "        # Take a break and compute predictions.\n",
    "        training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
    "\n",
    "        validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "\n",
    "        # Compute training and validation loss.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  Training loss for period %02d: %0.2f\" % (period, training_log_loss))\n",
    "        print(\"  Validation loss for period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.  SKIPPING PERIODS\n",
    "        #training_log_losses.append(training_log_loss)\n",
    "        #validation_log_losses.append(validation_log_loss)\n",
    "        \n",
    "    print(\"Model training finished.\")\n",
    "\n",
    "    # Periods slow down Kaggle, so only do one; makes this graph pointless\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    #plt.ylabel(\"LogLoss\")\n",
    "    #plt.xlabel(\"Periods\")\n",
    "    #plt.title(\"LogLoss vs. Periods\")\n",
    "    #plt.tight_layout()\n",
    "    #plt.plot(training_log_losses, label=\"training\")\n",
    "    #plt.plot(validation_log_losses, label=\"validation\")\n",
    "    #plt.legend()\n",
    "\n",
    "    return linear_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cbf0df5e-5a17-44f5-aecd-28c730b6367a",
    "_uuid": "f9901a64052f83bf1a59fc888d4c6c29b5a06af3"
   },
   "source": [
    "**6. Training**\n",
    "<a id=\"lineartraining\"></a>\n",
    "\n",
    "First, create the input functions used to predict.  (Unlike the Programming Exercises, I'm doing this outside of the train_model function since they need to be used later outside of that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "da11e039-ddc7-43f0-aa6e-7b059a080d8f",
    "_uuid": "858d68fae97aec7533e5dc18e3784a4a09169706",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create input functions.\n",
    "predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                              training_targets[TARGET_STR], \n",
    "                              num_epochs=1, \n",
    "                              shuffle=False)\n",
    "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                validation_targets[TARGET_STR], \n",
    "                                num_epochs=1, \n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "c36f1950-4592-434f-bed0-26de6b971bf6",
    "_uuid": "a1800b9a09bb40f8b167d42ed32bc329941279a5"
   },
   "source": [
    "And now it's time to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "121715f5-b7d1-4a6f-980d-70157610276b",
    "_uuid": "9ece9292ef3a8f7afbd65494029ab0d389d5a322",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_OLD_MODELS & (not IS_DNN_CLASSIFIER):\n",
    "    linear_classifier = train_linear_classifier_model(\n",
    "        learning_rate=.1,\n",
    "        regularization_strength=0.1,\n",
    "        steps=1000,  \n",
    "        batch_size=140,\n",
    "        periods=1,\n",
    "        feature_columns=construct_feature_columns(),\n",
    "        training_examples=training_examples,\n",
    "        training_targets=training_targets,\n",
    "        validation_examples=validation_examples,\n",
    "        validation_targets=validation_targets) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8468a22e-44f9-4043-85cb-678dfb233582",
    "_uuid": "25e70f971e010dc383f26bf93ad35a82e5041b0f"
   },
   "source": [
    "**7. ROC and AUC**\n",
    "<a id=\"rocandauc\"></a>\n",
    "\n",
    "The code for this is found in [Chapter 12's Programming Exercises](https://colab.research.google.com/notebooks/mlcc/logistic_regression.ipynb?hl=en).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "1729891a-59a5-4c07-af9b-a4c7096fa04f",
    "_uuid": "7129ad4a7e58362e07c5fa680b63888a59b09734",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_OLD_MODELS & (not IS_DNN_CLASSIFIER):\n",
    "    #training_metrics = linear_classifier.evaluate(input_fn=predict_training_input_fn)\n",
    "    validation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "    #print(\"AUC on the training set: %0.2f\" % training_metrics['auc'])\n",
    "    #print(\"Accuracy on the training set: %0.2f\" % training_metrics['accuracy'])\n",
    "\n",
    "    print(\"AUC on the validation set: %0.2f\" % validation_metrics['auc'])\n",
    "    print(\"Accuracy on the validation set: %0.2f\" % validation_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "f7ede5f8-20cd-48d3-a93e-d5ad88cbaace",
    "_uuid": "478b40c5340baf2d2664c4daf2fe77469b7d7e28",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if USING_OLD_MODELS & (not IS_DNN_CLASSIFIER):\n",
    "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    # Get just the probabilities for the positive class.\n",
    "    validation_probabilities = np.array([item['probabilities'][1] for item in validation_probabilities])\n",
    "\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
    "        validation_targets, validation_probabilities)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
    "    plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
    "    _ = plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d557e6a7-c51d-4f13-9492-3e54c8bfba7e",
    "_uuid": "a300caf0545ecab20eadb4f1ba893bd22994ad8e"
   },
   "source": [
    "**8. Experiment Results**\n",
    "<a id=\"linearpretextexps\"></a>\n",
    "\n",
    "I tried a couple combinations of the following features:  previous_submissions, total_cost, teacher_prefix, school_state, grade_category.  Along with adjusting various hyperparameters, the best AUC ever yielded was only .57, which isn't much better than random.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0987767-4772-446a-9c5f-9e97c44ea8f9",
    "_uuid": "0c9f1075708156bd711cfe2c044e1ab43194a866"
   },
   "source": [
    "**IV. Dealing with Text-Heavy Features:  Using the Crash Course to Bypass TensorFlow Pain**\n",
    "<a id=\"bruteforcebaby\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "7fd65850-a74f-4d1c-9e1f-2694e50f8ce5",
    "_uuid": "a29dabeeaa7288ae7764bd7198dc7445961ff4c0"
   },
   "source": [
    "Since an AUC of .57 isn't very good, my next step was to start working text-heavy features like project_essays into the model.  And that's where I ran into huge problems.\n",
    "\n",
    "In the style of this set of [Programming Exercises](https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb?hl=en) from Chapter 17 - Embeddings, I implemented (or at least thought I had) a giant vocabulary list for use against the feature \"project_resource_summary\" text.  However, it had absolutely no affect on AUC at all, which surprised me.\n",
    "\n",
    "Taking a look at the training process, it looked like TensorFlow was treating a given example's data for that feature as a *full entire string* (for example:  \"My students need 7 ipads and 8 ipod nanos and cookies.\").  It was then taking that *full* string and comparing it with the individual words in the vocabulary list (\"my\", \"students\", \"need\", \"cookies\", etc.).  But since none of those full sentences exist in the vocabulary_list, it was basically doing *nothing*.  What I wanted it to do was look at each individual word in the full string and compare those to the words in the vocabulary list.\n",
    "\n",
    "However, I was never able to get TensorFlow to do that with the previous code.  I'm sure there's a way, but nothing I tried worked, and I either didn't ask the right questions in web searches or just couldn't find anything.  Plus, I got tired and frustrated of looking.\n",
    "\n",
    "So... what's my solution?  Well, the code in the Programming Exercises linked above has TensorFlow evaluating a full sentence as individual words.  Thus, whatever it was doing, I knew that code worked.  That Exercise, however, is based on *loading the data into TensorFlow in a completely different way,* by using something called a TFRecord.  So I decided to take the data that's currently in my Pandas Dataframe, [save it as a TFRecord](https://stackoverflow.com/questions/41402332/tensorflow-create-a-tfrecords-file-from-csv), and then use the code in the Programming Exercises to load that TFRecord data back into TensorFlow to have it actually do what I want.  It seems ridiculous... but it works!!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1d0cf9cd-4647-4a38-bc57-921ed1dc1700",
    "_uuid": "b309979efffd4e7d45a5e38b1f49388bb8bc66e0"
   },
   "source": [
    "**1.  Converting from Pandas to TFRecord**\n",
    "<a id=\"tfrecordrescue\"></a>\n",
    "\n",
    "The below functions are to make it much easier to add and remove features by you simply editing the DESIRED_FEATURES list.  The following functions then prepare the conversion of that feature data to a format that TFRecord can handle.  (One special note about strings is that TFRecord can't handle those.  Instead, they need to be converted to byte strings first with .encode().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "9491de17-056d-42cf-ad4d-87e2e6f360ea",
    "_uuid": "c9a1050a01dfef2e3523665a037d335b7d4cf6f4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#functions to write correct values to TFRecord\n",
    "#depending on the column value, TFRecord either needs to store a bytes_list, int64_list, or float_list value\n",
    "def make_bytes_list_append(col_index, value, example):\n",
    "    example.features.feature[DESIRED_COLUMNS[col_index]].bytes_list.value.append(value.encode())\n",
    "\n",
    "#for the data with actual sentences, split them into individual words\n",
    "def make_bytes_list_extend(col_index, value, example):\n",
    "    arr_strings = value.split(' ')\n",
    "    arr_bstrings = list(map(lambda x: x.encode(), arr_strings))\n",
    "    example.features.feature[DESIRED_COLUMNS[col_index]].bytes_list.value.extend(arr_bstrings)\n",
    "    \n",
    "def make_int64_list(col_index, value, example):\n",
    "    example.features.feature[DESIRED_COLUMNS[col_index]].int64_list.value.append(value)\n",
    "\n",
    "def make_float_list(col_index, value, example):\n",
    "    example.features.feature[DESIRED_COLUMNS[col_index]].float_list.value.append(value)\n",
    "\n",
    "\n",
    "#function to create an list of the right function to call depending on the feature\n",
    "def match_feature_with_tfrecord_function():\n",
    "    arr_functions = []\n",
    "    for str_name, str_type, dontcare in DESIRED_FEATURES:\n",
    "        #features consisting of a full string = bytes, append\n",
    "        if (str_type == WHOLE_STRING):\n",
    "            arr_functions.append(make_bytes_list_append)\n",
    "        #features consisting of text that should be broken down into words = bytes, extend\n",
    "        elif (str_type == WORDS_STRING):\n",
    "            arr_functions.append(make_bytes_list_extend)\n",
    "        #features consisting of non-integer numbers = float\n",
    "        elif (str_type == NUM_FLOAT) or (str_type == BUCKET_FLOAT):\n",
    "            arr_functions.append(make_float_list)\n",
    "        #features consisting of integers = int64\n",
    "        elif (str_type == NUM_INT) or (str_type == BUCKET_INT):\n",
    "            arr_functions.append(make_int64_list)\n",
    "        \n",
    "    return arr_functions\n",
    "\n",
    "#loop through all the rows in a dataset, convert each individual cell to the right data type that TFRecord requires,\n",
    "#take those tf.examples and save them into a TFRecord file\n",
    "#NOTE:  REQUIRES arr_tfrecord_funcs = match_feature_with_tfrecord_function() to be called first\n",
    "def save_rows_as_TFRecord(array_of_rows, tfrecord_file_name):\n",
    "    with tf.python_io.TFRecordWriter(tfrecord_file_name) as writer:\n",
    "        last_column = len(DESIRED_COLUMNS)\n",
    "        for row in array_of_rows:\n",
    "            example = tf.train.Example()\n",
    "            for col_index in range(0, last_column):\n",
    "                arr_tfrecord_funcs[col_index](col_index, row[col_index], example)  #for each feature, call the corresponding function\n",
    "\n",
    "            writer.write(example.SerializeToString())\n",
    "    return\n",
    "\n",
    "#just a test function to make sure all the above works\n",
    "def TEST_save_rows_as_TFRecord(array_of_rows, tfrecord_file_name):\n",
    "    last_column = len(DESIRED_COLUMNS)\n",
    "    for row in array_of_rows:\n",
    "        example = tf.train.Example()\n",
    "        for col_index in range(0, last_column):\n",
    "            arr_tfrecord_funcs[col_index](col_index, row[col_index], example)  #for each feature, call the corresponding function\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "e7e30aad-d0d9-45aa-b286-f3ff3f4869ee",
    "_uuid": "9c62ee580f921a2d1a7ed8c71fd25bec14bd6f36"
   },
   "source": [
    "Now test to make sure that all works as intended by looking at a couple of examples before writing to a TFRecord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "84159015-fc86-422c-b7a7-0df6c2043518",
    "_kg_hide-output": true,
    "_uuid": "1fcbfdc3ce9503ebd262218a4f9b1cc1136d8a32"
   },
   "outputs": [],
   "source": [
    "#testing new functions:\n",
    "arr_tfrecord_funcs = match_feature_with_tfrecord_function()\n",
    "testdata = training_examples[DESIRED_COLUMNS][0:2]\n",
    "\n",
    "TEST_save_rows_as_TFRecord(testdata.values, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24c64d69-3f27-4baf-8052-fb15956da30e",
    "_uuid": "172395df05eb9031369ed3b10642f2e7c00593e6"
   },
   "source": [
    "*i. Handle the Test Dataset*\n",
    "<a id=\"testrecord\"></a>\n",
    "\n",
    "Before saving the data as TFRecords, there's one change to make to the *test* dataset.  That dataset doesn't have a 'project_is_approved' column, which would cause the upcoming code to fail.  So, to make things easy, I'm just going to add that column to the dataset and fill it with a bunch of zeroes since those numbers won't be used for anything anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "af50f734-d489-4268-bcf0-be58604aa782",
    "_uuid": "c77031649ce04d76dc9310701981248493248543"
   },
   "outputs": [],
   "source": [
    "combined_test_dataset['project_is_approved'] = 0\n",
    "combined_test_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "cc993c08-9db8-450e-b551-1f692d435148",
    "_uuid": "438d8cfafd9e6ba267f6e6acf2a35b1ee54d30fb"
   },
   "source": [
    "Using all the previous functions, now actually save the data sets as TFRecords.  This step takes **a LONG time**.  However, using this TFRecord method makes the actual training later on go blazing fast!\n",
    "\n",
    "Additionally, this eats up **A LOT of disk space**, especially if you include all the features.\n",
    "\n",
    "*On that note, if you want to play around with a lot of features and/or experiment with different hyperparameters, I'd recommend commenting out the last line that saves the test_dataset to free up some space.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "7038c04d-882c-4e44-b58e-4f58dec04a36",
    "_uuid": "f7a3077d2aed071c553d371434902e5801551d60"
   },
   "outputs": [],
   "source": [
    "#Files to save:\n",
    "TRAINING_TFRECORD = \"training.tfrecords\" #for training_examples\n",
    "VALIDATION_TFRECORD = \"validation.tfrecords\" #for validation_examples\n",
    "TEST_TFRECORD = \"test.tfrecords\" #for combined_test_dataset\n",
    "\n",
    "arr_tfrecord_funcs = match_feature_with_tfrecord_function() #grab the conversion functions to call for each cell\n",
    "#make the TFRecord for training data:\n",
    "save_rows_as_TFRecord(training_examples[DESIRED_COLUMNS].values, TRAINING_TFRECORD)\n",
    "#make the TFRecord for validation data:\n",
    "save_rows_as_TFRecord(validation_examples[DESIRED_COLUMNS].values, VALIDATION_TFRECORD)\n",
    "#make the TFRecord for test data:\n",
    "save_rows_as_TFRecord(combined_test_dataset[DESIRED_COLUMNS].values, TEST_TFRECORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2be83c9-7f93-4664-b1d8-ac592eb32f0a",
    "_uuid": "30221dbe35af3815d8ff34cdf7c0be9892581f1f"
   },
   "source": [
    "**2. Loading TFRecord into TensorFlow**\n",
    "<a id=\"recordtotf\"></a>\n",
    "\n",
    "The following code is taken straight from the [Exercise](https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb) but adapted to fit the data for this project and, once again, to make it easier to add/remove features by only editing the DESIRED_FEATURES constant from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "a0c9a71e-9968-441b-88ce-6e8f6271c72d",
    "_uuid": "0019c60f912161ac1a48a5bb6f6ad3f576967aba",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_function(record):\n",
    "    \"\"\"Extracts features and labels from a TFRecord file.\n",
    "\n",
    "    Args:\n",
    "        record: file name of the TFRecord file    \n",
    "    Returns:\n",
    "        A `tuple` `(features, labels)`:\n",
    "        features: A dict of tensors representing the features\n",
    "        labels: A tensor with the corresponding labels.\n",
    "    \"\"\"\n",
    "    features_to_parse = {}\n",
    "    for str_name, str_type, dontcare in DESIRED_FEATURES:\n",
    "        #features consisting of a full string that wasn't split apart\n",
    "        if (str_type == WHOLE_STRING):\n",
    "            features_to_parse[str_name] = tf.FixedLenFeature(shape=[1], dtype=tf.string)\n",
    "        #features consisting of a string split apart into pieces of varying lengths.  Thus, need the VarLenFeature:\n",
    "        elif (str_type == WORDS_STRING):\n",
    "            features_to_parse[str_name] = tf.VarLenFeature(dtype=tf.string)\n",
    "        #features consisting of non-integer numbers = float\n",
    "        elif (str_type == NUM_FLOAT) or (str_type == BUCKET_FLOAT):\n",
    "            features_to_parse[str_name] = tf.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "        #features consisting of integers = int64\n",
    "        elif (str_type == NUM_INT) or (str_type == BUCKET_INT):\n",
    "            features_to_parse[str_name] = tf.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "\n",
    "    parsed_features = tf.parse_single_example(record, features_to_parse)\n",
    "    \n",
    "    final_features = {}\n",
    "    for str_name, str_type, dontcare in DESIRED_FEATURES:\n",
    "        #don't include target in features; handle later as label\n",
    "        if (str_name == STR_TARGET):\n",
    "            continue\n",
    "        ##DO NOT FORGET .values for the VarLenFeature strings (doing so creates a cryptic error that was hard to figure out)\n",
    "        elif (str_type == WORDS_STRING):\n",
    "            final_features[str_name] = parsed_features[str_name].values\n",
    "        #everything else as normal\n",
    "        else:\n",
    "            final_features[str_name] = parsed_features[str_name]\n",
    "    \n",
    "    #grab the targets:\n",
    "    labels = parsed_features[STR_TARGET]\n",
    "    \n",
    "    return final_features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "c1da361f-6cd2-45cc-a5ca-29f87e5185d7",
    "_uuid": "6a9d43f48dba7a7e73f3c86cda16d7f81e8c5765"
   },
   "source": [
    "I now check to see that this is working and that the data is in the correct format (ie, separated strings).  By the way, if you happen to look at the output in the Programming Exercises, it won't have the byte strings displayed like b'teaching'.  Instead, it will have odd numerical representations like '\\x0323teaching'.  That difference initially concerned me that my TFRecord attempt here had failed.  However, the Colab Programming environment (at least for me) uses Python 2, which apparently displays byte strings in that way.  Python 3 here on Kaggle just uses the b'value' approach.  So the data is indeed formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "4206813f-7445-405b-8229-09b4e58ad974",
    "_kg_hide-output": true,
    "_uuid": "b0dd36ef43a3833f0f3e6c4b6b1ab2071fef5c51"
   },
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset(TRAINING_TFRECORD)\n",
    "ds = ds.map(parse_function)\n",
    "\n",
    "n = ds.make_one_shot_iterator().get_next()\n",
    "sess = tf.Session()\n",
    "sess.run(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ed775ed-466c-4e00-8d09-31921432243c",
    "_uuid": "bc6d94c13bce77ed87746a689afb0d58faef837e"
   },
   "source": [
    "**3. Setting up Training (with Text-Heavy Features)**\n",
    "<a id=\"traintextheavy\"></a>\n",
    "\n",
    "This code for the input_fn is taken straight from the Programming Exercise.  And the upcoming code for training is, too.  However, I've adapted them both to better fit the style of previous Exercises (so that you only have to change hyperparameters in one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "2baeb89a-5874-4518-b096-d7468bf2abd2",
    "_uuid": "be5c5544b4a870f7a9dd9b852a70dbab4a37225f",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an input_fn that parses the tf.Examples from the given files,\n",
    "# and split them into features and targets.\n",
    "def tfrecord_input_fn(input_filenames, batch_size=100, num_epochs=None, shuffle=True):\n",
    "    # Create a dataset and map features and labels.\n",
    "    ds = tf.data.TFRecordDataset(input_filenames)\n",
    "    ds = ds.map(parse_function)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    # Our feature data is variable-length, so we pad and batch\n",
    "    # each field of the dataset structure to whatever size is necessary.\n",
    "    ds = ds.padded_batch(batch_size, ds.output_shapes)\n",
    "\n",
    "    ds = ds.repeat(num_epochs)\n",
    "\n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "fa292cb4-ddee-493d-a366-29adc80eef37",
    "_uuid": "466ec0b117064c91734040db7460c35cfed41058",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier_text(learning_rate=.05, steps=2000, batch_size=100):\n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "    feature_columns = construct_feature_columns()\n",
    "\n",
    "    classifier = tf.estimator.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer,\n",
    "    )\n",
    "\n",
    "    classifier.train(\n",
    "      input_fn=lambda: tfrecord_input_fn(TRAINING_TFRECORD, batch_size=batch_size),\n",
    "      steps=steps)\n",
    "\n",
    "    predict_training_input_fn = lambda: tfrecord_input_fn(TRAINING_TFRECORD,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_epochs=1, \n",
    "                                  shuffle=False)\n",
    "    predict_validation_input_fn = lambda: tfrecord_input_fn(VALIDATION_TFRECORD, \n",
    "                                    batch_size=batch_size,\n",
    "                                    num_epochs=1, \n",
    "                                    shuffle=False)\n",
    "    \n",
    "    return classifier, [predict_training_input_fn, predict_validation_input_fn]\n",
    "                    #return these last two so you can call them later without editing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "17a3c64a-6eaf-496f-abd7-85938d60b797",
    "_uuid": "b58ab1c36936d8819d944955f41327bea18169b8"
   },
   "source": [
    "**4. Training**\n",
    "<a id=\"lineartexttraining\"></a>\n",
    "\n",
    "And now the model can train while correctly incorporating the heavy-text features!\n",
    "\n",
    "[Take this link to quickly edit DESIRED_FEATURES.](#desiredfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "07f00318-f667-47a2-a903-96f83148492e",
    "_uuid": "e2523bf80d4e873cc20230c4388c125a1ffa9b2f"
   },
   "outputs": [],
   "source": [
    "if (USING_OLD_MODELS == False) & (IS_DNN_CLASSIFIER == False):\n",
    "    classifier, arr_funcs = train_linear_classifier_text(learning_rate=.01,\n",
    "                                                        steps=1500,\n",
    "                                                        batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fe236e2-186e-42e0-8cfd-78b6c1359931",
    "_uuid": "25d4451601143160a712e6a70516bd41f56f8f6a"
   },
   "source": [
    "**5. AUC**\n",
    "<a id=\"lineartextauc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b246e6b6-4699-40fb-ae71-9aa8a5394527",
    "_uuid": "3cff0f28aa43355c7f8b59d0b25a76af0f3aab41"
   },
   "outputs": [],
   "source": [
    "if (USING_OLD_MODELS == False) & (IS_DNN_CLASSIFIER == False):\n",
    "    #evaluation_metrics = classifier.evaluate(input_fn=arr_funcs[0])\n",
    "\n",
    "    #print(\"Training set metrics:\")\n",
    "    #for m in evaluation_metrics:\n",
    "    #    print(m, evaluation_metrics[m])\n",
    "    #print(\"---\")\n",
    "\n",
    "    evaluation_metrics = classifier.evaluate(input_fn=arr_funcs[1])\n",
    "\n",
    "    print(\"Test set metrics:\")\n",
    "    for m in evaluation_metrics:\n",
    "        print(m, evaluation_metrics[m])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b985c740-65e3-429f-bd79-96d631b001a3",
    "_uuid": "1c7ffe5541a88effc3b4c9ddefc96c94f8b9d915"
   },
   "source": [
    "**6. Experiment Results**\n",
    "<a id=\"lineartextexps\"></a>\n",
    "\n",
    "WOOHOO!!!!!  YESSSSSSSS, IT WORKED!!!  Ahem, I was just excited to finally not get .56 or .57 AUC as a result and, more importantly, to finally have the heavy-text features being analyzed as desired.\n",
    "\n",
    "i.  For the first experiment, I  incorporated \"project_resource_summary\" as the only heavy-text feature.  With various hyperparameters (steps=500/100, and learning_rate=.1/.01), this got the AUC up to around .64.\n",
    "\n",
    "ii.  For the next experiment, I added project_title and could get to around .66 or .67 AUC, so not too much difference.\n",
    "\n",
    "iii.  Up next was incorporating the two essays as separate features with separate vocab_lists, which got the AUC up to around .74 or so.  Quite a jump!\n",
    "\n",
    "iv. Tried out some different feature combinations with more features (like 'year_week' and 'full_description'), but AUC seems to max around .76.  I also tossed in every feature at one point to see what would happen, but it didn't have much of an effect.  The likely best hyperparameters are:  steps=2000, learning_rate=.05, batch_size=100.  And sufficient features are likely:  school_state (?), the essays, previous submissions, full_description, total_cost\n",
    "\n",
    "v.  Made some updates and maybe even fixes.  Changed 'year_week' from buckets to a vocabulary_list.  Used quantized buckets for previous_submissions.  Additionally, all of the above's vocabulary_lists / buckets were based on the full combined_training_dataset.   This experiment, they were based on *only training_examples* just in case using the full dataset would have some kind of influence on the validation data.  Used all features and the best hyperparameters from above and got AUC = .768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5581921a-dbeb-4eaa-af0b-f6b34f129222",
    "_uuid": "3a7cdd06709b294718be40c53f62a70b50e86ad6"
   },
   "source": [
    "**V. Applying the Crash Course to Create a DNNClassifier**\n",
    "<a id=\"dnnclassifier\"></a>\n",
    "\n",
    "The great news is that now, since the data is already being loaded into TensorFlow via the TFRecord that the [Programming Exercise](https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb) used, it's simple to keep following that Exercise and switch over to a DNN Classifier.\n",
    "\n",
    "But first, I want to highlight a couple of points mentioned in either that Exercise or the chapters leading up to it:\n",
    "\n",
    " - [Chapter 15 (Training Neural Nets)](https://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices) talked about using **dropout regularization** for DNN.  A dropout of 0 means no regularization, 1 means max and nothing gets learned\n",
    " - It also mentioned that the **AdamOptimizer** might be a better/more efficient option for a non-convex NN, over the AdagradOptimizer.\n",
    " - Also, in general, this chapter was about some important \"gotchas\" that can arise when training and how to possibly handle those with hyperparameters\n",
    " - [Chapter 17 (Embeddings)](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) talked about the number of dimensions to use in embeddings.  (\"Higher dimensions are good because it allows us to tease apart more distinctions and therefore you can learn better relationships.  On the downside, more dimensions means a higher chance of overfitting and leads to slower training and the need for more data.\")  And they gave the following empirical rule of thumb:  *the number of dimensions should be the **4th-root of the size of your vocabulary***.\n",
    " - The Exercise also discussed how using an **embedding_column** when setting up the feature columns is more computational efficient (and which is what I've set up the construct_feature_columns function to do for the DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea351530-92eb-46d9-851a-3cdae7a0b0dc",
    "_uuid": "64021928d36912c006dd321d8f17014445666611"
   },
   "source": [
    "**1. Setting Up for Training**\n",
    "<a id=\"dnntrainingsetup\"></a>\n",
    "\n",
    "All that's really needed now (besides how construct_feature_columns has already been set up for the embedding columns) is a slightly different training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "e6655eb6-0b30-47a2-a962-dddfcd41ece7",
    "_uuid": "bb080a56301997f5dd53d08e26c24da73536f40f",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_dnn_classifier_text(learning_rate=.05, steps=2000, batch_size=100, hidden_units=[20,20], dropout=.1):\n",
    "    #my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "    feature_columns = construct_feature_columns()\n",
    "\n",
    "##WHAT'S CHANGED:##\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=hidden_units,  #how many hidden units?:  https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "      optimizer=my_optimizer,\n",
    "      dropout=dropout\n",
    "    )\n",
    "\n",
    "    classifier.train(\n",
    "      input_fn=lambda: tfrecord_input_fn(TRAINING_TFRECORD, batch_size=batch_size),\n",
    "      steps=steps)\n",
    "\n",
    "    predict_training_input_fn = lambda: tfrecord_input_fn(TRAINING_TFRECORD,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_epochs=1, \n",
    "                                  shuffle=False)\n",
    "    predict_validation_input_fn = lambda: tfrecord_input_fn(VALIDATION_TFRECORD, \n",
    "                                    batch_size=batch_size,\n",
    "                                    num_epochs=1, \n",
    "                                    shuffle=False)\n",
    "    \n",
    "    return classifier, [predict_training_input_fn, predict_validation_input_fn]\n",
    "                    #return these last two so you can call them later without editing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06a102d7-0dc8-4754-8fcf-b35922430933",
    "_uuid": "c497451968a9dec60b063571f381686f4a6f7cf3"
   },
   "source": [
    "**2. Training**\n",
    "<a id=\"dnntraining\"></a>\n",
    "\n",
    "And then a slightly different way to train.\n",
    "\n",
    "[Take this link to quickly edit DESIRED_FEATURES.](#desiredfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "9e426fde-97ec-40e9-ad37-53a5d03c0c6a",
    "_uuid": "5bae089377674c40f27937332a44f0e9c58495b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (USING_OLD_MODELS == False) & (IS_DNN_CLASSIFIER):\n",
    "    dnn_classifier, dnn_arr_funcs = train_dnn_classifier_text(learning_rate=.005,\n",
    "                                                        steps=1500,\n",
    "                                                        batch_size=100,\n",
    "                                                        hidden_units=[6],\n",
    "                                                        dropout=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eff1171b-62f6-4b1b-a252-8e704c4866f5",
    "_uuid": "b0df47030bb5e27809ad32711184f7462d3e0a8c"
   },
   "source": [
    "**3. AUC**\n",
    "<a id=\"dnnauc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "bc516a32-420a-4a27-9bae-8540f0caa83b",
    "_uuid": "8346ba206131c5cc306ba22a5fdfc000bb27c918"
   },
   "outputs": [],
   "source": [
    "if (USING_OLD_MODELS == False) & (IS_DNN_CLASSIFIER):\n",
    "    #evaluation_metrics = dnn_classifier.evaluate(input_fn=dnn_arr_funcs[0])\n",
    "\n",
    "    #print(\"Training set metrics:\")\n",
    "    #for m in evaluation_metrics:\n",
    "    #    print(m, evaluation_metrics[m])\n",
    "    #print(\"---\")\n",
    "\n",
    "    evaluation_metrics = dnn_classifier.evaluate(input_fn=dnn_arr_funcs[1])\n",
    "\n",
    "    print(\"Test set metrics:\")\n",
    "    for m in evaluation_metrics:\n",
    "        print(m, evaluation_metrics[m])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "faa3886c-b361-4c1d-8fed-f7f40b18bbb1",
    "_uuid": "f45ea4a9231159831a58faa2a93926d4d9e9e3e4"
   },
   "source": [
    "**4. Experiment Results**\n",
    "<a id=\"dnnpretextexps\"></a>\n",
    "\n",
    "And... after using various hyperparameters and tweaks, the AUC here hasn't been much different from the Linear Classifier, with the best AUC being around .76.  (Though, some high learning_rates and hidden units did lead to an AUC of .50!  Yikes!)\n",
    "\n",
    "The best of the best AUC was .769, and the model used these features:  state, categories, subcategories, the essays, previous subs, full_desc, total_quantity, total_cost, year_week.  The hyperparameters were:  learning_rate = .005, hidden_units = [5], steps=1500, batch_size=100.\n",
    "\n",
    "However, by using far less features (only:  the essays, previous subs, full_desc, total_cost), the best AUC was .764.  Thus, those are likely sufficient features.  Hyperparameters were:  learning_rate = .005, hidden_units = [2], steps=1500, batch_size=100, dropout=.1.\n",
    "\n",
    "*Other Types of Experiments*\n",
    "I wasn't sure if joining multiple text features together as one feature vs keeping them as separate features would make a difference or not.  Smashing ALL the text features together into a single feature indeed had a negative effect on the models.  However, combining only the two essays into a single big essay had only a slightly negative effect (if any at all since the slight difference in AUC may have just been due to randomness).\n",
    "\n",
    "Since most applications get accepted, I was curious if using a vocabulary_list based on only the *rejected* entries would do anything.  It turns that this only had a minimal negative impact as well.  (However, using only the unique words from the rejected set - accepted set was a disaster resulting in .57 AUC.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a4f3733-3b6d-432f-a32d-7454e1dc2821",
    "_uuid": "b1c9f1bf278f2263c397e5c88de025f3f97c69f2"
   },
   "source": [
    "**VI. Submitting Predictions**\n",
    "<a id=\"submit\"></a>\n",
    "\n",
    "I've decided to go with the Linear Classifier model, using these features:  school_state, project_essay_1, project_essay_2, previous_submissions, full_desc, total_cost, year_week.  (Though I probably don't need to include school_state or year_week.)  And these were the hyperparameters used:  steps = 1500, batch_size = 100, learning_rate = .05.  This tends to yield an AUC a little above .76. \n",
    "\n",
    "And I'll submit one for the DNN Classifier model, too.  That will use the same features as the linear_classifier.  And the hyperparameters are:  learning_rate = .005, hidden_units = [4], steps=1500, batch_size=100, dropout=.1.  This also tended to yield an AUC a little above .76.\n",
    "\n",
    "I also did an experiment.  The first two submissions trained on 80% of the data and validated on 20% of the data.  For the last two submissions, I tested what would happen if the models were trained on all 100% of the data without doing any validation (since the features/hyperparameters had effectively been validated through the previous experiments).  This resulted in a minimally better AUC.\n",
    "\n",
    "Finally, I did various tests of both model types involving the new spelling/typing error features.  However, the increase was minimal.  The best ever AUC was .773 with *every single feature*.  Thus, the new error ones didn't add much and only increase the model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8e967d10-544d-4f03-979f-0588be89bc39",
    "_uuid": "b145e10748deab8803c4e4e61143b17948b0e880"
   },
   "source": [
    "**1. Making Predictions on the Test Dataset**\n",
    "<a id=\"predictions\"></a>\n",
    "\n",
    "Since the combined_test_dataset has been processed and saved (and loaded) in the same way as the other data via TFRecord, we just need to create an input function and call .predict() on the returned classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "b0b3b535-a4dc-4aa2-a39b-6888d7887711",
    "_uuid": "cb319df4f3d33ca1596daeeae2267fef4a884166"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/skleinfeld/getting-started-with-the-donorschoose-data-set\n",
    "predict_test_input_fn = lambda: tfrecord_input_fn(TEST_TFRECORD, \n",
    "                                    num_epochs=1, \n",
    "                                    shuffle=False)\n",
    "\n",
    "if (USING_OLD_MODELS == False):\n",
    "    if (IS_DNN_CLASSIFIER == False):\n",
    "        predictions_generator = classifier.predict(input_fn=predict_test_input_fn)\n",
    "    else:\n",
    "        predictions_generator = dnn_classifier.predict(input_fn=predict_test_input_fn)\n",
    "        \n",
    "    predictions_list = list(predictions_generator)\n",
    "    probabilities = [p[\"probabilities\"][1] for p in predictions_list]\n",
    "    print('Now have the probabilities.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da09f8b1-2085-4ebd-b1b0-987d130a2519",
    "_uuid": "f9df645b11d23b77e8e0467d6d49423ee57d1d88"
   },
   "source": [
    "**2. Convert to .csv**\n",
    "<a id=\"csv\"></a>\n",
    "\n",
    "This competition requires the submission to be a .csv file with entries of the form *id*, *probability prediction*.  I was originally worried that I would also have to save the 'id' value in the test dataset's TFRecord and then [load in all those values](https://stackoverflow.com/questions/37151895/tensorflow-read-all-examples-from-a-tfrecords-at-once/44879011#44879011) to complete this bizarre full circle:  .csv -> Pandas -> TFRecord -> Pandas -> .csv.  Fortunately, that's not the case.  Since the TFRecord file was made row-by-row from the Pandas data, we can just take the Pandas column as is since its order matches that of the probabilities array.\n",
    "\n",
    "Thus, we can just make a new Pandas dataframe using those two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "152e6237-651c-410e-8509-56737fcaee76",
    "_uuid": "1e3e2142f9b876a076338bb05a7b05b835e523da"
   },
   "outputs": [],
   "source": [
    "submission_values = pd.DataFrame({'id': combined_test_dataset['id'], 'project_is_approved': probabilities})\n",
    "submission_values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "cbbe5bbc-dd42-4981-b675-4ed85508334d",
    "_uuid": "b8257e8534c49ed3bb78d598b41b0dbed752db55"
   },
   "source": [
    "And turn that into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_cell_guid": "94089ef8-9c99-4cd5-80ef-5f3635ec41c0",
    "_uuid": "28cad94d93af57b8714c684381cd18cacbda7d5d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission_values.to_csv('linear-spelling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2ee6e01-536f-473e-8b03-ce1e4dfbf96b",
    "_uuid": "d02bdc1b8fa82a13b08cd3cc8131ac3314944d89"
   },
   "source": [
    "**3. Submit to Kaggle**\n",
    "<a id=\"submitfile\"></a>\n",
    "\n",
    "Then click the \"Commit & Run\" button.  Once that's finished, on your kernel's \"homepage\" is an \"Output\" tab where you can click on the \"Submit to Competition\" button to submit your .csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
