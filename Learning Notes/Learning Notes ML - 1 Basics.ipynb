{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Machine Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Some Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ML Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Type of tasks\n",
    "  - Classification\n",
    "  - Regression\n",
    "  - Structured annotation\n",
    "  - Clustering\n",
    "  - Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Challenges\n",
    "  - Quality of data \n",
    "  - Time-Consuming task − Another challenge faced by ML models is the consumption of time especially for data acquisition, feature extraction and retrieval. \n",
    "  - Lack of specialist persons − As ML technology is still in its infancy stage, availability of expert resources is a tough job.\n",
    "  - No clear objective for formulating business problems \n",
    "  - Issue of overfitting & underfitting \n",
    "  - Curse of dimensionality − Another challenge ML model faces is too many features of data points. This can be a real hindrance.\n",
    "  - Difficulty in deployment − Complexity of the ML model makes it quite difficult to be deployed in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Applications\n",
    "  - Emotion analysis\n",
    "  - Sentiment analysis\n",
    "  - Error detection and prevention\n",
    "  - Weather forecasting and prediction\n",
    "  - Stock market analysis and forecasting\n",
    "  - Speech synthesis\n",
    "  - Speech recognition\n",
    "  - Customer segmentation\n",
    "  - Object recognition\n",
    "  - Fraud detection\n",
    "  - Fraud prevention\n",
    "  - Recommendation of products to customer in online shopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept    14.952480\n",
      "B             0.401182\n",
      "C             0.000352\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "df = pd.DataFrame({\"A\": [10,20,30,40,50], \"B\": [20, 30, 10, 40, 50], \"C\": [32, 234, 23, 23, 42523]})\n",
    "result = sm.ols(formula=\"A ~ B + C\", data=df).fit()\n",
    "print(result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      A   R-squared:                       0.579\n",
      "Model:                            OLS   Adj. R-squared:                  0.158\n",
      "Method:                 Least Squares   F-statistic:                     1.375\n",
      "Date:                Wed, 10 Jun 2020   Prob (F-statistic):              0.421\n",
      "Time:                        17:47:02   Log-Likelihood:                -18.178\n",
      "No. Observations:                   5   AIC:                             42.36\n",
      "Df Residuals:                       2   BIC:                             41.19\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     14.9525     17.764      0.842      0.489     -61.481      91.386\n",
      "B              0.4012      0.650      0.617      0.600      -2.394       3.197\n",
      "C              0.0004      0.001      0.650      0.583      -0.002       0.003\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.061\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.498\n",
      "Skew:                          -0.123   Prob(JB):                        0.780\n",
      "Kurtosis:                       1.474   Cond. No.                     5.21e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.21e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delchain_default\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\stattools.py:71: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  \"samples were given.\" % int(n), ValueWarning)\n"
     ]
    }
   ],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Type of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "- The majority of practical machine learning uses supervised learning.\n",
    "- Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.\n",
    "- Y = f(X)\n",
    "- It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Supervised learning problems can be further grouped into regression and classification problems.\n",
    "  - Classification: A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”.\n",
    "  - Regression: A regression problem is when the output variable is a real value, such as “dollars” or “weight”.\n",
    "\n",
    "Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively.\n",
    "\n",
    "Some popular examples of supervised machine learning algorithms are:\n",
    "  - Linear regression for regression problems.\n",
    "  - Random forest for classification and regression problems.\n",
    "  - Support vector machines for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Unsupervised learning is where you only have input data (X) and no corresponding output variables.\n",
    "The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\n",
    "\n",
    "These are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. \n",
    "Algorithms are left to their own devises to discover and present the interesting structure in the data.\n",
    "\n",
    "Unsupervised learning problems can be further grouped into clustering and association problems.\n",
    "  - Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\n",
    "  - Association:  An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y.\n",
    "\n",
    "Some popular examples of unsupervised learning algorithms are:\n",
    "  - k-means for clustering problems.\n",
    "  - Apriori algorithm for association rule learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### Semi Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Problems where you have a large amount of input data (X) and only SOME of the data is labeled (Y) are called semi-supervised learning problems.\n",
    "\n",
    "These problems sit in between both supervised and unsupervised learning.\n",
    "  - A good example is a photo archive where only some of the images are labeled, (e.g. dog, cat, person) and the majority are unlabeled.\n",
    "  - Many real world machine learning problems fall into this area.\n",
    "  - This is because it can be expensive or time-consuming to label data as it may require access to domain experts. Whereas unlabeled data is cheap and easy to collect and store.\n",
    "\n",
    "You can use unsupervised learning techniques to discover and learn the structure in the input variables.\n",
    "You can also use supervised learning techniques to make best guess predictions for the unlabeled data, feed that data back into the supervised learning algorithm as training data and use the model to make predictions on new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Machine Learning vs Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Deep learning is machine learning.\n",
    "  - More specifically, deep learning is considered an evolution of machine learning. \n",
    "  - It uses a programmable neural network that enables machines to make accurate decisions without help from humans.\n",
    "\n",
    "However, its capabilities are different.\n",
    "  - While basic machine learning models do become progressively better at whatever their function is, they still need some guidance. \n",
    "  - If an AI algorithm returns an inaccurate prediction, then an engineer has to step in and make adjustments. \n",
    "  - With a deep learning model, an algorithm can determine on its own if a prediction is accurate or not through its own neural network.\n",
    "    \n",
    "A deep learning model is designed to continually analyze data with a logic structure similar to how a human would draw conclusions. \n",
    "  - To achieve this, deep learning applications use a layered structure of algorithms called an artificial neural network. \n",
    "  - The design of an artificial neural network is inspired by the biological neural network of the human brain, leading to a process of learning that’s far more capable than that of standard machine learning models.\n",
    "\n",
    "It’s a tricky prospect to ensure that a deep learning model doesn’t draw incorrect conclusions—like other examples of AI, it requires lots of training to get the learning processes correct. \n",
    "But when it works as it’s intended to, functional deep learning is often received as a scientific marvel that many consider being the backbone of true artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Simple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Importing the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### The basics in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Declare the X and y\n",
    "X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms','Avg. Area Number of Bedrooms','Area Population']]\n",
    "y = df['Price']\n",
    "\n",
    "# Prepare the test / train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Get the sets size\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "\n",
    "#printing the output and coefficients\n",
    "coeff_df = pd.DataFrame(linreg.coef_,X.columns,columns=['Coefficient']) \n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Visualisation of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Plotting the predictions vs the test set\n",
    "y_pred = lm.predict(X_test)  \n",
    "plt.scatter(y_test,y_pred)\n",
    "\n",
    "# Plotting the errors\n",
    "sns.distplot((y_test-y_pred),bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Mean Absolute Error (MAE) is the mean of the absolute value of the errors\n",
    "Mean Squared Error (MSE) is the mean of the squared errors:\n",
    "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors\n",
    "\n",
    "Comparing these metrics:\n",
    "\n",
    "MAE is the easiest to understand because it’s the average error.\n",
    "MSE is more popular than MAE because MSE “punishes” larger errors, which tends to be useful in the real world.\n",
    "RMSE is even more popular than MSE because RMSE is interpretable in the “y” units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# to get the metrics\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred)) \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred)) \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n",
    "\n",
    "# Different way, just for illustration\n",
    "y_pred = linreg.predict(X_test) \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(MSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Lets say that the model inputs are\n",
    "X = df[['Weight', 'Volume']]\n",
    "y = df['CO2']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Simply do that for predicting the CO2 emission of a car where the weight is 2300kg, and the volume is 1300ccm:\n",
    "predictedCO2 = regr.predict([[2300, 1300]])\n",
    "\n",
    "print(predictedCO2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "https://docs.w3cub.com/statsmodels/generated/statsmodels.regression.linear_model.ols.fit_regularized/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "est=sm.OLS(y, X)\n",
    "est = est.fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Plotting Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# provided that y_test and y_pred have been called (example below)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# y_pred = linreg.predict(X_test)\n",
    "\n",
    "sns.distplot((y_test-y_pred),bins=50)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "There is always a need to validate the stability of the machine learning model and need some kind of assurance that:\n",
    "  - the  model has got most of the patterns from the data correct\n",
    "  - the model is not picking up too much on the noise\n",
    "  - the model is low on bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Validation\n",
    "  - process of deciding whether the numerical results quantifying hypothesized relationships between variables, are acceptable as descriptions of the data.\n",
    "\n",
    "Residiuals\n",
    "  - evaluation of residuals = error estimation for the model is made after training \n",
    "  - a numerical estimate of the difference in predicted and original responses is done, also called the training error. \n",
    "  - However, this only gives us an idea about how well our model does on the data used to train it. \n",
    "  - It possible that the model is underfitting or overfitting the data. \n",
    "\n",
    "Cross Validation:\n",
    "  - Pupose: get an indication of how well the learner will generalize to an independent / unseen data set\n",
    "  - How: discussed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Model Bias / Variance\n",
    "\n",
    "Bias\n",
    "  - In an ideal scenario, these error values should sum up to zero. \n",
    "  - To return the model’s bias, we take the average of all the errors. \n",
    "  - The lower the average value, better the model.\n",
    "\n",
    "Variance\n",
    "  - Similarly for calculating the model variance, we take standard deviation of all the errors. \n",
    "  - A low value of standard deviation suggests our model does not vary a lot with different subsets of training data.\n",
    "\n",
    "We should focus on achieving a balance between bias and variance. \n",
    "  - This can be done by reducing the variance and controlling bias to an extent.\n",
    "  - This will result in a better predictive model.\n",
    "  - This trade-off usually leads to building less complex predictive models as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Hold Out Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Simple\n",
    "  - Removing a part of the training data and using it to get predictions from the model trained on rest of the data. \n",
    "  - The error estimation then tells how our model is doing on unseen data or the validation set. \n",
    "\n",
    "However\n",
    "  - suffers from issues of high variance since It is not certain which data points will end up in the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "The Problem\n",
    "  - As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. \n",
    "  - By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. \n",
    "\n",
    "The Solution\n",
    "  - What we require is a method that provides ample data for training the model and also leaves ample data for validation. \n",
    "  - K-Fold cross validation does exactly that.\n",
    "\n",
    "K Fold cross validation\n",
    "  - the data is divided into k subsets. \n",
    "  - the holdout method is repeated k times, such that each time:\n",
    "      - one of the k subsets is used as the test set / validation set\n",
    "      - the other k-1 subsets are put together to form a training set. \n",
    "  - The error estimation is averaged over all k trials to get total effectiveness of our model. \n",
    "  - As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. \n",
    "  - This significantly reduces\n",
    "      - bias as we are using most of the data for fitting\n",
    "      - variance as most of the data is also being used in validation set. \n",
    "  - Interchanging the training and test sets also adds to the effectiveness of this method. \n",
    "  - As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing’s fixed and it can take any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Example of 5 Fold Cross Validation\n",
    "\n",
    "Validation  XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX\n",
    "XXXXXXXXXX  Validation  XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX \n",
    "XXXXXXXXXX  XXXXXXXXXX  Validation  XXXXXXXXXX  XXXXXXXXXX \n",
    "XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX  Validation  XXXXXXXXXX \n",
    "XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX  XXXXXXXXXX  Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "In some cases, there may be a large imbalance in the response variables. \n",
    "  - For example, in dataset concerning price of houses, there might be large number of houses having high price. \n",
    "  - Or in case of classification, there might be several times more negative samples than positive samples. \n",
    "\n",
    "For such problems, a slight variation in the K-Fold cross validation technique is made:\n",
    "  - Each fold contains approximately the same percentage of samples of each target class as the complete set\n",
    "  - in case of prediction problems, the mean response value is approximately equal in all the folds. \n",
    "\n",
    "This variation is also known as Stratified K Fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Leave-P-Out Cross Validation (exchaustive method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Exhaustive Methods computes all possible ways the data can be split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Leave P-Out\n",
    "  - Leaves p data points out of training data\n",
    "  - Meaning: \n",
    "      - if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set. \n",
    "      - This is repeated for all combinations in which original sample can be separated this way\n",
    "      - Then the error is averaged for all trials, to give overall effectiveness.\n",
    "\n",
    "This method is exhaustive in the sense that:\n",
    "  - it needs to train and validate the model for all possible combinations\n",
    "  - for moderately large p, it can become computationally infeasible.\n",
    "    \n",
    "A particular case of this method is when p = 1. \n",
    "  - This is known as Leave one out cross validation. \n",
    "  - This method is generally preferred over the previous one because it does not suffer from the intensive computation\n",
    "  - Number of possible combinations is equal to number of data points in original sample or n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "# Implementing K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "kf = KFold(n_splits=5, shuffle = True) \n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    result = next(kf.split(X), None)\n",
    "    X_train = X.iloc[result[0]]\n",
    "    X_test = X.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    t_test = y.iloc[result[1]]\n",
    "    model = linreg.fit(X_train,y_train)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "\n",
    "print('scores from each iteration', scores)\n",
    "print('average k-fold score', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Stratified k-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    print(\"Train:\", train_index, \"Validation:\", val_index) \n",
    "    X_train, X_test = X[train_index], X[val_index] \n",
    "    y_train, y_test = y[train_index], y[val_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation with repetition (if the train set does not adequately represent the entire population, strtified is not goo)\n",
    "# In repeated cross-validation, the cross-validation procedure is repeated n times, yielding n random partitions of the original sample\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in rkf.split(X):\n",
    "     print(\"Train:\", train_index, \"Validation:\", val_index)\n",
    "     X_train, X_test = X[train_index], X[val_index]\n",
    "     y_train, y_test = y[train_index], y[val_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Interpretation of Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "R-squared        - coeff of determination. How well the regression line approximates real data points\n",
    "Adj. R-squared   - same as above, adjsuted for number of observations and degrees of freedom of residuals\n",
    "F-Stat           - measure of how significant the fit is. Mean sq error / mean sq error of residuals\n",
    "Prob(F-stat)     - prob to get F-stat, given the null hypothesis they are unrelated\n",
    "Log-Likelihood   - value of the likelihood function of the fitted model\n",
    "AIC              - Akaike Information Criterion: adjusts log-likelihood based on number of observations and complexity of model   \n",
    "BIC              - Bayesian Information Criterion: same as AIC, but with higher penalty for models wit more parameters\n",
    "Df Residuals     - degrees of freedom of the residulas. Number of observations - number of parameters\n",
    "Df Model         - number of parameters in the model (not including the constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Coef             - estimated value of coeff\n",
    "Std err          - basic standard error of the estimate of coeff. \n",
    "t                - t-stat (how statistically significant the coeff is)\n",
    "P>|t|            - p-value that null hypothesis that the coeff = 0 is true. if < 0.05: strong relationship between term and response\n",
    "95% Conf.Int     - lower and upper value of the 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Omnibus          - Angostino test: provides a combined statistical test of the presence of skewness and kurtosis\n",
    "Prob(Omnibus)    - same as above, turned into prob\n",
    "Skew             - measure of symmetry of data around mean\n",
    "Kurtosis         - measure of shape distribution\n",
    "Durbin-Watson    - test for autocorrelation (important in time series)\n",
    "Jarque=Bera      - different test of skewness and kurtosis\n",
    "Prob (JB)\n",
    "Cond.No          - test for multicolinearity (parameters are related to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Log Likelihood\n",
    "  - not possible to compare raw log-lieklihoods between models (better to use AIC or BIC)\n",
    "  - Likelihood is the likelihood of the entire model given a set of parameter estimates.\n",
    "  - It is calculated by \n",
    "    - taking a set of parameter estimates\n",
    "    - calculating the probability density for each one\n",
    "    - multiplying the probability densities for all the observations together \n",
    "    - >> this follows from probability theory in that P(A and B) = P(A)P(B) if A and B are independent)\n",
    "  - In practice, what this means for linear regression:\n",
    "    - you take a set of parameter estimates (beta, sd)\n",
    "    - plug them into the normal pdf\n",
    "    - calculate the density for each observation y at that set of parameter estimates\n",
    "    - multiply them all together. \n",
    "  - Typically, we choose to work with the log-likelihood because \n",
    "    - it is easier to calculate because instead of multiplying we can sum (log(a*b) = log(a) + log(b)), which is computationally faster. \n",
    "\n",
    "Log likelihood is used for almost everything. \n",
    "  - It is the basic quantity that we use to find parameter estimates (Maximum Likelihood Estimates) for a huge suite of models. \n",
    "  - For simple linear regression, these estimates turn out to be the same as those for least squares, but for more complicated models least squares may not work.\n",
    "\n",
    "AIC\n",
    "  - Lower value of AIC suggests \"better\" model, but it is a relative measure of model fit \n",
    "  - It is used for model selection (only), i.e. it lets you to compare different models estimated on the same dataset\n",
    "  - Lower indicates a more parsimonious model, relative to a model fit with a higher AIC.\n",
    "  - Model selection conducted with the AIC will choose the same model as leave-one-out cross validation \n",
    "  - Dont compare too many models with the AIC (like with p-values) because lowest AIC does not mean that it is the most appropriate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Omnibus          \n",
    "  - We want something close to zero, which means normalcy of residuals\n",
    "        \n",
    "Prob(Omnibus)    \n",
    "  - statistical test that residuals are normally distributed\n",
    "  - we want something close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Skew\n",
    "  - closer to 0 means symetric residual distribution\n",
    "  - If skewness is less than −1 or greater than +1, the distribution is highly skewed.\n",
    "  - If skewness is between −1 and −½ or between +½ and +1, the distribution is moderately skewed.\n",
    "  - If skewness is between −½ and +½, the distribution is approximately symmetric.\n",
    "    \n",
    "Kurtosis\n",
    "  - a uniform distribution has a kurtosis of 1.8 (excess -1.2) (lowest is discrete with 2 outcomes: kurto 1)\n",
    "  - a normal distribution has a kurtosis of 3 (excess 0)\n",
    "  - a logistic distribution has a kurtosis of 4.2 (excess 1.2)\n",
    "  - highest kurtosis is a student distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Durbin-Watson\n",
    "  - Value between zero and 4.0\n",
    "  - we hope to get a value between 1 and 2. ideally 2\n",
    "  - A value of 2.0 means there is no autocorrelation detected in the sample. \n",
    "  - values from zero to 2.0 indicate positive autocorrelation\n",
    "  - values from 2.0 to 4.0 indicate negative autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Condition Number \n",
    "  – This test measures the sensitivity of a function output as compared to its input.\n",
    "  - When we have multicollinearity, we can expect much higher fluctuations to small changes in the data\n",
    "  - We want a relatively small number (something below 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Confusion matrix \n",
    "  - matrix (table) that can be used to measure the performance of an machine learning algorithm, usually a supervised learning one\n",
    "\n",
    "By convention here\n",
    "  - row = instances of an actual class \n",
    "  - column = instances of a predicted class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# 2 Class looks like this\n",
    "-----------------------------------------------------------------\n",
    "                    Predicted Negative      Predicted Positive\n",
    "    \n",
    "Actual Negative       True Negative            False Positive\n",
    "\n",
    "Actual Positive       False NEgative           True Positive\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "Accuracy = (TN + TP) / (Total)\n",
    "Precision = TP / (FP + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Multi Class Case\n",
    "\n",
    "Accuracy\n",
    "  - \n",
    "\n",
    "Precision \n",
    "  - fraction of cases where the algorithm correctly predicted class i out of all instances where the algorithm predicted i (correctly and incorrectly). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Implementation in python - Example 1 - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cm = np.array(\n",
    "[[5825,    1,   49,   23,    7,   46,   30,   12,   21,   26],\n",
    " [   1, 6654,   48,   25,   10,   32,   19,   62,  111,   10],\n",
    " [   2,   20, 5561,   69,   13,   10,    2,   45,   18,    2],\n",
    " [   6,   26,   99, 5786,    5,  111,    1,   41,  110,   79],\n",
    " [   4,   10,   43,    6, 5533,   32,   11,   53,   34,   79],\n",
    " [   3,    1,    2,   56,    0, 4954,   23,    0,   12,    5],\n",
    " [  31,    4,   42,   22,   45,  103, 5806,    3,   34,    3],\n",
    " [   0,    4,   30,   29,    5,    6,    0, 5817,    2,   28],\n",
    " [  35,    6,   63,   58,    8,   59,   26,   13, 5394,   24],\n",
    " [  16,   16,   21,   57,  216,   68,    0,  219,  115, 5693]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()\n",
    "\n",
    "def precision_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n",
    "\n",
    "def recall_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label precision recall\n",
      "    0     0.983  0.964\n",
      "    1     0.987  0.954\n",
      "    2     0.933  0.968\n",
      "    3     0.944  0.924\n",
      "    4     0.947  0.953\n",
      "    5     0.914  0.980\n",
      "    6     0.981  0.953\n",
      "    7     0.928  0.982\n",
      "    8     0.922  0.949\n",
      "    9     0.957  0.887\n"
     ]
    }
   ],
   "source": [
    "print(\"label precision recall\")\n",
    "for label in range(10):\n",
    "    print(f\"{label:5d} {precision(label, cm):9.3f} {recall(label, cm):6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision total: 0.9496885564052286\n",
      "recall total: 0.9514531547877969\n"
     ]
    }
   ],
   "source": [
    "print(\"precision total:\", precision_macro_average(cm))\n",
    "print(\"recall total:\", recall_macro_average(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Implementation in python - Example 2 - simple pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_Actual  y_Predicted\n",
      "0          1            1\n",
      "1          0            1\n",
      "2          0            0\n",
      "3          1            1\n",
      "4          0            0\n",
      "5          1            1\n",
      "6          0            1\n",
      "7          0            0\n",
      "8          1            1\n",
      "9          0            0\n",
      "10         1            0\n",
      "11         0            0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'y_Actual':    [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "        'y_Predicted': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]}\n",
    "\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0  1\n",
      "Actual         \n",
      "0          5  2\n",
      "1          1  4\n"
     ]
    }
   ],
   "source": [
    "# in pandas, we create the confution matrix using pd.crosstab\n",
    "\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Implementation in Python - Example 3 - Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0],\n",
       "       [0, 1, 2],\n",
       "       [2, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_actu = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]\n",
    "y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]\n",
    "\n",
    "confusion_matrix(y_actu, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Arrange Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Normalise, Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Normalisation:\n",
    "  - Data Z is rescaled such that any specific Z will now be 0 ≤ Z ≤ 1, and is done through this formula: [(x - min(x)] / [ max(x) - min(x) ]\n",
    "  - Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients.      \n",
    "  - After normalisation, features are now more consistent with each other, which will allow us to evaluate the output of our future models better.\n",
    "  - Normalization makes the data better conditioned for convergence.\n",
    "  - Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible.\n",
    "\n",
    "However:\n",
    "  - When data is proportional, normalizing might not provide correct estimators. \n",
    "  - Or, when the scale between your data features does matters so you want to keep in your dataset.\n",
    "  - You need to think about your data, and understand if the transformations you’re applying are in line with the outcomes you’re searching for.                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# separate the data from the target attributes\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "# normalize the data attributes \n",
    "normalized_X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Normalisation vs Standardisation\n",
    "  - Keep in mind, there is some debate stating it is better to have the input values centred around 0 — standardization — rather than between 0 and 1. \n",
    "  - So doing your research is important as well, so you understand what type of data is needed by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Standardization\n",
    "  - Here your data Z is rescaled such that μ = 0 and 𝛔 = 1, and is done through this formula: ( xi - μ) / 𝛔 \n",
    "  - good for comparing features that have large difference of units or scales\n",
    "  - good for running models (logistic regression, SVMs, perceptrons, neural networks etc.) as the estimated weights will update similarly rather than at different rates during the build process. \n",
    "  - Standardizing tends to make the training process well behaved because the numerical condition of the optimization problems is improved. (example, for PCA, need to have features centered around mean)\n",
    "\n",
    "However\n",
    "  - if you do standardize your data be warned you might be discarding some information. \n",
    "  - If that information is not needed, the process can be helpful else it will impede your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# separate the data from the target attributes\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "# standardize the data attributes\n",
    "standardized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "A 3rd option is binning\n",
    "  - Consider the latitude feature, which has a geo point of the area in question\n",
    "  - We’re going to made new columns for each latitude range, and encode each value in our dataset with a 0 or 1 to see if it is within that latitude range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create range for your new columns\n",
    "lat_range = zip(xrange(32, 44), xrange(33, 45))\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# Iterate and create new columns, with the 0 and 1 encoding\n",
    "for r in lat_range\n",
    "        new_df[\"latitude_%d_to_%d\" % r] = df[\"latitude\"].apply(\n",
    "            lambda l: 1.0 if l >= r[0] and l < r[1] else 0.0)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Now that we can binned values, we have a binary value for each latitude in California. \n",
    "With this additional approach, you have another way to clean your data and get it ready for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Note: Iris dataset import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seaborn Method\n",
    "import seaborn as sns\n",
    "iris_sns = sns.load_dataset('iris')\n",
    "iris_sns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas\n",
    "import pandas as pd\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sckikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "iris_scikit = load_iris()\n",
    "# This will produce arrays of data and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Note: Import Iris, standardise, return a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.019004</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.131979</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.328414</td>\n",
       "      <td>-1.397064</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.098217</td>\n",
       "      <td>-1.283389</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.249201</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0     -0.900681     1.019004     -1.340227    -1.315444  setosa\n",
       "1     -1.143017    -0.131979     -1.340227    -1.315444  setosa\n",
       "2     -1.385353     0.328414     -1.397064    -1.315444  setosa\n",
       "3     -1.506521     0.098217     -1.283389    -1.315444  setosa\n",
       "4     -1.021849     1.249201     -1.340227    -1.315444  setosa"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "\n",
    "# separate the data from the target attributes\n",
    "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y = iris_df['species']\n",
    "\n",
    "# Get column names first (provided data was organised this way)\n",
    "names = ['sepal_length','sepal_width','petal_length','petal_width']   # it could be : iris_df.columns  but we have non numeric values in the last column\n",
    "\n",
    "# standardize the data attributes\n",
    "standardized_X = preprocessing.scale(X)\n",
    "standardized_X = pd.DataFrame(standardized_X, columns=names)\n",
    "standardized_X = pd.concat(( standardized_X,y),axis=1)\n",
    "standardized_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Almost all the real-world problems that you are going to encounter will have more than two variables. \n",
    "Linear regression involving multiple variables is called “multiple linear regression” or multivariate linear regression. \n",
    "The steps to perform multiple linear regression are almost similar to that of simple linear regression. \n",
    "\n",
    "The difference lies in the evaluation. \n",
    "You can use it to find out which factor has the highest impact on the predicted output and how different variables relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
