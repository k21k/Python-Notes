{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 1/ REGRESSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Desciption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Regression algorithms can be used for example, when you want to compute some continuous value as compared to Classification where the output is categoric. \n",
    "So whenever you are told to predict some future value of a process which is currently running, you can go with regression algorithm. \n",
    "Linear Regressions are however unstable in case features are redundant, i.e. if there is multicollinearity\n",
    "\n",
    "Some examples where linear regression can used are:\n",
    "- Time to go one location to another\n",
    "- Predicting sales of particular product next month\n",
    "- Impact of blood alcohol content on coordination\n",
    "- Predict monthly gift card sales and improve yearly revenue projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Importing the Tools\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Declare the X and y\n",
    "X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms','Avg. Area Number of Bedrooms','Area Population']]\n",
    "y = df['Price']\n",
    "\n",
    "# Prepare the test / train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Get the sets size\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "\n",
    "#printing the output and coefficients\n",
    "coeff_df = pd.DataFrame(linreg.coef_,X.columns,columns=['Coefficient']) \n",
    "coeff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Visualisation of the errors\n",
    "\n",
    "# Plotting the predictions vs the test set\n",
    "y_pred = lm.predict(X_test)  \n",
    "plt.scatter(y_test,y_pred)\n",
    "\n",
    "# Plotting the errors\n",
    "sns.distplot((y_test-y_pred),bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Mean Absolute Error (MAE) is the mean of the absolute value of the errors\n",
    "Mean Squared Error (MSE) is the mean of the squared errors:\n",
    "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors\n",
    "\n",
    "Comparing these metrics:\n",
    "\n",
    "MAE is the easiest to understand because it’s the average error.\n",
    "MSE is more popular than MAE because MSE “punishes” larger errors, which tends to be useful in the real world.\n",
    "RMSE is even more popular than MSE because RMSE is interpretable in the “y” units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# to get the metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred)) \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred)) \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n",
    "\n",
    "\n",
    "# Different way, just for illustration\n",
    "y_pred = linreg.predict(X_test) \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# To get the summary\n",
    "linreg.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Lets say that the model inputs are\n",
    "X = df[['Weight', 'Volume']]\n",
    "y = df['CO2']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Simply do that for predicting the CO2 emission of a car where the weight is 2300kg, and the volume is 1300ccm:\n",
    "predictedCO2 = regr.predict([[2300, 1300]])\n",
    "\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Almost all the real-world problems that you are going to encounter will have more than two variables. \n",
    "Linear regression involving multiple variables is called “multiple linear regression” or multivariate linear regression. \n",
    "The steps to perform multiple linear regression are almost similar to that of simple linear regression. \n",
    "\n",
    "The difference lies in the evaluation. \n",
    "You can use it to find out which factor has the highest impact on the predicted output and how different variables relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Logistic regression performs binary classification, so the label outputs are binary. \n",
    "It takes linear combination of features and applies non-linear function (sigmoid) to it, so it’s a very small instance of neural network.\n",
    "\n",
    "Logistic regression provides lots of ways to regularize your model, and you don’t have to worry as much about your features being correlated, like you do in Naive Bayes. \n",
    "You also have a nice probabilistic interpretation, and you can easily update your model to take in new data, unlike decision trees or SVMs. \n",
    "Use it if you want a probabilistic framework or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model. \n",
    "Logistic regression can also help you understand the contributing factors behind the prediction, and is not just a black box method.\n",
    "\n",
    "Logistic regression can be used in cases such as:\n",
    "- Predicting the Customer Churn\n",
    "- Credit Scoring & Fraud Detection\n",
    "- Measuring the effectiveness of marketing campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, y_train) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Correlation coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(stand_dfX.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test) \n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "RSq = metrics.r2_score(y_test, y_pred)\n",
    "print(MSE)\n",
    "print(RSq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=100)\n",
    "model_kfold = LogisticRegression()\n",
    "results_kfold = model_selection.cross_val_score(model_kfold, X_test, y_test, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (results_kfold.mean()*100.0))\n",
    "print(results_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors = 10)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "regressor = DecisionTreeRegressor() \n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "acc_treereg = round(regressor.score(X_train, y_train) * 100, 2)\n",
    "acc_treereg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 2/ CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "Despite its simplicity, it has been quite successful in a large number of classification and regression problems, for example character recognition or image analysis.\n",
    "\n",
    "The principle behind nearest neighbor classification consists in finding a predefined number (k) of training samples closest in distance to a new sample, which has to be classified. \n",
    "The label of the new sample will be defined from these neighbors. \n",
    "k-nearest neighbor classifiers have a fixed user defined constant for the number of neighbors which have to be determined. \n",
    "There are also radius-based neighbor learning algorithms, which have a varying number of neighbors based on the local density of points, all the samples inside of a fixed radius. \n",
    "The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. \n",
    "Neighbors-based methods are known as non-generalizing machine learning methods, since they simply \"remember\" all of its training data. \n",
    "Classification can be computed by a majority vote of the nearest neighbors of the unknown sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Optimization: find best n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(range(1,10),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Single trees are used very rarely, but in composition with many others they build very efficient algorithms such as Random Forest or Gradient Tree Boosting.\n",
    "\n",
    "Decision Tree is a white box type of ML algorithm. \n",
    "It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. \n",
    "Its training time is faster compared to the neural network algorithm. \n",
    "The time complexity of decision trees is a function of the number of records and number of attributes in the given data. \n",
    "The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. \n",
    "\n",
    "Advantages:\n",
    "  - Simple to understand, interpret, visualize\n",
    "  - Decision trees implicitly perform variable screening or feature selection.\n",
    "  - Can handle both numerical and categorical data. Can also handle multi-output problems.\n",
    "  - Decision trees require relatively little effort from users for data preparation.\n",
    "  - Nonlinear relationships between parameters do not affect tree performance.\n",
    "  - Decision trees can handle high dimensional data with good accuracy.\n",
    "  - Decision trees easily handle feature interactions and they’re non-parametric, so you don’t have to worry about outliers or whether the data is linearly separable. \n",
    "\n",
    "Disavantages:\n",
    "  - don’t support online learning, so you have to rebuild your tree when new examples come on. \n",
    "  - can create over-complex trees that do not generalize the model well, and hence overfit\n",
    "  - can create biased trees if some class dominate\n",
    "  - Decision Trees can also take a lot of memory (the more features you have, the deeper and larger your decision tree is likely to be)\n",
    "\n",
    "Trees are excellent tools for helping you to choose between several courses of action.\n",
    "  - Investment decisions\n",
    "  - Customer churn\n",
    "  - banks loan defaulters\n",
    "  - Build vs Buy decisions\n",
    "  - Sales lead qualifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)       # >>> This is a Numpy array\n",
    "\n",
    "acc_tree_train = round(classifier.score(X_train, y_train) * 100, 2)\n",
    "acc_tree_test = round(classifier.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "print(\"Train Accuracy: \",acc_tree_train)\n",
    "print(\"Test Accuracy: \", acc_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "feature_cols = ['gender','location','person','time','g_frontal','g_vertical','g_lateral','antenna','rssi','phase']\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(classifier, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['1','2', '3','4'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('label.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Random Forest is an ensemble of decision trees. \n",
    "It can solve both regression and classification problems with large data sets. It also helps identify most significant variables from thousands of input variables. \n",
    "Random Forest is highly scalable to any number of dimensions and has generally quite acceptable performances. \n",
    "\n",
    "Then finally, there are genetic algorithms, which scale to any dimension and any data with minimal knowledge of the data itself.\n",
    "With Random Forest however, learning may be slow (depending on the parameterization) and it is not possible to iteratively improve the generated models\n",
    "\n",
    "Random Forest can be used in real-world applications such as:\n",
    "- Predict patients for high risks\n",
    "- Predict parts failures in manufacturing\n",
    "- Predict loan defaulters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "acc_rtree_train = round(clf.score(X_train, y_train) * 100, 2)\n",
    "acc_rtree_test = round(clf.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "print(\"Train Accuracy: \",acc_rtree_train)\n",
    "print(\"Test Accuracy: \", acc_rtree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,500):\n",
    "    random_forest = RandomForestClassifier(n_estimators=i)\n",
    "    random_forest.fit(X_train,y_train)\n",
    "    pred_i = random_forest.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(range(1,500),error_rate,color='blue')\n",
    "\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Error Rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the exact output values\n",
    "error = pd.DataFrame(error_rate)\n",
    "print(error.idxmin())\n",
    "print(error.min()*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Neural Networks take in the weights of connections between neurons . \n",
    "The weights are balanced, learning data point in the wake of learning data point . \n",
    "When all weights are trained, the neural network can be utilized to predict the class or a quantity, if there should arise an occurrence of regression of a new input data point.\n",
    "With Neural networks, complex models can be trained and they can be utilized as a black box, without playing out an unpredictable complex feature engineering before training the model. \n",
    "Joined with the “deep approach” even more unpredictable models can be picked up to realize new possibilities. \n",
    "E.g. object recognition has been as of late enormously enhanced utilizing Deep Neural Networks. \n",
    "Applied to unsupervised learning tasks, such as feature extraction, deep learning also extracts features from raw images or speech with much less human intervention.\n",
    "\n",
    "On the other hand, neural networks are very hard to just clarify and parameterization is extremely mind boggling. They are also very resource and memory intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Support Vector Machine (SVM) is a supervised machine learning technique that is widely used in pattern recognition and classification problems.\n",
    "\n",
    "High accuracy, nice theoretical guarantees regarding overfitting, appropriate kernel can work well even the data isn’t linearly separable in the base feature space.\n",
    "Especially popular in text classification problems where very high-dimensional spaces are the norm. \n",
    "SVMs are however memory-intensive, hard to interpret, and difficult to tune.\n",
    "\n",
    "SVM can be used in real-world applications such as:\n",
    "- detecting persons with common diseases such as diabetes\n",
    "- hand-written character recognition\n",
    "- text categorization — news articles by topics\n",
    "- stock market price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "svc_train = round(svc.score(X_train, y_train) * 100, 2)\n",
    "svc_test = round(svc.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "print(\"Train Accuracy: \",svc_train)\n",
    "print(\"Test Accuracy: \", svc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/svm-hyper-parameter-tuning-using-gridsearchcv-49c0bc55ce29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Optimization is done with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Principal component analysis provides dimensionality reduction. \n",
    "Sometimes you have a wide range of features, probably highly correlated between each other, and models can easily overfit on a huge amount of data. Then, you can apply PCA.\n",
    "\n",
    "One of the keys behind the success of PCA is that in addition to the low-dimensional sample representation, it provides a synchronized low-dimensional representation of the variables. \n",
    "The synchronized sample and variable representations provide a way to visually find variables that are characteristic of a group of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "It is a classification technique based on Bayes’ theorem and very easy to build and particularly useful for very large data sets. \n",
    "Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods. \n",
    "Naive Bayes is also a good choice when CPU and memory resources are a limiting factor.\n",
    "\n",
    "Naive Bayes is super simple, you’re just doing a bunch of counts. \n",
    "If the NB conditional independence assumption holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. \n",
    "And even if the NB assumption doesn’t hold, a NB classifier still often does a great job in practice. \n",
    "A good bet if want something fast and easy that performs pretty well. Its main disadvantage is that it can’t learn interactions between features.\n",
    "\n",
    "Naive Bayes can be used in real-world applications such as:\n",
    "- Sentiment analysis and text classification\n",
    "- Recommendation systems like Netflix, Amazon\n",
    "- To mark an email as spam or not spam\n",
    "- Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
