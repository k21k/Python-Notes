{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# RECOMMENDER SYSTEMS LESSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "https://realpython.com/build-recommendation-engine-collaborative-filtering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\delchain_default\\Documents\\GitHub\\Python-Notes\\Machine Learning\\Recommender System (Advanced)\\Movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>880473582</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>891271545</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>888552084</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>879362124</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  user_id  item_id  rating  timestamp             title\n",
       "0           0        0       50       5  881250949  Star Wars (1977)\n",
       "1           1      290       50       5  880473582  Star Wars (1977)\n",
       "2           2       79       50       4  891271545  Star Wars (1977)\n",
       "3           3        2       50       5  888552084  Star Wars (1977)\n",
       "4           4        8       50       5  879362124  Star Wars (1977)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**The Dataset**\n",
    "\n",
    "To experiment with recommendation algorithms, you’ll need data that contains a set of items and a set of users who have reacted to some of the items.\n",
    "The reaction can be\n",
    "\n",
    "* explicit (rating on a scale of 1 to 5, likes or dislikes)\n",
    "\n",
    "* implicit (viewing an item, adding it to a wish list, the time spent on an article).\n",
    "\n",
    "While working with such data, you’ll mostly see it in the form of a matrix consisting of reactions of users to some items from a set of items.\n",
    "\n",
    "* Each row would contain the ratings given by a user\n",
    "\n",
    "* Each column would contain the ratings received by an item.\n",
    "\n",
    "A rating matrix with five users and five items could look like this:\n",
    "\n",
    "![](data:;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Steps Involved in Collaborative Filtering**\n",
    "\n",
    "To build a system that can automatically recommend items to users based on the preferences of other users:\n",
    "\n",
    "* the first step is to find similar users or items. \n",
    "\n",
    "* the second step is to predict the ratings of the items that are not yet rated by a user. \n",
    "\n",
    "So, you will need the answers to these questions:\n",
    "\n",
    "* How do you determine which users or items are similar to one another?\n",
    "\n",
    "* Given that you know which users are similar, how do you determine the rating that a user would give to an item based on the ratings of similar users?\n",
    "\n",
    "* How do you measure the accuracy of the ratings you calculate?\n",
    "  \n",
    "\n",
    "The first two questions don’t have single answers. Collaborative filtering is a family of algorithms where there are multiple ways to find similar users or items and multiple ways to calculate rating based on ratings of similar users. Depending on the choices you make, you end up with a type of collaborative filtering approach. \n",
    "\n",
    "One important thing to keep in mind is that in an approach based purely on collaborative filtering, the similarity is not calculated using factors like the age of users, genre of the movie, or any other data about users or items. It is calculated only on the basis of the rating a user gives to an item. \n",
    "\n",
    "The third question for how to measure the accuracy of your predictions also has multiple answers :\n",
    "\n",
    "* Root Mean Square Error (RMSE), in which you predict ratings for a test dataset of user-item pairs whose rating values are already known. The difference between the known value and the predicted value would be the error. Square all the error values for the test set, find the average (or mean), and then take the square root of that average to get the RMSE.\n",
    "\n",
    "* Another metric to measure the accuracy is Mean Absolute Error (MAE), in which you find the magnitude of error by finding its absolute value and then taking the average of all error values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Memory Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The first category of systems includes algorithms that are memory based, in which statistical techniques are applied to the entire dataset to calculate the predictions.\n",
    "\n",
    "To find the rating R that a user U would give to an item I, the approach includes:\n",
    "\n",
    "* Finding users similar to **U** who have rated the item **I**\n",
    "\n",
    "* Calculating the rating **R** based the ratings of users found in the previous step\n",
    "\n",
    "**How to Find Similar Users on the Basis of Ratings**\n",
    "To understand the concept of similarity, let’s create a simple dataset first.\n",
    "\n",
    "The data includes four users A, B, C, and D, who have rated two movies. The ratings are stored in lists, and each list contains two numbers indicating the rating of each movie:\n",
    "\n",
    "* Ratings by **A** are `\\[1.0, 2.0\\]`.\n",
    "\n",
    "* Ratings by **B** are `\\[2.0, 4.0\\]`.\n",
    "\n",
    "* Ratings by **C** are `\\[2.5, 4.0\\]`.\n",
    "\n",
    "* Ratings by **D** are `\\[4.5, 5.0\\]`.\n",
    "\n",
    " To start off with a visual clue, plot the ratings of two movies given by the users on a graph and look for a pattern. \n",
    "\n",
    "The graph looks like this:\n",
    "\n",
    "![](data:;base64,UklGRhYOAABXRUJQVlA4IAoOAACQegCdASqLAqwBPkkkkEYioiGhIHM4SFAJCWlu+EzIwBhJgDkNSv+vbwhFMO+UOf0gf6TzAOmr4kHq0c7d1p39C9QDzsfWe/xfm6atX0p/DDwq/qv5J9Y95Y9cM4j9UPrX5Qf2rmN4AX45/JP7B+WPCqAA/Gv6B/evyQ/uHot/w34ze7viAfxj+d/4v8p/3/+j/9P4QlAD+Ofzz/N/3z8kvji/lf8h+7X+Z9rn5j/h/9t9zH2C/x7+bf5H+3/47/t/5T///+b7qvXl+5HscfrSCJs2ZgRgWLFixYsWLFfls8SJEiRIkSJEg+j2hYsWG6OchYsV3rC7k0KFChQoUKFC2N2/fv6crVq1dkchYsWLFixYpg2d4mkRvHVYnF8WersbfLXWqUKQyGPGEsvI5CxYsWDb0Jh7hfFnq5Mq04cPsJEiRIkSJEiRd8LFixmIhhTJ0a1WUIsGnxrkSJEiRIkR6YtRoO4COY2TE0nJLNPQiTklmXP4pZ6ixhQbFIcbbjxhV7CKhuJIuPHjx45Yvm4NxEiRCGf4kLFiztU+JYDpXTp06dOnTpkiQkLFgzuirohPg8Xl8xI4KTIWMxEMKZMmTJkyHiqG3lMqFIY7btkNad4Nn1gplUYPvNWJYWmOxshprUVbJtog2QKwuqN6QJxGEx5CxYsWNgzx3ZPx605PmMg9BzKYj6/syxmMZZP4s1U3HpqVpw4cOHBpHtdlc8Km5aYZaPBYFJtZnXeJ1EmTJmMq04cPsJEiRIkSIDXIgVWjwyh9zVTosDTD4/zFlU06H+9tXWqUKQyGQyGQyGQyGO/zeMmTJkyZDZlU4607a7G3y11qlCkMfJFdvq7Gvs169evXvbiGDRvNGDYiwtnesWMmTJlANMmTJkyZMmS8Qi0uxOJ3Iceuz2jat5bJkyZ0BTJkyZMmTJehMQVxhvNbOsRJ2n0HRtBXBa1ShSGQyGQyGQx+u0LaxIkSJEjOJNmzaB1ChQosZMmTJkyZMmSyhCbJ06dPeMKZMnS+rThw4cOHDg3WzzhX3WqUKQxowyGQyGQyGQvXQeU6dOnTp9SFMmTMZVpw4fYSJEiRIkSJEfUIU0GmTJmMq04cPsJEiRIkSJEiBbKDEx9UQRKUcDSz1F2/4pYui7f8Us9RdlaSzLiySn9+/fv32NMfOLDJdGIWLFMKJJy3KaoSEFkyZLI7JZY651oAXDhw4cOHDhw4cOHDmHq1atYE1s2bNmzZs2bNmzZs2C2uSnPEAz/hXl4SB94F89YAAMmQ1gpd7qzOhRgXXr169evXr169evXr169e28KYVq1atWrVq1aqwAD+/7czdmwT6kNTidvYHEsmXDpg6liCfzFwoIiQBqP1JigWnepvinddg1V2FRzoAqFaYtOH74ZgKp5FDzcuRxwBpfQ9msG35GPqbPmzBvk+Kf6Tv1X+EdS8aAhoncY3lAFfwbYMqruOUxwNvQSsIzolpxlHb72GFMKKo15vkdU1Fsrzv+MUzRhNS3m2lGO0ju5+HGBtnGDbGs8HC0HZjb5QCnnJICoHCQg4kkoXGeM3Ll5+0LsSbPPRbnHJ6+gF9geDaAo7+vklt2Jcw1RrUgUjCWCr/6dDmvRJiSLpJu/lUj52pYGOyQTcv7mxSyVmGrnTSXcNjpzycVOo9x9PlWlVcfL+3eqXj71VWijj4LT2zGwtntjpBpZXhZBbm6I5MeEoNSZsr+fWuCfUAFF/NnCvwGy1Tb68NYTggYDvDlfT6fT6fT6h5JIO+voG1LEoKoFC/gFEpPOP4iCIsE9gn7l3Dgcqf//Br/2h1zfydy5tD/fguyTV0NlTzKeK8Y82ZSxCTtFEJ2v6gkwk+c9yhVGFnbgcjtKNSyQhD/d2fdccC40Zggvp6ZmBnn1Ph9ghLMP38gAOLr7J7dDuLOp50qKjuGVQ34QIfrgSbEOF6AArKXRxIc4cETkHJfKafVGVaXvnhdrc8zeOb9YtSXOUetFD0zkrlvcXsDiEHHLOLtf0czdRregGNFMMevRgHc4umDk4qK15lgmkN0mrcXFyyZ5H3jFKukVM+5GgUAi//JkhEFd9LwX4ijzJaBs3PomAAOtFI8GrR9C75hnR1sM9pVxaFkRCG/mH9dBAEBSzzrCqjAeNSTqI0oq2FmZlcgWsjRWTz4d+PzRDyZnXZF7oppan+5AYb+Fljw8123i8BPK5EiLlleDR6kd/zexMU1PKayWjC0+keip6zKKn8JH4/O8AsDwvsh7AksH+Vx92lM89pOI5lBJPJ8Jl+W9bvNETnimeeWoqO4kuLgFz+NenYJbeWB37dKMp7ghFKCoUPjUqP7e6WB6HQl9vesgP8CdGQxtoCMEqZu/MwJlLrEEISHxXNhlaK/pq2tHLJaN7u4QtXhwn1TZj+wHq4fiSJ8WSin4x0kkSYxxOe0c1KfPdwXEnRQQ8HVNWjMEAqOmf8XIy30moUzpTFCEACVg3DCY+FKyyTOKW+PHgf0Fu+hGtIHtkzYBlqgdY1vu9zk7JsFwTitS2UT0sXbElu3jAv1KmaZJTps0MBACwaYc5vCopavXIXBEAcrN6+tE9QWlom3ju/Zk6cCAkAmnMAG1BH8/+DOmwxMbSKYUPjw3DtKY8zenb5nNUMzfXIegNPgEU+mFPdfDsMmvnH9jCJDb3zKUHGju3S3z/kNuT4WfaoKQUSWeE9SJc0wDsNR0ibNlJZWzFsV7XQM2J92pf3PaCXqlKcP7pY4rfxla/61biUxZ6RUI61AGrtgLZlN4QRoV95FXGvhD/8rbpp/XFcJUsFDn/+bRdwniNvxjl7vnQgKjVmn569hzVSDrbrBE2qfpbhGtrIWsHd4/beFmjTCpgHTEmHuPS13QUOM+TgnpEE7v6IIEO6xSIdm8QKwCxETbO93cMn8jsYYC76A4aPX9QVOAJfeUj0qznx1CCvs9b2AleFMFRReNvCP8B9c2AMJTU33eHmB+EgFcGV77J35bxw6P51X/1iRInzi7W8KzlIaI+kOS0Ld+f8X9ub5tci3x3dJzTkrM/vueHrSaAADAFAVmelZaZ671ryoSjyt5ngKYdo+irkTeTNF1yePOjNmJybSnb0IUpINY9Dtec+Igqy0jHA2NBIdM+WNSGY7sSixVw4jO8qW1ZpxcPfuJwPY4/FsUI6v3mEUNb3LWk7uqkxVt5V+K+gBYWHfV+inSWE/lg84Dl9d2/3bCt4KQaw1cAVu3+5/n/4WzcYz2Dhu67o1ki0aBDwRRMpePt4e6Elj+rRlkr2tGDg26T9wkDK4CPT+AwaKvQlMcmfN6y5rvLc/PYK0evbqw3xqMIPYrq5DyEmq3o5Uq2iiwoAlEN+Ss7/V8SrElrd6lJOhwigX317CgOjmfq+gh1YqA0DTVSb3HW+UT9G15nrothk0ngm0XLwfHc0/tH6ukvbOnzHMzOAN7GAdB1Euef17rZ78E6ty8VF8gsOP4N4YMfDzSLfulG5Ix83vy5fSGLa1Io7KJgUnkp2wAXMWrbFiOYpt0R7d1aR7DKPte4JYROObVGiqP9In7cK59nznJGaDA2FEYv1YdJlwz1YDVTC31ueBJf+6qnBdw09s8aKJyWWMScADM/wq0WdNdj+odRjhcSdtd9D0gcAEtfdkqlp/YLltI+R1gw3unXBA3aRb7IMzN1GQwLAo95pxjJDW00SAPTi4YEhJ8xE9IBcTkY/zWiK2p5E45bIZEUhtQiXW2siUjHxfGHOn5jGZRQvTQAO8ZYPoqxufsf3+tBHaSHR0zKips9fc3jxYhgHDxy/Gn9vHF1CzYnFfKUTjg6d5+4IDbRceGdqI02tq+ZEvrPsSQVkpJmW1av4kyCOxuW0rP3F3PKcapNLcGi5bAwU+m6keEwHh2vcLwKqtJkR1CTMrjtba8MMrRolf4erzjn+A/DAzUcZ6mvg5l67M9V77/dMt5Cv6ptIP3ExqwWVo97StFWtG8dPpbSCj9IP+p4kJNqlNDwjqHDYEPg/CIyzB+AIgLfpNzuBihsT2JsPoHgEujqtEo+1HvR7xsaZAr2uTXHdXbnZCV+q/QtYGQ3Kvcj/1R/5EBkh1ktH18nxKV9J1qlca3/CAsnftQPWNtwU9Te32EDaQCvuUzq3cXSZFV6iui5ZJhos0FiM1lCr3X2cbmDXLV41nNolYazZNfjh5U1AAAAAwoTkcp8h1mXBbtK5Q2tyxd8ZaWFx7MuLIUY0GKJooFzNqFjhpsMSDnNkTQzxh34wvMMEqxPDJSAmCsnRKcXy4oeXium6DlInw4gT5LRNa5aYfJGuQjJTxCTqwNlmQ8dq1lQlqvtBeL1uKh5hJ2eEjyZINumlBP+JkiqNIWW7g/+DMt8m+9SaZs7OchRKW73b0Rwbk85c3g5JYz9iJtgqealuWrgl7HmBHe0AzNW1uWsxRLBQk9+h6DNvSgVx7VvcVQBiG4sIWtgYnr6g3537yOPUmDEsJ3BRfip/ErnscjLR+tf4Rbau+Wvwdp5g1n2KiT8yRIA4qn6FqwU6XSXi88+eOnAX4/GhMFK/VH74u+DjxYgqlhOwOP+wSvktpbHjU+OWL0NKAKDtbHJZtV5/iVEVVYvJueemwllW+pMBTtcXNdcvSSoAbs3vNF8uXVe6gjEIVn5lRdx4mE6Ia/Ul41jH9gDofU/quj4BTnuOIZgSoYzz9zfahyqOaxz9HuAPnNp7rcflLOB43+jsdobfZo15Y/vjTeG6wWRbuCMm5jz9tNMD5oy0NC5YZOAP3claQ9P6/cF06pxFqph9FmWYpirzh+GVmElznx49AAAAAAAAAA=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "0.5\n",
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "## To calculate euclidian distances, use scipy\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "a = [1, 2]\n",
    "b = [2, 4]\n",
    "c = [2.5, 4]\n",
    "d = [4.5, 5]\n",
    "\n",
    "print(spatial.distance.euclidean(c, a))\n",
    "print(spatial.distance.euclidean(c, b))\n",
    "print(spatial.distance.euclidean(c, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You could say C is closer to D in terms of distance. But looking at the rankings, it would seem that the choices of C would align with that of A more than D because both A and C like the second movie almost twice as much as they like the first movie, but D likes both of the movies equally.\n",
    "\n",
    "So, what can you use to identify such patterns that Euclidean distance cannot? Can the angle between the lines joining the points to the origin be used to make a decision? You can take a look at the angle between the lines joining the origin of the graph to the respective points as shown:\n",
    "\n",
    "![](data:;base64,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)\n",
    "\n",
    "The graph shows four lines joining each point to the origin. The lines for **A** and **B** are coincident, making the angle between them zero.\n",
    "\n",
    "You can consider that, if the angle between the lines is increased, then the similarity decreases, and if the angle is zero, then the users are very similar.\n",
    "\n",
    "To calculate similarity using angle, you need a function that returns \n",
    "\n",
    "\\- a  **higher similarity** or **smaller distance** for a lower angle and \n",
    "\n",
    "\\- a **lower similarity** or **larger distance** for a higher angle. \n",
    "\n",
    "The cosine of an angle is a function that decreases from 1 to -1 as the angle increases from 0 to 180.\n",
    "\n",
    "You can use the cosine of the angle to find the similarity between two users. The higher the angle, the lower will be the cosine and thus, the lower will be the similarity of the users. You can also inverse the value of the cosine of the angle to get the cosine distance between the users by subtracting it from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004504527406047898\n",
      "0.004504527406047898\n",
      "0.015137225946083022\n"
     ]
    }
   ],
   "source": [
    "## To calculate cosine distance of vectors, use the scipy function that returns a higher value for higher angle:\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "a = [1, 2]\n",
    "b = [2, 4]\n",
    "c = [2.5, 4]\n",
    "d = [4.5, 5]\n",
    "\n",
    "print(spatial.distance.cosine(c, a))\n",
    "print(spatial.distance.cosine(c, b))\n",
    "print(spatial.distance.cosine(c, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The lower angle between the vectors of C and A gives a lower cosine distance value. If you want to rank user similarities in this way, use cosine distance.\n",
    "\n",
    "**Note**: In the above example, only two movies are considered, which makes it easier to visualize the rating vectors in two dimensions. This is only done to make the explanation easier. Real use cases with multiple items would involve more dimensions in rating vectors. You might want to go into the mathematics of cosine similarity as well.\n",
    "\n",
    "Notice that users A and B are considered absolutely similar in the cosine similarity metric despite having different ratings. This is actually a common occurrence in the real world, and the users like the user A are what you can call tough raters. An example would be a movie critic who always gives out ratings lower than the average, but the rankings of the items in their list would be similar to the Average raters like B.\n",
    "\n",
    "To factor in such individual user preferences, you will need to bring all users to the same level by removing their biases. You can do this by subtracting the average rating given by that user to all items from each item rated by that user. Here’s what it would look like:\n",
    "\n",
    "* For user A, the rating vector `\\[1, 2\\]` has the average `1.5`. Subtracting `1.5` from every rating would give you the vector `\\[-0.5, 0.5\\]`.\n",
    "\n",
    "* For user B, the rating vector `\\[2, 4\\]` has the average `3`. Subtracting `3` from every rating would give you the vector `\\[-1, 1\\]`.\n",
    "\n",
    "By doing this, you have changed the value of the average rating given by every user to 0. Try doing the same for users C and D, and you’ll see that the ratings are now adjusted to give an average of 0 for all users, which brings them all to the same level and removes their biases.\n",
    "\n",
    "<ins>The cosine of the angle between the adjusted vectors is called centered cosine.</ins> This approach is normally used when there are a lot of missing values in the vectors, and you need to place a common value to fill up the missing values.\n",
    "\n",
    "Filling up the missing values in the ratings matrix with a random value could result in inaccuracies. A good choice to fill the missing values could be the average rating of each user, but the original averages of user A and B are `1.5` and `3` respectively, and filling up all the empty values of A with `1.5` and those of B with `3` would make them dissimilar users.\n",
    "\n",
    "But after adjusting the values, the centered average of both users is `0`, which allows you to capture the idea of the item being above or below average more accurately for both users with all missing values in both user’s vectors having the same value `0`.\n",
    "\n",
    "Euclidean distance and cosine similarity are some of the approaches that you can use to find users similar to one another and even items similar to one another. (The function used above calculates cosine distance. To calculate cosine similarity, subtract the distance from 1.)\n",
    "\n",
    "**Note:** The formula for centered cosine is the same as that for Pearson correlation coefficient. You will find that many resources and libraries on recommenders refer to the implementation of centered cosine as Pearson Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**How to Calculate the Ratings** After you have determined a list of users similar to a user U, you need to calculate the rating R that U would give to a certain item I. \n",
    "\n",
    "Again, just like similarity, you can do this in multiple ways.\n",
    "\n",
    "* Average rating: You can predict that a user’s rating R for an item I will be close to the average of the ratings given to I by the top 5 or top 10 users most similar to U.\n",
    "\n",
    "* Weighted rating: You give more consideration to the ratings of similar users in order of their similarity.\\\n",
    "  \\\n",
    "  Now, you know how to find similar users and how to calculate ratings based on their ratings. There’s also a variation of collaborative filtering where you predict ratings by finding items similar to each other instead of users and calculating the ratings. You’ll read about this variation in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**User-Based vs Item-Based Collaborative Filtering**\n",
    "\n",
    "The technique in the examples explained above, where the rating matrix is used to find similar users based on the ratings they give, is called <ins>user-based or user-user collaborative filtering</ins>. If you use the rating matrix to find similar items based on the ratings given to them by users, then the approach is called <ins>item-based or item-item collaborative filtering</ins>.\n",
    "\n",
    "The two approaches are mathematically quite similar, but there is a conceptual difference between the two. Here’s how the two compare:\n",
    "\n",
    "<ins>User-based</ins>: For a user U, with a set of similar users determined based on rating vectors consisting of given item ratings, the rating for an item I, which hasn’t been rated, is found by picking out N users from the similarity list who have rated the item I and calculating the rating based on these N ratings.\n",
    "\n",
    "<ins>Item-based:</ins> For an item I, with a set of similar items determined based on rating vectors consisting of received user ratings, the rating by a user U, who hasn’t rated it, is found by picking out N items from the similarity list that have been rated by U and calculating the rating based on these N ratings.\n",
    "\n",
    "Item-based collaborative filtering was developed by Amazon. In a system where there are more users than items, item-based filtering is faster and more stable than user-based. It is effective because usually, the average rating received by an item doesn’t change as quickly as the average rating given by a user to different items. It’s also known to perform better than the user-based approach when the ratings matrix is sparse.\n",
    "\n",
    "Although, the item-based approach performs poorly for datasets with browsing or entertainment related items such as MovieLens, where the recommendations it gives out seem very obvious to the target users. Such datasets see better results with matrix factorization techniques, which you’ll see in the next section, or with hybrid recommenders that also take into account the content of the data like the genre by using content-based filtering.\n",
    "\n",
    "You can use the library Surprise to experiment with different recommender algorithms quickly. (You will see more about this later in the article.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The second category covers the Model based approaches, which involve a step to reduce or compress the large but sparse user-item matrix. For understanding this step, a basic understanding of dimensionality reduction can be very helpful.\n",
    "\n",
    "**Dimensionality Reduction**\n",
    "In the user-item matrix, there are two dimensions:\n",
    "\n",
    "* The number of users\n",
    "\n",
    "* The number of items \n",
    "\n",
    "If the matrix is mostly empty, reducing dimensions can improve the performance of the algorithm in terms of both space and time. You can use various methods like matrix factorization or autoencoders to do this.\n",
    "\n",
    "<ins>Matrix factorization</ins> can be seen as breaking down a large matrix into a product of smaller ones.\n",
    "\n",
    "This is similar to the factorization of integers, where 12 can be written as 6 x 2 or 4 x 3. In the case of matrices, a matrix A with dimensions m x n can be reduced to a product of two matrices X and Y with dimensions m x p and p x n respectively.\n",
    "\n",
    "The reduced matrices actually represent the users and items individually. The **m** rows in the first matrix represent the **m** users, and the **p** columns tell you about the features or characteristics of the users. The same goes for the item matrix with **n** items and **p** characteristics.\n",
    "\n",
    "**Note:** In matrix multiplication, a matrix **X** can be multiplied by **Y** only if the number of columns in **X** is equal to the number of rows in **Y**. Therefore the two reduced matrices have a common dimension **p**. Depending on the algorithm used for dimensionality reduction, the number of reduced matrices can be more than two as well.\n",
    "\n",
    " Here’s an example of how matrix factorization looks:\n",
    "\n",
    "![](data:;base64,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)\n",
    "\n",
    "In the image above, the matrix is reduced into two matrices. The one on the left is the user matrix with *m* users, and the one on top is the item matrix with *n* items. The rating `4` is reduced or factorized into:\n",
    "\n",
    "1. A user vector `(2, -1)`\n",
    "\n",
    "2. An item vector `(2.5, 1)`\n",
    "\n",
    "The two columns in the user matrix and the two rows in the item matrix are called latent factors and are an indication of hidden characteristics about the users or the items. A possible interpretation of the factorization could look like this:\n",
    "\n",
    "* Assume that in a user vector `(u, v)`, `u` represents how much a user likes the Horror genre, and `v` represents how much they like the Romance genre.\n",
    "\n",
    "* The user vector `(2, -1)` thus represents a user who likes horror movies and rates them positively and dislikes movies that have romance and rates them negatively.\n",
    "\n",
    "* Assume that in an item vector `(i, j)`, `i` represents how much a movie belongs to the Horror genre, and `j` represents how much that movie belongs to the Romance genre.\n",
    "\n",
    "* The movie `(2.5, 1)` has a Horror rating of `2.5` and a Romance rating of `1`. Multiplying it by the user vector using matrix multiplication rules gives you `(2 \\* 2.5) + (-1 \\* 1) = 4`.\n",
    "\n",
    "* So, the movie belonged to the Horror genre, and the user could have rated it `5`, but the slight inclusion of Romance caused the final rating to drop to `4`.\n",
    "\n",
    "The factor matrices can provide such insights about users and items, but in reality they are usually much more complex than the explanation given above. The number of such factors can be anything from one to hundreds or even thousands. This number is one of the things that need to be optimized during the training of the model.\n",
    "\n",
    "In the example, you had two latent factors for movie genres, but in real scenarios, these latent factors need not be analyzed too much. These are patterns in the data that will play their part automatically whether you decipher their underlying meaning or not.\n",
    "\n",
    "The number of latent factors affects the recommendations in a manner where the greater the number of factors, the more personalized the recommendations become. But too many factors can lead to overfitting in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Algorithms for Matrix Factorization**\n",
    "One of the popular algorithms to factorize a matrix is the singular value decomposition (SVD) algorithm. SVD came into the limelight when matrix factorization was seen performing well in the Netflix prize competition. Other algorithms include PCA and its variations, NMF, and so on. Autoencoders can also be used for dimensionality reduction in case you want to use Neural Networks.\n",
    "\n",
    "You can find the implementations of these algorithms in various libraries for Python so you don’t need to worry about the details at this point. But in case you want to read more, the chapter on dimensionality reduction in the book Mining of Massive Datasets is worth a read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are quite a few libraries and toolkits in Python that provide implementations of various algorithms that you can use to build a recommender. But the one that you should try out while understanding recommendation systems is [Surprise](https://github.com/NicolasHug/Surprise).\n",
    "\n",
    "Surprise is a Python [SciKit](https://www.scipy.org/scikits.html) that comes with various recommender algorithms and similarity metrics to make it easy to build and analyze recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To use Surprise, you should first know some of the basic [modules](https://realpython.com/python-modules-packages/) and [classes](https://realpython.com/lessons/classes-python/) available in it:\n",
    "\n",
    "* The `Dataset` module is used to load data from files, [Pandas dataframes](https://realpython.com/courses/pandas-dataframes-101/), or even built-in datasets available for experimentation. (MovieLens 100k is one of the built-in datasets in Surprise.) To load a dataset, some of the available methods are:\n",
    "\n",
    "  * `Dataset.load_builtin()`\n",
    "\n",
    "  * `Dataset.load_from_file()`\n",
    "\n",
    "  * `Dataset.load_from_df()`\n",
    "\n",
    "* The `Reader` class is used to parse a file containing ratings. The default format in which it accepts data is that each rating is stored in a separate line in the order `user item rating`. This order and the separator can be configured using parameters:\n",
    "\n",
    "  * **`line_format`** is a [string](https://realpython.com/python-strings/) that stores the order of the data with field names separated by a space, as in `\"item user rating\"`.\n",
    "\n",
    "  * **`sep`** is used to specify separator between fields, such as `','`.\n",
    "\n",
    "  * **`rating_scale`** is used to specify the rating scale. The default is `(1, 5)`.\n",
    "\n",
    "  * **`skip_lines`** is used to indicate the number of lines to skip at the beginning of the file. The default is `0`.\n",
    "\n",
    "Here’s a program that you can use to load data from a Pandas dataframe or the from built in MovieLens 100k dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load_data.py\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "# This is the same data that was plotted for similarity earlier\n",
    "# with one new user \"E\" who has rated only movie 1\n",
    "ratings_dict = {\n",
    "    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n",
    "    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "# Loads the builtin Movielens-100k data\n",
    "movielens = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "**Algorithms Based on K-Nearest Neighbours** \n",
    "\n",
    "The choice of algorithm for the recommender function depends on the technique you want to use. For the memory-based approaches discussed above, the algorithm that would fit the bill is [Centered k-NN](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans) because the algorithm is very close to the centered cosine similarity formula explained above. It is available in Surprise as `KNNWithMeans`.\n",
    "\n",
    "To find the similarity, you simply have to configure the function by passing a dictionary as an argument to the recommender function. The dictionary should have the required keys, such as the following:\n",
    "\n",
    "* **`name`** contains the similarity metric to use. Options are `cosine`, `msd`, `pearson`, or `pearson_baseline`. The default is [`msd`](https://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.msd).\n",
    "\n",
    "* **`user_based`** is a `boolean` that tells whether the approach will be user-based or item-based. The default is `True`, which means the user-based approach will be used.\n",
    "\n",
    "* **`min_support`** is the minimum number of common items needed between users to consider them for similarity. For the item-based approach, this corresponds to the minimum number of common users for two items.\n",
    "\n",
    "The following program configures the `KNNWithMeans` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# recommender.py\n",
    "\n",
    "from surprise import KNNWithMeans\n",
    "\n",
    "# To use item-based cosine similarity\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # Compute  similarities between items\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The recommender function in the above program is configured to use the cosine similarity and to find similar items using the item-based approach.\n",
    "\n",
    "To try out this recommender, you need to create a `Trainset` from `data`. `Trainset` is built using the same data but contains more information about the data, such as the number of users and items (`n_users`, `n_items`) that are used by the algorithm. You can create it either by using the entire data or a part of the data. You can also divide the data into folds where some of the data will be used for training and some for testing.\n",
    "\n",
    "Here’s an example to find out how the user **E** would rate the movie 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNWithMeans at 0x26085a77a48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from load_data import data\n",
    "# from recommender import algo\n",
    "\n",
    "trainingSet = data.build_full_trainset()\n",
    "\n",
    "algo.fit(trainingSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = algo.predict('E', 2)\n",
    "prediction.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The algorithm predicted that the user **E** would rate the movie 4.15, which could be high enough to be shown as a recommendation.\n",
    "\n",
    "You should try out the different [k-NN based algorithms](https://surprise.readthedocs.io/en/stable/knn_inspired.html) along with different similarity options and [matrix factorization algorithms](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) available in the Surprise library. Try them out on the MovieLens dataset to see if you can beat some benchmarks. The next section will cover how to use Surprise to check which parameters perform best for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Tuning the Algorithm Parameters**\n",
    "\n",
    "Surprise provides a GridSearchCV class analogous to GridSearchCV from scikit-learn.\n",
    "\n",
    "With a dict of all parameters, GridSearchCV tries all the combinations of parameters and reports the best parameters for any accuracy measure\n",
    "\n",
    "For example, you can check which similarity metric works best for your data in memory-based approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.942781087879165\n",
      "{'sim_options': {'name': 'msd', 'min_support': 3, 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "sim_options = {\n",
    "    \"name\": [\"msd\", \"cosine\"],\n",
    "    \"min_support\": [3, 4, 5],\n",
    "    \"user_based\": [False, True],\n",
    "}\n",
    "\n",
    "param_grid = {\"sim_options\": sim_options}\n",
    "\n",
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So, for the MovieLens 100k dataset, Centered-KNN algorithm works best if you go with item-based approach and use msd as the similarity metric with minimum support 3.\n",
    "\n",
    "Similarly, for model-based approaches, we can use `Surprise` to check which values for the following factors work best:\n",
    "\n",
    "* **`n_epochs`** is the number of iterations of SGD, which is basically an iterative method used in Statistics to minimize a function.\n",
    "\n",
    "* **`lr_all`** is the learning rate for all parameters, which is a parameter that decides how much the parameters are adjusted in each iteration.\n",
    "\n",
    "* **`reg_all`** is the regularization term for all parameters, which is a penalty term added to prevent overfitting.\n",
    "\n",
    "The following program will check the best values for the [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp) algorithm, which is a matrix factorization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9645165608255408\n",
      "{'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10],\n",
    "    \"lr_all\": [0.002, 0.005],\n",
    "    \"reg_all\": [0.4, 0.6]}\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So, for the MovieLens 100k dataset, the `SVD` algorithm works best if you go with 10 epochs and use a learning rate of 0.005 and 0.4 regularization.\n",
    "\n",
    "Other Matrix Factorization based algorithms available in `Surprise` are [SVD++](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp) and [NMF](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp).\n",
    "\n",
    "Following these examples, you can dive deep into all the parameters that can be used in these algorithms. You should definitely check out the mathematics behind them. Since you won’t have to worry much about the implementation of algorithms initially, recommenders can be a great way to segue into the field of machine learning and build an application based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## When Can Collaborative Filtering Be Used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Collaborative filtering works around the interactions that users have with items. These interactions can help find patterns that the data about the items or users itself can’t. Here are some points that can help you decide if collaborative filtering can be used:\n",
    "\n",
    "* Collaborative filtering doesn’t require features about the items or users to be known. It is suited for a set of different types of items, for example, a supermarket’s inventory where items of various categories can be added. In a set of similar items such as that of a bookstore, though, known features like writers and genres can be useful and might benefit from content-based or hybrid approaches.\n",
    "\n",
    "* Collaborative filtering can help recommenders to not overspecialize in a user’s profile and recommend items that are completely different from what they have seen before. If you want your recommender to not suggest a pair of sneakers to someone who just bought another similar pair of sneakers, then try to add collaborative filtering to your recommender spell.\n",
    "\n",
    "Although collaborative Filtering is very commonly used in recommenders, some of the challenges that are faced while using it are the following:\n",
    "\n",
    "* Collaborative filtering can lead to some problems like cold start for new items that are added to the list. Until someone rates them, they don’t get recommended.\n",
    "\n",
    "* Data sparsity can affect the quality of user-based recommenders and also add to the cold start problem mentioned above.\n",
    "\n",
    "* Scaling can be a challenge for growing datasets as the complexity can become too large. Item-based recommenders are faster than user-based when the dataset is large.\n",
    "\n",
    "* With a straightforward implementation, you might observe that the recommendations tend to be already popular, and the items from the long tail section might get ignored.\n",
    "\n",
    "With every type of recommender algorithm having its own list of pros and cons, it’s usually a hybrid recommender that comes to the rescue. The benefits of multiple algorithms working together or in a pipeline can help you set up more accurate recommenders. In fact, the solution of the winner of the Netflix prize was also a complex mix of multiple algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You now know what calculations go into a collaborative-filtering type recommender and how to try out the various types of algorithms quickly on your dataset to see if collaborative filtering is the way to go. Even if it does not seem to fit your data with high accuracy, some of the use cases discussed might help you plan things in a hybrid way for the long term.\n",
    "\n",
    "Here are some resources for more implementations and further reading on collaborative filtering and other recommendation algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
